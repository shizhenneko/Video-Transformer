# 图像分类：核心概念与挑战

> 🎯 **一句话核心**
> 本课程深入探讨图像分类这一核心计算机视觉任务，揭示其对人类而言看似简单，对机器而言却因语义鸿沟、视角变化、类内差异、细粒度类别、背景杂乱、光照变化、形变和遮挡等挑战而变得复杂的本质，并强调其在医疗、天文学和目标检测等领域的广泛应用。

## 🖼️ 核心图解
![图像分类任务的核心概念与挑战总览](../blueprints/BV19TKHzUEVs_p2_mind_map.png)

## 第一部分：图像分类基础与核心挑战

图像分类是计算机视觉领域的一项核心任务，其目标是给定一张输入图像，算法能够将其归类到预先定义的一组固定类别标签中的某一个。语义鸿沟是指人类对图像的直观理解与计算机对图像的原始数据表示之间存在的巨大差异。

**图像分类定义**：图像分类是计算机视觉领域的一项核心任务，其目标是给定一张输入图像，算法能够将其归类到预先定义的一组固定类别标签中的某一个。例如，输入一张猫的图片，算法输出“猫”这个标签。 例如：输入：一张斑点猫的图片。
**语义鸿沟 (Semantic Gap)**：语义鸿沟是指人类对图像的直观理解与计算机对图像的原始数据表示之间存在的巨大差异。人类看到猫的图片能立即识别出猫，这是通过大脑复杂的感知和处理过程实现的，但我们并非有意识地感知到像素值。 例如：输入：一张800x600像素的彩色猫图片。
**视角变化 (Viewpoint Variation)**：即使是同一个物体，从稍微不同的角度拍摄，图像中的所有像素值都会以一种非常不直观的方式发生剧烈变化。对于人类来说，这只猫仍然是那只猫，我们能识别出它脸上的斑纹等特征。 例如：假设一张猫的正面照，像素矩阵A。
**类内差异 (Intraclass Variation)**：类内差异是指同一类别下的不同个体之间存在的显著视觉差异。例如，不同品种、年龄、毛色和体型的猫看起来非常不同。 例如：一张橘猫和一张布偶猫的图片，虽然都是猫，但它们的毛色、形态差异很大，导致其像素值网格也截然不同。
**细粒度类别 (Fine-Grained Categories)**：细粒度类别识别是指识别那些在视觉上非常相似但属于不同子类别的物体。例如，区分缅因猫（Maine Coon）、布偶猫（Ragdoll）和美国短毛猫（American Shorthair）。 例如：给定三张猫的图片，分别是一只缅因猫、一只布偶猫和一只美国短毛猫。
**背景杂乱 (Background Clutter)**：背景杂乱是指图像中我们想要识别的物体与背景环境融合在一起，难以区分。这可能是由于自然伪装（如猫在落叶中）或其他复杂的场景元素造成的。 例如：一张猫坐在满是落叶的地面上的图片。
**光照变化 (Illumination Changes)**：光照变化是指场景中光照条件的变化（如明暗、方向、颜色），这会导致图像中物体的像素值发生剧烈改变，但物体本身的潜在语义（即它是什么）并未改变。例如，同一只猫在黑暗中、白天阳光下或不同色温的光照下，其图像的亮度、对比度和颜色分布会大相径庭。 例如：同一只黑猫，在强光下可能呈现出深灰色，在弱光下则几乎全黑。
**形变 (Deformation)**：形变是指物体在图像中可能以非常不同的姿态或形状出现。猫是特别容易形变的物体类别，它们可以坐、卧、伸展或蜷缩成各种姿势。 例如：一张猫坐着的图片和一张猫躺着的图片，虽然是同一只猫，但身体姿态完全不同，导致其图像特征差异很大。
**遮挡 (Occlusion)**：遮挡是指图像中想要识别的物体可能部分或几乎完全不可见。例如，一只猫躲在毯子下只露出眼睛，或者只看到猫的尾巴从沙发垫下伸出来。 例如：一张图片中只看到一条斑纹尾巴从沙发垫下伸出。

### 📋 第一部分自测

1. 「图像分类定义」的核心含义是什么？
2. 「语义鸿沟 (Semantic Gap)」在图像分类任务中主要解决什么问题？
3. 什么时候更容易遇到「视角变化 (Viewpoint Variation)」相关的困难？

## 第二部分：价值、应用与数据驱动范式

图像分类是一项非常有用且强大的技术。图像分类不仅本身具有很高的实用价值，它还是许多其他更复杂计算机视觉算法的基本构建模块。

**图像分类的实用性**：图像分类是一项非常有用且强大的技术。如果能够克服上述挑战并开发出鲁棒的图像分类算法，它将解锁大量强大而实用的应用。 例如：在医学影像中，图像分类可用于诊断皮肤病变是否为恶性肿瘤；在天文学中，可用于对望远镜收集的视觉数据进行分类，识别不同类型的星系；在生态学中，可用于识别和分类不同类型的动物，如鲸鱼。
**图像分类作为其他任务的构建模块**：图像分类不仅本身具有很高的实用价值，它还是许多其他更复杂计算机视觉算法的基本构建模块。例如，在目标检测（Object Detection）任务中，算法不仅需要识别图像中存在哪些物体，还需要用边界框（bounding box）标出它们在图像中的精确位置。 例如：输入：一张海边骑马人的图片。
**图像分类的基本概念与重要性**：图像分类是计算机视觉中的一个基本任务，旨在将输入的图像自动归类到预定义的类别中。它被认为是“非常有用”的问题，因为其核心能力是理解图像内容的本质，这为许多实际应用提供了基础。 例如：假设一个图像分类器需要识别常见的动物。
**图像分类的实际应用案例**：图像分类技术在多个科学和工业领域都展现出巨大的潜力。例如，在医疗成像中，它可以帮助医生区分良性与恶性肿瘤，辅助早期诊断。 例如：以医疗成像为例，医生通过乳腺X光片（mamogram）来诊断乳腺癌。
**图像分类作为其他计算机视觉任务的构建块**：图像分类不仅自身是一个有用的问题，它还是许多其他计算机视觉算法的“基本构建块”。这意味着，通过组合或扩展图像分类的核心能力，可以实现更高级、更复杂的视觉任务。 例如：考虑一个简单的例子，如果我们想构建一个识别图像中所有动物的系统。
**数据驱动的机器学习范式**：数据驱动的机器学习方法通过以下三个核心步骤实现：1.收集带有图像和对应标签的数据集；2. 例如：假设我们要训练一个识别猫和狗的分类器： 输入： 1.

### 📋 第二部分自测

1. 「图像分类的实用性」的核心含义是什么？
2. 「图像分类作为其他任务的构建模块」在图像分类任务中主要解决什么问题？
3. 什么时候更容易遇到「图像分类的基本概念与重要性」相关的困难？

## 第三部分：常用数据集与小样本学习

MNIST是一个包含手写数字的图像分类数据集，是计算机视觉领域最常见的基准数据集之一。CIFAR-10是另一个常用的图像分类数据集，包含10个类别，如飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车。

**MNIST数据集**：MNIST是一个包含手写数字的图像分类数据集，是计算机视觉领域最常见的基准数据集之一。它包含0到9共10个类别，图像尺寸为28x28像素的灰度图，非常小巧。 例如：输入：一张28x28像素的灰度手写数字图片，例如数字“7”。
**CIFAR-10数据集**：CIFAR-10是另一个常用的图像分类数据集，包含10个类别，如飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车。它提供了5万张训练图像（每个类别5000张）和1万张测试图像（每个类别1000张）。 例如：输入：一张32x32像素的彩色汽车图片。
**CIFAR-100数据集**：CIFAR-100是CIFAR-10的扩展版本，具有类似的统计特性，但类别数量从10个增加到了100个。它同样包含5万张训练图像（每个类别500张）和1万张测试图像（每个类别100张），图像尺寸仍为32x32像素的彩色RGB图像。 例如：输入：一张32x32像素的彩色“海豚”图片。
**ImageNet数据集**：ImageNet已成为图像分类数据集的“黄金标准”。它包含1000个不同的类别，远超CIFAR或MNIST的10个类别，任务非常有趣且极具挑战性。 例如：输入：一张斑点狗（dalmatian）的图片。
**MIT Places数据集**：MIT Places是另一个有趣的图像分类数据集，与ImageNet侧重于识别猫、狗、鱼和卡车等物体不同，Places数据集致力于场景识别，如兽医办公室、电梯门、鱼塘、卧室、自助餐厅、饮水洞、楼梯、酒吧、田野小路、会议中心、鞋店和热带雨林等。它包含365种不同的场景类型，约800万张训练图像、18250张验证图像（每个类别50张）和328500张测试图像（每个类别900张）。 例如：输入：一张包含会议桌和投影仪的房间图片。
**图像分类数据集的训练像素数量对比**：通过比较不同图像分类数据集的训练像素数量，可以直观地看出数据集规模的增长趋势。MNIST的训练像素约为47M（百万），CIFAR-10和CIFAR-100约为154M，ImageNet则达到了约251B（十亿），而Places365更是高达约1. 例如：假设一个算法在MNIST上达到了99%的准确率，这可能很快，但如果它在ImageNet上表现不佳，则说明其泛化能力不足。
**Omniglot数据集与小样本学习**：Omniglot数据集将“数据量越来越少”的趋势推向了极致，旨在评估算法在数据量相对较少的情况下进行学习的能力，即“小样本学习”（Few-Shot Learning）。该数据集包含1623个不同的类别，每个类别是地球上某种语言、某种字母表中的一个字母（来自50种不同的字母表）。 例如：输入：一种新语言的字母表中的一个新字符，只提供20张手写示例。
**Omniglot数据集概览**：Omniglot是一个专门为图像分类任务设计的数据集，特别是为了推动少样本学习（Few-Shot Learning）的研究。它包含了来自50种不同字母表的1623个独特的字符类别，每个类别仅提供20张手写字符图像。 例如：假设一个新字母表中的某个字符，Omniglot数据集将只提供该字符的20个手写样本。

### 📋 第三部分自测

1. 「MNIST数据集」的核心含义是什么？
2. 「CIFAR-10数据集」在图像分类任务中主要解决什么问题？
3. 什么时候更容易遇到「CIFAR-100数据集」相关的困难？

## 第四部分：最近邻方法与距离度量

最近邻算法的核心思想是“记忆”训练数据，并在预测时找到与新样本最相似的训练样本，然后将该训练样本的标签赋给新样本。为了比较两张图像的相似性，最近邻算法需要一个距离度量（Distance Metric）。

**最近邻算法简介与实现**：最近邻算法的核心思想是“记忆”训练数据，并在预测时找到与新样本最相似的训练样本，然后将该训练样本的标签赋给新样本。在代码中，`train` 方法直接将输入的图像 `X` 和标签 `y` 存储为模型的成员变量 `self. 例如：输入： 训练图像：`X_train = [[1,1], [2,2], [5,5]]`，标签：`y_train = [0, 0, 1]` 测试图像：`X_test = [2.
**图像间的距离度量：L1距离**：为了比较两张图像的相似性，最近邻算法需要一个距离度量（Distance Metric）。L1距离，也称为曼哈顿距离或城市街区距离，是一种常用的选择。 例如：输入： 测试图像像素值（4x4）： `[[56, 32, 10, 18], [90, 23, 128, 133], [24, 26, 178, 170], [2, 0, 255, 220]]` 训练图像像素值（4x4）： `[[10, 20, 24, 17], [8, 10, 89, 100], [12, 16, 178, 170], [4, 32, 233, 112]]` 关键步骤（计算像素级绝对差值并求和）： 1.
**最近邻算法的性能分析与局限性**：最近邻算法的训练阶段仅是记忆训练数据，因此训练时间复杂度为O(1)。然而，其预测阶段需要将每个测试图像与所有N个训练图像进行比较，导致测试时间复杂度为O(N)。 例如：假设一个图像分类系统有100万张训练图像。
**像素距离与语义差异**：最近邻分类器通过计算测试图像与所有训练图像之间的距离，并选择距离最小的训练图像的类别作为预测结果。这种方法依赖于距离度量来定义“相似性”。 例如：输入：一张测试图片（一只青蛙，主体为棕色，背景为白色）和训练集中的两张图片：一张猫的图片（主体为棕色，背景为白色）和一张汽车的图片（主体为蓝色，背景为灰色）。
**决策边界的直观理解**：为了直观理解最近邻分类器的工作方式，我们可以将其映射到2D特征空间。假设图像只有两个像素，X轴代表第一个像素的强度值，Y轴代表第二个像素的强度值。 例如：输入：一个2D特征空间，其中包含不同颜色的训练点（例如，红点代表“猫”，蓝点代表“狗”，绿点代表“青蛙”）。
**K=1最近邻的噪声与异常值敏感性**：K=1的最近邻分类器对训练数据中的噪声和异常值非常敏感。由于它仅考虑单个最近邻来决定类别，即使训练集中有一个被错误标记的样本（异常值），它也会在其周围创建一个小的、不正确的决策区域。 例如：输入：一个2D特征空间，其中包含大量绿色训练点，但在绿色点云中心有一个孤立的黄色训练点（异常值）。
**K-最近邻分类器 (KNN) 原理**：K-最近邻（KNN）算法是对单一最近邻分类器的改进。它不再仅仅复制单个最近邻的标签，而是通过考虑K个最接近点的多数投票来确定测试点的类别。 例如：输入：一个测试点，其K=3个最近邻分别是：邻居1（红色类别）、邻居2（红色类别）、邻居3（蓝色类别）。
**K值对决策边界平滑度的影响**：K值的选择直接影响K-最近邻算法的决策边界平滑度。当K=1时，决策边界会紧密地围绕每个训练点，甚至为单个异常点创建独立的决策区域，导致边界非常嘈杂和不规则。 例如：输入：一个包含红色和蓝色点的2D特征空间。
**K > 1 时的平局处理**：当K-最近邻算法中的K值大于1，且K为偶数时，或者即使K为奇数但在某些情况下投票结果仍然出现多个类别票数相同的情况时，就会出现平局。例如，如果K=2，测试点的两个最近邻分别属于A类和B类，那么就无法通过多数投票确定类别。 例如：输入：一个测试点，K=3。
**L1与L2距离度量对决策边界的影响**：距离度量的选择是K-最近邻算法中一个重要的设计决策，它直接影响决策边界的形状和分类结果。L1（曼哈顿）距离和L2（欧几里得）距离是两种常见的度量方式，它们在决策边界上表现出定性差异。 例如：输入：一个2D特征空间，包含两个类别（例如，蓝色点和红色点）。
**K-最近邻算法的通用性与定制距离度量**：K-最近邻算法的真正强大之处在于其高度的通用性。只要能够为特定类型的数据定义一个合适的距离度量（或相似性度量），就可以将K-最近邻应用于该数据。 例如：输入：一篇关于“Mesh R-CNN”的论文（作为查询论文），以及一个包含数千篇研究论文的数据库。
**距离度量的选择**：KNN算法的核心在于找到与待分类样本“最近”的K个邻居。这个“最近”的定义完全取决于所选择的距离度量。 例如：以比较研究论文为例，我们可以使用TF-IDF（词频-逆文档频率）相似度作为距离度量。
**TF-IDF 相似度**：TF-IDF（Term Frequency-Inverse Document Frequency，词频-逆文档频率）是一种用于信息检索与文本挖掘的常用加权技术。它通过词语在单个文档中出现的频率（Term Frequency, TF）和词语在整个文档集合中出现的频率（Inverse Document Frequency, IDF）来衡量一个词语对于一个文档集或一个语料库中的其中一份文档的重要性。 例如：假设我们有两篇论文A和B。
**KNN Web 交互式演示功能**：通过JavaScript编写的KNN交互式Web演示工具，用户可以直观地探索KNN分类器的行为。该演示允许用户拖动数据点，实时观察决策边界如何响应变化。 例如：在演示中，如果将K值设置为1，决策边界会变得非常不规则和“锯齿状”，紧密围绕每个训练点。

### 📋 第四部分自测

1. 「最近邻算法简介与实现」的核心含义是什么？
2. 「图像间的距离度量：L1距离」在图像分类任务中主要解决什么问题？
3. 什么时候更容易遇到「最近邻算法的性能分析与局限性」相关的困难？

## 第五部分：超参数选择与评估

超参数（Hyperparameters）是机器学习算法中在学习过程开始之前就必须设定的参数，而不是通过训练数据学习得到的。第一种设置超参数的思路是选择在训练集上表现最好的超参数值。

**超参数的定义与特性**：超参数（Hyperparameters）是机器学习算法中在学习过程开始之前就必须设定的参数，而不是通过训练数据学习得到的。它们是关于学习算法本身的'选择'或'配置'。 例如：对于KNN，我们不能通过梯度下降等优化方法来'学习'K值或使用哪种距离度量。
**错误方法1：在训练集上选择超参数**：第一种设置超参数的思路是选择在训练集上表现最好的超参数值。这看起来合理，因为我们希望算法表现良好，而训练集就是用于训练的。 例如：假设我们有一个包含猫狗图像的训练集。
**错误方法2：在测试集上选择超参数**：第二种设置超参数的思路是，将数据集划分为训练集和测试集，然后选择在测试集上表现最好的超参数值。这种方法虽然比在训练集上选择超参数更进一步，但它仍然是一个根本性的错误，并且应该避免。 例如：假设你正在开发一个图像识别模型，并将其划分为训练集和测试集。
**正确实践：训练集、验证集和测试集划分**：这种方法的核心步骤如下： 1.**训练集 (Train Set)**：用于训练机器学习算法，让模型学习数据中的模式和关系。 例如：假设我们有一个包含1000张图片的数据集。
**超参数设置的错误方法：仅使用训练数据**：第一种不恰当的方法是直接在整个数据集上选择超参数。例如，对于K近邻（KNN）算法，如果K被设置为1，那么在训练集上，每个数据点都会找到它自己作为最近邻，并被正确分类。 例如：假设有一个二分类问题，训练集包含10个数据点。
**超参数设置的错误方法：训练集与测试集划分**：第二种不恰当的方法是将数据划分为训练集和测试集，然后在测试集上选择表现最好的超参数。这种做法的根本问题在于，它使得测试集不再是“未见数据”。 例如：假设我们有1000个数据点，分为800个训练点和200个测试点。
**超参数设置的正确实践：训练集、验证集和测试集划分**：最推荐的方法是将数据集划分为训练集、验证集（Validation Set）和测试集（Test Set）。训练集用于模型的学习，验证集用于调整超参数和进行模型选择，而测试集则只在所有超参数确定、模型训练完成后的最后阶段，且仅被触碰一次，用于评估模型的最终泛化性能。 例如：假设我们有一个图片分类任务。
**超参数设置的更优方法：交叉验证**：交叉验证（Cross-Validation）是一种比单一训练/验证/测试划分更稳健的超参数选择方法。它将数据集（通常是训练集和验证集的组合，或者整个可用数据减去最终测试集）分成多个“折叠”（folds）。 例如：以一个5折交叉验证为例： 1.

### 📋 第五部分自测

1. 「超参数的定义与特性」的核心含义是什么？
2. 「错误方法1：在训练集上选择超参数」在图像分类任务中主要解决什么问题？
3. 什么时候更容易遇到「错误方法2：在测试集上选择超参数」相关的困难？

## 第六部分：高维挑战与改进方向

K近邻（KNN）算法的一项有趣特性是其“通用逼近性质”。为了直观理解K近邻的通用逼近性质，我们可以观察1近邻（K=1）在回归任务中的表现。

**K近邻算法的通用逼近性质**：K近邻（KNN）算法的一项有趣特性是其“通用逼近性质”。这意味着，当训练样本的数量趋于无穷大时，K近邻算法实际上可以表示任何函数。 例如：考虑一个简单的回归任务：给定输入x，预测输出y。
**通用逼近的直观示例：1近邻回归**：为了直观理解K近邻的通用逼近性质，我们可以观察1近邻（K=1）在回归任务中的表现。在一个一维输入空间中，真实函数（蓝色曲线）通过有限的训练样本（黑色点）来近似。 例如：假设真实函数为 `y = x^3 - x^2 + 0.
**问题：维度诅咒**：尽管K近邻算法具有通用逼近的强大理论能力，但在实践中它面临一个严峻的挑战，即“维度诅咒”（Curse of Dimensionality）。这个问题指出，为了在多维空间中获得均匀的数据覆盖，所需的训练样本数量会随着维度的增加呈指数级增长。 例如：考虑一个在每个维度上需要4个点才能均匀覆盖的空间： 1.
**维度灾难 (Curse of Dimensionality)**：“维度灾难”描述的是在高维空间中，数据点变得极其稀疏的现象。这意味着为了在所有维度上均匀覆盖数据空间，所需的训练样本数量会随着维度的增加呈指数级增长。 例如：假设我们希望在空间中均匀分布数据点以达到某种密度。
**图像空间的巨大性**：图像空间是一个极其庞大的高维空间。即使是相对低分辨率的图像，如32x32像素的二值图像（每个像素只有黑白两种状态），其可能的组合数量也达到了惊人的2^(32*32) ≈ 10^308。 例如：考虑一个32x32像素的二值图像。
**K-近邻在测试时的速度问题 (KNN Speed at Test Time)**：K-近邻算法在训练阶段非常简单（只需存储所有训练数据），但在测试阶段，它需要计算待分类样本与训练集中所有样本的距离，并找出最近的K个邻居。当训练集规模巨大时，这种逐一计算距离的操作将导致测试时间呈线性增长，在高维空间中更是如此。 例如：假设一个训练集有100万张图像，每张图像有1024个像素维度。
**原始像素距离度量的非信息性 (Non-informative Distance Metrics on Raw Pixels)**：基于原始像素值（如L2范数或L1范数）的距离度量在图像领域通常缺乏语义意义。这是因为图像的语义内容对像素级别的微小扰动（如平移、旋转、遮挡或色调变化）非常敏感，这些扰动可能导致原始像素值之间巨大的L2距离，但在人类感知中，图像的语义内容（如物体类别、场景）可能保持不变。 例如：考虑一张原始人脸图像。
**结合ConvNet特征的K-近邻 (KNN with ConvNet Features)**：令人惊讶的是，虽然原始像素上的K-近邻效果不佳，但使用深度卷积神经网络（ConvNet）计算出的特征向量进行最近邻检索却能取得很好的效果。ConvNet通过多层卷积和池化操作，能够从原始像素中学习到更高级、更抽象的语义特征。 例如：给定一张查询图像，例如一张婴儿的图片。
**图像字幕生成示例 (Image Captioning Example)**：使用带有卷积神经网络特征的最近邻检索方法，对于许多问题来说都是一个非常强大的基线，包括图像字幕生成。在这种方法中，我们有一个包含大量图像及其对应字幕的训练数据集。 例如：在一个图像字幕生成任务中，我们有一个包含图像-字幕对的数据集。

### 📋 第六部分自测

1. 「K近邻算法的通用逼近性质」的核心含义是什么？
2. 「通用逼近的直观示例：1近邻回归」在图像分类任务中主要解决什么问题？
3. 什么时候更容易遇到「问题：维度诅咒」相关的困难？

## 📌 覆盖清单 (Coverage Index)

1. 图像分类定义 — 第一部分：图像分类基础与核心挑战
2. 语义鸿沟 (Semantic Gap) — 第一部分：图像分类基础与核心挑战
3. 视角变化 (Viewpoint Variation) — 第一部分：图像分类基础与核心挑战
4. 类内差异 (Intraclass Variation) — 第一部分：图像分类基础与核心挑战
5. 细粒度类别 (Fine-Grained Categories) — 第一部分：图像分类基础与核心挑战
6. 背景杂乱 (Background Clutter) — 第一部分：图像分类基础与核心挑战
7. 光照变化 (Illumination Changes) — 第一部分：图像分类基础与核心挑战
8. 形变 (Deformation) — 第一部分：图像分类基础与核心挑战
9. 遮挡 (Occlusion) — 第一部分：图像分类基础与核心挑战
10. 图像分类的实用性 — 第二部分：价值、应用与数据驱动范式
11. 图像分类作为其他任务的构建模块 — 第二部分：价值、应用与数据驱动范式
12. 图像分类的基本概念与重要性 — 第二部分：价值、应用与数据驱动范式
13. 图像分类的实际应用案例 — 第二部分：价值、应用与数据驱动范式
14. 图像分类作为其他计算机视觉任务的构建块 — 第二部分：价值、应用与数据驱动范式
15. 数据驱动的机器学习范式 — 第二部分：价值、应用与数据驱动范式
16. MNIST数据集 — 第三部分：常用数据集与小样本学习
17. CIFAR-10数据集 — 第三部分：常用数据集与小样本学习
18. CIFAR-100数据集 — 第三部分：常用数据集与小样本学习
19. ImageNet数据集 — 第三部分：常用数据集与小样本学习
20. MIT Places数据集 — 第三部分：常用数据集与小样本学习
21. 图像分类数据集的训练像素数量对比 — 第三部分：常用数据集与小样本学习
22. Omniglot数据集与小样本学习 — 第三部分：常用数据集与小样本学习
23. Omniglot数据集概览 — 第三部分：常用数据集与小样本学习
24. 最近邻算法简介与实现 — 第四部分：最近邻方法与距离度量
25. 图像间的距离度量：L1距离 — 第四部分：最近邻方法与距离度量
26. 最近邻算法的性能分析与局限性 — 第四部分：最近邻方法与距离度量
27. 像素距离与语义差异 — 第四部分：最近邻方法与距离度量
28. 决策边界的直观理解 — 第四部分：最近邻方法与距离度量
29. K=1最近邻的噪声与异常值敏感性 — 第四部分：最近邻方法与距离度量
30. K-最近邻分类器 (KNN) 原理 — 第四部分：最近邻方法与距离度量
31. K值对决策边界平滑度的影响 — 第四部分：最近邻方法与距离度量
32. K > 1 时的平局处理 — 第四部分：最近邻方法与距离度量
33. L1与L2距离度量对决策边界的影响 — 第四部分：最近邻方法与距离度量
34. K-最近邻算法的通用性与定制距离度量 — 第四部分：最近邻方法与距离度量
35. 距离度量的选择 — 第四部分：最近邻方法与距离度量
36. TF-IDF 相似度 — 第四部分：最近邻方法与距离度量
37. KNN Web 交互式演示功能 — 第四部分：最近邻方法与距离度量
38. 超参数的定义与特性 — 第五部分：超参数选择与评估
39. 错误方法1：在训练集上选择超参数 — 第五部分：超参数选择与评估
40. 错误方法2：在测试集上选择超参数 — 第五部分：超参数选择与评估
41. 正确实践：训练集、验证集和测试集划分 — 第五部分：超参数选择与评估
42. 超参数设置的错误方法：仅使用训练数据 — 第五部分：超参数选择与评估
43. 超参数设置的错误方法：训练集与测试集划分 — 第五部分：超参数选择与评估
44. 超参数设置的正确实践：训练集、验证集和测试集划分 — 第五部分：超参数选择与评估
45. 超参数设置的更优方法：交叉验证 — 第五部分：超参数选择与评估
46. K近邻算法的通用逼近性质 — 第六部分：高维挑战与改进方向
47. 通用逼近的直观示例：1近邻回归 — 第六部分：高维挑战与改进方向
48. 问题：维度诅咒 — 第六部分：高维挑战与改进方向
49. 维度灾难 (Curse of Dimensionality) — 第六部分：高维挑战与改进方向
50. 图像空间的巨大性 — 第六部分：高维挑战与改进方向
51. K-近邻在测试时的速度问题 (KNN Speed at Test Time) — 第六部分：高维挑战与改进方向
52. 原始像素距离度量的非信息性 (Non-informative Distance Metrics on Raw Pixels) — 第六部分：高维挑战与改进方向
53. 结合ConvNet特征的K-近邻 (KNN with ConvNet Features) — 第六部分：高维挑战与改进方向
54. 图像字幕生成示例 (Image Captioning Example) — 第六部分：高维挑战与改进方向
