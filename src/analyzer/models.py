"""
æ•°æ®æ¨¡å‹å®šä¹‰æ¨¡å—

å®šä¹‰è§†é¢‘åˆ†æç»“æœçš„æ•°æ®ç»“æ„ï¼Œç”¨äºç»“æ„åŒ–å­˜å‚¨å’Œä¼ é€’åˆ†ææ•°æ®ã€‚
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any
import re
import warnings

from .validators import detect_stub_output, validate_markdown_structure


@dataclass
class VisualSchemaItem:
    """å•å¼ çŸ¥è¯†è“å›¾çš„ Visual Schema"""

    type: str
    """å›¾ç‰‡ç±»å‹: overview / detail_flow / comparison"""

    description: str
    """å›¾ç‰‡æè¿°ï¼ˆä¸­æ–‡ï¼‰"""

    schema: str
    """Visual Schema Markdown å­—ç¬¦ä¸²"""


@dataclass
class KnowledgeDocument:
    """ç²¾è‹±çŸ¥è¯†ç¬”è®°æ•°æ®ç»“æ„"""

    title: str
    """ç¬”è®°æ ‡é¢˜"""

    one_sentence_summary: str
    """ä¸€å¥è¯æ ¸å¿ƒæ€»ç»“"""

    key_takeaways: list[str]
    """å…³é”®ç»“è®º/è¡ŒåŠ¨å»ºè®®åˆ—è¡¨"""

    deep_dive: list[dict[str, Any]]
    """
    æ·±åº¦è§£æåˆ—è¡¨ï¼ˆåˆ†ç« èŠ‚ï¼‰ï¼Œæ¯é¡¹åŒ…å«:
    - chapter_title: ç« èŠ‚æ ‡é¢˜
    - chapter_summary: ç« èŠ‚æ¦‚è¿°
    - sections: çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªçŸ¥è¯†ç‚¹åŒ…å«:
        - topic: çŸ¥è¯†ç‚¹ä¸»é¢˜
        - explanation: åŸç†è§£æ
        - example: å…·ä½“ç¤ºä¾‹
        - code: ä»£ç æ¼”ç¤º (å¯é€‰)
        - connections: ä¸å…¶ä»–çŸ¥è¯†ç‚¹çš„å…³è”è¯´æ˜åˆ—è¡¨
    """

    glossary: dict[str, str]
    """å…³é”®æœ¯è¯­è¡¨ï¼š{æœ¯è¯­: é€šä¿—å®šä¹‰}"""

    visual_schemas: list[VisualSchemaItem] = field(default_factory=list)
    """çŸ¥è¯†è“å›¾ Visual Schema åˆ—è¡¨ï¼ˆ1-2 å¼ ï¼‰"""

    def to_markdown(
        self,
        image_paths: list[str] | None = None,
        self_check_mode: str = "static",
    ) -> str:
        """
        å°†çŸ¥è¯†ç¬”è®°è½¬æ¢ä¸º Markdown æ ¼å¼

        Args:
             image_paths: çŸ¥è¯†è“å›¾å›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„åˆ—è¡¨(å¯é€‰)
             self_check_mode: è‡ªæµ‹é¢˜æ¸²æŸ“æ¨¡å¼(static/interactive/questions_only)

        Returns:
            æ ¼å¼åŒ–çš„ Markdown æ–‡æ¡£å­—ç¬¦ä¸²
        """
        self_check_mode = self._normalize_self_check_mode(self_check_mode)

        if self_check_mode == "default":
            return self._to_markdown_default(image_paths)

        if self_check_mode == "lecture":
            return self._to_markdown_lecture(image_paths)

        lines = [
            f"# {self.title}",
            "",
            "> ğŸ¯ **ä¸€å¥è¯æ ¸å¿ƒ**",
            f"> {self.one_sentence_summary}",
            "",
            "## ğŸ“ å…³é”®ç»“è®º (Key Takeaways)",
            "",
        ]

        # æ·»åŠ å…³é”®ç»“è®º
        for point in self.key_takeaways:
            lines.append(f"- {point}")
        lines.append("")

        # æ·»åŠ çŸ¥è¯†è“å›¾å›¾ç‰‡ï¼ˆæ”¯æŒå¤šå¼ ï¼‰
        if image_paths:
            lines.extend(
                [
                    "## ğŸ–¼ï¸ æ ¸å¿ƒå›¾è§£ (Visual Architecture)",
                    "",
                ]
            )
            for idx, img_path in enumerate(image_paths):
                desc = ""
                if idx < len(self.visual_schemas):
                    desc = self.visual_schemas[idx].description
                label = desc if desc else f"çŸ¥è¯†è“å›¾ {idx + 1}"
                lines.append(f"**{label}**")
                lines.append("")
                lines.append(f"![{label}]({img_path})")
                lines.append("")

        # æ·»åŠ æ·±åº¦è§£æï¼ˆåˆ†ç« èŠ‚ï¼‰
        lines.extend(
            [
                "## ğŸ” æ·±åº¦è§£æ (Deep Dive)",
                "",
            ]
        )

        chapter_num = 0
        global_section_num = 0
        legacy_answers: list[str] = []

        for chapter in self.deep_dive:
            chapter_num += 1
            chapter_title = chapter.get("chapter_title", f"ç¬¬{chapter_num}ç« ")
            chapter_summary = chapter.get("chapter_summary", "")
            sections = chapter.get("sections", [])

            # å¦‚æœæ˜¯æ—§æ ¼å¼ï¼ˆæ‰å¹³ deep_diveï¼Œæ—  chapter_titleï¼‰ï¼Œå…¼å®¹å¤„ç†
            if "topic" in chapter and "chapter_title" not in chapter:
                global_section_num += 1
                answers = self._render_section(
                    lines, global_section_num, chapter, self_check_mode
                )
                if self_check_mode == "static" and answers:
                    legacy_answers.extend(answers)
                continue

            lines.append(f"### ç¬¬{chapter_num}ç« ï¼š{chapter_title}")
            lines.append("")
            if chapter_summary:
                lines.append(f"> {chapter_summary}")
                lines.append("")

            chapter_answers: list[str] = []
            for section in sections:
                global_section_num += 1
                answers = self._render_section(
                    lines, global_section_num, section, self_check_mode
                )
                if self_check_mode == "static" and answers:
                    chapter_answers.extend(answers)

            if self_check_mode == "static" and chapter_answers:
                lines.append("#### ğŸ“Œ æœ¬ç« è‡ªæµ‹ç­”æ¡ˆ")
                lines.append("")
                lines.extend(chapter_answers)

        if self_check_mode == "static" and legacy_answers:
            lines.append("### ğŸ“Œ è‡ªæµ‹ç­”æ¡ˆ")
            lines.append("")
            lines.extend(legacy_answers)

        # æ·»åŠ å…³é”®æœ¯è¯­è¡¨
        if self.glossary:
            lines.extend(
                [
                    "## ğŸ“– å…³é”®æœ¯è¯­è¡¨ (Glossary)",
                    "",
                ]
            )
            for term, definition in self.glossary.items():
                lines.append(f"- **{term}**: {definition}")
            lines.append("")

        return "\n".join(lines)

    def _to_markdown_default(self, image_paths: list[str] | None) -> str:
        lines = [
            f"# {self.title}",
            "",
            "> ğŸ¯ **ä¸€å¥è¯æ ¸å¿ƒ**",
            f"> {self.one_sentence_summary}",
            "",
            "## ğŸ“ å…³é”®ç»“è®º (Key Takeaways)",
            "",
        ]

        for point in self.key_takeaways:
            lines.append(f"- {point}")
        lines.append("")

        if image_paths:
            lines.extend(
                [
                    "## ğŸ–¼ï¸ æ ¸å¿ƒå›¾è§£ (Visual Architecture)",
                    "",
                ]
            )
            for idx, img_path in enumerate(image_paths):
                desc = ""
                if idx < len(self.visual_schemas):
                    desc = self.visual_schemas[idx].description
                label = desc if desc else f"çŸ¥è¯†è“å›¾ {idx + 1}"
                lines.append(f"**{label}**")
                lines.append("")
                lines.append(f"![{label}]({img_path})")
                lines.append("")

        lines.extend(
            [
                "## ğŸ” æ·±åº¦è§£æ (Deep Dive)",
                "",
            ]
        )

        chapter_num = 0
        global_section_num = 0
        appendix_lines: list[str] = []
        coverage_lines: list[str] = []

        chapters: list[dict[str, Any]] = []
        if any("chapter_title" in item for item in self.deep_dive):
            chapters = self.deep_dive
        else:
            chapters = [
                {
                    "chapter_title": "æ ¸å¿ƒè¦ç‚¹",
                    "chapter_summary": "",
                    "sections": self.deep_dive,
                }
            ]

        for chapter in chapters:
            chapter_num += 1
            chapter_title = chapter.get("chapter_title", f"ç¬¬{chapter_num}ç« ")
            chapter_summary = chapter.get("chapter_summary", "")
            sections = chapter.get("sections", [])

            lines.append(f"### ç¬¬{chapter_num}ç« ï¼š{chapter_title}")
            lines.append("")
            if chapter_summary:
                lines.append(f"> {chapter_summary}")
                lines.append("")

            coverage_lines.append(f"- ç¬¬{chapter_num}ç« ï¼š{chapter_title}")

            chapter_questions: list[dict[str, str]] = []

            appendix_lines.append(f"### ç¬¬{chapter_num}ç« ï¼š{chapter_title}")
            appendix_lines.append("")
            if chapter_summary:
                appendix_lines.append(f"> {chapter_summary}")
                appendix_lines.append("")

            for section in sections:
                global_section_num += 1
                self._render_section_compact(lines, global_section_num, section)
                appendix_lines.extend(
                    self._render_section_appendix(global_section_num, section)
                )

                topic = section.get("topic", "æœªçŸ¥ä¸»é¢˜")
                coverage_lines.append(f"- {topic}")

                raw_self_check = section.get("self_check", [])
                if isinstance(raw_self_check, list):
                    for item in raw_self_check:
                        if isinstance(item, dict) and "q" in item and "a" in item:
                            chapter_questions.append(item)

            if chapter_questions:
                lines.append(f"### ğŸ“‹ ç¬¬{chapter_num}ç« è‡ªæµ‹")
                lines.append("")
                for idx, qa in enumerate(chapter_questions, 1):
                    label = f"Q{chapter_num}.{idx}"
                    question_text = str(qa["q"]).strip()
                    answer_text = str(qa["a"]).strip()
                    lines.append(f"- {label}ï¼š{question_text}")
                    lines.append(f"- {label} -> ç­”æ¡ˆï¼š{answer_text}")
                lines.append("")

        lines.extend(
            [
                "## ğŸ“Œ è¦†ç›–æ¸…å• (Coverage Index)",
                "",
            ]
        )
        lines.extend(coverage_lines)
        lines.append("")

        lines.extend(
            [
                "## ğŸ“ é™„å½• (Appendix)",
                "",
            ]
        )
        if appendix_lines:
            lines.extend(appendix_lines)
        else:
            lines.append("- ï¼ˆæ— é™„å½•å†…å®¹ï¼‰")
        lines.append("")

        if self.glossary:
            lines.extend(
                [
                    "## ğŸ“– å…³é”®æœ¯è¯­è¡¨ (Glossary)",
                    "",
                ]
            )
            for term, definition in self.glossary.items():
                lines.append(f"- **{term}**: {definition}")
            lines.append("")

        return "\n".join(lines)

    def _to_markdown_lecture(self, image_paths: list[str] | None) -> str:
        def cleaned(value: Any) -> str:
            return self._sanitize_lecture_text(value)

        def cleaned_main(value: Any) -> str:
            sanitized = cleaned(value)
            return sanitized.replace("```", "").strip()

        def normalize_code_block(value: Any) -> str:
            sanitized = cleaned(_normalize_field_value(value))
            if not sanitized:
                return ""
            return "\n".join(
                [line for line in sanitized.splitlines() if line.strip() != "```"]
            ).strip()

        def split_sentences(text: str) -> list[str]:
            if not text:
                return []
            parts = re.split(r"[ã€‚ï¼ï¼Ÿ!?]", text)
            return [part.strip() for part in parts if part.strip()]

        def collect_unique_sentences(text: str, seen: set[str]) -> list[str]:
            sentences: list[str] = []
            for sentence in split_sentences(text):
                if sentence in seen:
                    continue
                seen.add(sentence)
                sentences.append(sentence)
            return sentences

        def collect_topics(sections: list[dict[str, Any]]) -> list[str]:
            topics: list[str] = []
            for section in sections:
                topic = cleaned_main(section.get("topic", ""))
                if topic:
                    topics.append(topic)
            return topics

        lines: list[str] = [f"# {self.title}", ""]
        chapters = self._normalize_chapters(self.deep_dive)

        lines.extend(["## æ ¸å¿ƒæ¦‚å¿µå›¾è°±", ""])
        thesis = cleaned_main(self.one_sentence_summary)
        if not thesis and self.key_takeaways:
            thesis = cleaned_main(self.key_takeaways[0])
        if thesis:
            lines.append(thesis)
        else:
            lines.append("æœ¬è®²å›´ç»•æ ¸å¿ƒæ¦‚å¿µä¸å®è·µè„‰ç»œå±•å¼€ã€‚")
        lines.append("")

        if chapters:
            for chapter_num, chapter in enumerate(chapters, 1):
                chapter_title = cleaned_main(
                    chapter.get("chapter_title", f"ç¬¬{chapter_num}ç« ")
                )
                if not chapter_title:
                    chapter_title = f"ç¬¬{chapter_num}ç« "
                lines.append(f"- ç¬¬{chapter_num}ç« ï¼š{chapter_title}")
                section_topics = collect_topics(chapter.get("sections", []))
                if section_topics:
                    for topic in section_topics:
                        lines.append(f"  - {topic}")
                else:
                    chapter_summary = cleaned_main(chapter.get("chapter_summary", ""))
                    if chapter_summary:
                        lines.append(f"  - {chapter_summary}")
        else:
            lines.append("- æœ¬è®²å›´ç»•å…³é”®ä¸»é¢˜é€æ­¥å±•å¼€")
        lines.append("")

        lines.extend(["## ä¸»é¢˜è¯¦è§£", ""])
        concept_index: list[str] = []
        appendix_code_blocks: list[tuple[str, str]] = []

        for chapter_num, chapter in enumerate(chapters, 1):
            chapter_title = cleaned_main(
                chapter.get("chapter_title", f"ç¬¬{chapter_num}ç« ")
            )
            if not chapter_title:
                chapter_title = f"ç¬¬{chapter_num}ç« "
            chapter_summary = cleaned_main(chapter.get("chapter_summary", ""))
            sections = chapter.get("sections", [])

            lines.append(f"### ç¬¬{chapter_num}ç« ï¼š{chapter_title}")
            lines.append("")

            topics = collect_topics(sections)
            if chapter_summary:
                lines.append(chapter_summary)
            elif topics:
                lines.append(f"æœ¬ç« å›´ç»• {'ã€'.join(topics)} å±•å¼€ã€‚")
            else:
                lines.append(f"æœ¬ç« æ¢³ç† {chapter_title} çš„å…³é”®é—®é¢˜ä¸åº”ç”¨åœºæ™¯ã€‚")
            lines.append("")

            if chapter_title:
                concept_index.append(chapter_title)
            concept_index.extend(topics)

            stitched_sentences: list[str] = []
            seen_sentences: set[str] = set()
            for section in sections:
                explanation = cleaned_main(
                    _normalize_field_value(section.get("explanation", ""))
                )
                example = cleaned_main(
                    _normalize_field_value(section.get("example", ""))
                )
                stitched_sentences.extend(
                    collect_unique_sentences(explanation, seen_sentences)
                )
                stitched_sentences.extend(
                    collect_unique_sentences(example, seen_sentences)
                )

                code = normalize_code_block(section.get("code", ""))
                if code:
                    label = cleaned_main(section.get("topic", "")) or chapter_title
                    appendix_code_blocks.append((label, code))

            if stitched_sentences:
                lines.append("å†…å®¹ä¸²è®²ï¼š")
                lines.append("")
                for sentence in stitched_sentences[:8]:
                    lines.append(f"- {sentence}")
                lines.append("")
            else:
                lines.append("å†…å®¹ä¸²è®²ï¼š")
                lines.append("")
                lines.append(f"- æœ¬ç« èšç„¦ {chapter_title} çš„æ ¸å¿ƒé€»è¾‘ä¸è½åœ°è·¯å¾„ã€‚")
                lines.append("")

        if not chapters:
            lines.append("æœ¬è®²å†…å®¹ä»¥å…³é”®æ¦‚å¿µä¸²è”ï¼Œæš‚æ— ç« èŠ‚æ‹†åˆ†ã€‚")
            lines.append("")

        lines.extend(["## å®æˆ˜ä¸ä»£ç ", ""])
        if appendix_code_blocks:
            for idx, (label, code) in enumerate(appendix_code_blocks[:2], 1):
                lines.append(f"### ç¤ºä¾‹ {idx}ï¼š{label}")
                lines.append("")
                code_lines = [line for line in code.splitlines() if line.strip()]
                if not code_lines:
                    lines.append("æœ¬ç¤ºä¾‹ä»…ç»™å‡ºæ€è·¯ï¼Œä»£ç ç•¥ã€‚")
                    lines.append("")
                    continue

                lines.append("ä»£ç ï¼š")
                lines.append("")
                for line_num, line in enumerate(code_lines, 1):
                    lines.append(f"{line_num}. {line}")
                lines.append("")

                lines.append("é€è¡Œè¯´æ˜ï¼š")
                lines.append("")
                for line_num, line in enumerate(code_lines, 1):
                    lowered = line.lower()
                    if "fit" in lowered or "train" in lowered:
                        explanation = "æ‰§è¡Œè®­ç»ƒæˆ–æ‹Ÿåˆæ­¥éª¤ã€‚"
                    elif "predict" in lowered:
                        explanation = "è¾“å‡ºé¢„æµ‹ç»“æœä¾›åç»­è¯„ä¼°ã€‚"
                    elif "print" in lowered or "log" in lowered:
                        explanation = "æ‰“å°æˆ–è®°å½•å…³é”®ç»“æœã€‚"
                    elif "load" in lowered or "read" in lowered:
                        explanation = "åŠ è½½å¿…è¦çš„æ•°æ®æˆ–æ¨¡å‹ã€‚"
                    else:
                        explanation = "å®Œæˆå…³é”®è®¡ç®—æˆ–è°ƒç”¨æ­¥éª¤ã€‚"
                    lines.append(f"{line_num}ï¼š{explanation}")
                lines.append("")
        else:
            lines.append("æœ¬è®²æ— å¯å¤ç”¨ä»£ç ç‰‡æ®µ")
            lines.append("")

        lines.extend(["## FAQ / é¿å‘æŒ‡å—", ""])
        pitfalls: list[str] = []
        for chapter in chapters:
            for section in chapter.get("sections", []):
                for mistake in self._coerce_list(section.get("common_mistakes", [])):
                    cleaned_mistake = cleaned_main(mistake)
                    if cleaned_mistake:
                        pitfalls.append(cleaned_mistake)
        deduped_pitfalls: list[str] = []
        seen_pitfalls = set()
        for pitfall in pitfalls:
            if pitfall not in seen_pitfalls:
                seen_pitfalls.add(pitfall)
                deduped_pitfalls.append(pitfall)
        if not deduped_pitfalls:
            deduped_pitfalls = [
                "åªçœ‹è®­ç»ƒæ•ˆæœï¼Œå¿½ç•¥éªŒè¯é›†è¡¨ç°ã€‚",
                "å…³é”®å‡è®¾æœªæ£€éªŒï¼Œå¯¼è‡´ç»“è®ºåå·®ã€‚",
            ]

        lines.append("å¸¸è§å‘ï¼š")
        lines.append("")
        for pitfall in deduped_pitfalls[:6]:
            lines.append(f"- {pitfall}")
        lines.append("")

        exercises: list[tuple[str, str]] = []
        for chapter in chapters:
            raw_questions = chapter.get("chapter_self_check", [])
            if isinstance(raw_questions, list):
                for item in raw_questions:
                    if not isinstance(item, dict):
                        continue
                    question = cleaned_main(item.get("q", ""))
                    answer = cleaned_main(item.get("a", ""))
                    if question and answer:
                        exercises.append((question, answer))

        if len(exercises) < 2:
            topic_pool: list[str] = []
            for chapter in chapters:
                topic_pool.extend(collect_topics(chapter.get("sections", [])))
            for topic in topic_pool[:4]:
                question = f"ä¸ºä»€ä¹ˆ {topic} åœ¨æœ¬è®²ä¸­æ˜¯å…³é”®ç¯èŠ‚ï¼Ÿ"
                answer = f"å› ä¸º {topic} ç›´æ¥å½±å“æ ¸å¿ƒæµç¨‹çš„æ•ˆæœä¸å¯è§£é‡Šæ€§ã€‚"
                exercises.append((cleaned_main(question), cleaned_main(answer)))

        deduped_exercises: list[tuple[str, str]] = []
        seen_questions = set()
        for question, answer in exercises:
            if question in seen_questions:
                continue
            seen_questions.add(question)
            deduped_exercises.append((question, answer))

        selected_exercises = deduped_exercises[:4]
        if len(selected_exercises) < 2:
            fallback_question = "ç»“åˆæœ¬è®²å†…å®¹ï¼Œè¯´æ˜ä¸€ä¸ªå…³é”®æ¦‚å¿µçš„åº”ç”¨åœºæ™¯ã€‚"
            fallback_answer = "å¯ç”¨äºè§£å†³ä¸æ ¸å¿ƒæ¦‚å¿µç›¸å…³çš„å®é™…å»ºæ¨¡æˆ–å†³ç­–é—®é¢˜ã€‚"
            selected_exercises.append((fallback_question, fallback_answer))
        selected_exercises = selected_exercises[:4]

        lines.append("ç»ƒä¹ ä¸ç­”è§£ï¼š")
        lines.append("")
        for idx, (question, _) in enumerate(selected_exercises, 1):
            lines.append(f"{idx}. {question}")
        for _, (_, answer) in enumerate(selected_exercises, 1):
            lines.append(f"ç­”ï¼š{answer}")
        lines.append("")

        lines.extend(["## ğŸ“ é™„å½• (Appendix)", ""])
        lines.append("### å›¾è§£ï¼ˆçŸ¥è¯†è“å›¾ï¼‰")
        lines.append("")
        if image_paths:
            for idx, img_path in enumerate(image_paths):
                desc = ""
                if idx < len(self.visual_schemas):
                    desc = cleaned(self.visual_schemas[idx].description)
                label = desc if desc else f"çŸ¥è¯†è“å›¾ {idx + 1}"
                lines.append(f"**{label}**")
                lines.append("")
                lines.append(f"![{label}]({img_path})")
                lines.append("")
        elif self.visual_schemas:
            for schema in self.visual_schemas:
                description = cleaned(schema.description)
                if description:
                    lines.append(f"- {description}")
                schema_text = cleaned(schema.schema)
                if schema_text:
                    lines.append("```")
                    lines.append(schema_text)
                    lines.append("```")
                    lines.append("")
        else:
            lines.append("- æš‚æ— å›¾è§£å†…å®¹")
            lines.append("")

        lines.append("### æœ¯è¯­è¡¨ï¼ˆGlossaryï¼‰")
        lines.append("")
        if self.glossary:
            for term, definition in self.glossary.items():
                cleaned_term = cleaned(term)
                cleaned_def = cleaned(definition)
                if cleaned_term and cleaned_def:
                    lines.append(f"- **{cleaned_term}**ï¼š{cleaned_def}")
        else:
            lines.append("- æš‚æ— æœ¯è¯­è¡¥å……")
        lines.append("")

        lines.append("### æ¦‚å¿µç´¢å¼•ï¼ˆConcept Indexï¼‰")
        lines.append("")
        index_items: list[str] = []
        if self.key_takeaways:
            index_items.extend([cleaned(takeaway) for takeaway in self.key_takeaways])
        index_items.extend(concept_index)
        if self.glossary:
            index_items.extend([cleaned(key) for key in self.glossary])
        seen = set()
        for item in index_items:
            if item and item not in seen:
                seen.add(item)
                lines.append(f"- {item}")
        if not seen:
            lines.append("- æš‚æ— æ¦‚å¿µç´¢å¼•")
        lines.append("")

        lines.append("### ä»£ç ä¸ä¼ªä»£ç ")
        lines.append("")
        if appendix_code_blocks:
            for label, code in appendix_code_blocks:
                if label:
                    lines.append(f"**{label}**")
                    lines.append("")
                lines.append("```python")
                lines.append(code)
                lines.append("```")
                lines.append("")
        else:
            lines.append("- æœ¬è®²æ— å¯å¤ç”¨ä»£ç ç‰‡æ®µ")
            lines.append("")

        return "\n".join(lines)

    @staticmethod
    def _coerce_list(val: Any) -> list[Any]:
        """å°†å€¼å¼ºåˆ¶è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå®¹é”™ Gemini å¶å‘ç±»å‹åå·®ï¼‰"""
        if isinstance(val, list):
            return val
        if isinstance(val, str) and val.strip():
            return [line.strip() for line in val.split("\n") if line.strip()]
        return []

    @staticmethod
    def _format_timestamp_for_display(section: dict[str, Any]) -> str:
        """
        ä» section æå–å¹¶æ ¼å¼åŒ–æ—¶é—´æˆ³ç”¨äºæ˜¾ç¤º

        Args:
            section: åŒ…å«æ—¶é—´æˆ³ä¿¡æ¯çš„ section å­—å…¸

        Returns:
            æ ¼å¼åŒ–çš„æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œå¦‚ "(00:12:34â€“00:13:10)" æˆ– "(00:12:34)"ï¼Œ
            å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        import re

        def parse_time_value(value: Any) -> float | None:
            """è§£ææ—¶é—´å€¼ä¸ºç§’æ•°"""
            if value is None:
                return None
            if isinstance(value, (int, float)):
                num = float(value)
                if num > 1000:
                    return num / 1000.0
                return num
            raw = str(value).strip()
            if not raw:
                return None
            try:
                num = float(raw)
                if num > 1000:
                    return num / 1000.0
                return num
            except ValueError:
                pass
            if ":" in raw:
                parts = raw.split(":")
                if len(parts) == 3:
                    hours, minutes, seconds = parts
                elif len(parts) == 2:
                    hours = "0"
                    minutes, seconds = parts
                else:
                    return None
                try:
                    return int(hours) * 3600 + int(minutes) * 60 + float(seconds)
                except ValueError:
                    return None
            return None

        def parse_time_range(value: Any) -> tuple[float | None, float | None]:
            """è§£ææ—¶é—´èŒƒå›´"""
            if isinstance(value, dict):
                start = parse_time_value(
                    value.get("start") or value.get("start_time") or value.get("begin")
                )
                end = parse_time_value(
                    value.get("end") or value.get("end_time") or value.get("finish")
                )
                return start, end
            if isinstance(value, str):
                matches = re.findall(r"\d{1,2}:\d{2}:\d{2}|\d{1,2}:\d{2}", value)
                if not matches:
                    return parse_time_value(value), None
                if len(matches) == 1:
                    return parse_time_value(matches[0]), None
                start = parse_time_value(matches[0])
                end = parse_time_value(matches[1])
                return start, end
            start = parse_time_value(value)
            return start, None

        def format_seconds(seconds: float) -> str:
            """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º HH:MM:SS"""
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"

        # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„é”®æå–æ—¶é—´æˆ³
        start_time: float | None = None
        end_time: float | None = None

        # ä¼˜å…ˆæ£€æŸ¥ timestamp/time_range/timecode/time å­—æ®µ
        for key in ("timestamp", "time_range", "timecode", "time"):
            if key in section:
                start_time, end_time = parse_time_range(section.get(key))
                if start_time is not None or end_time is not None:
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥ start_time/end_time å­—æ®µ
        if start_time is None and end_time is None:
            start_time = parse_time_value(
                section.get("start_time")
                or section.get("start")
                or section.get("begin")
            )
            end_time = parse_time_value(
                section.get("end_time") or section.get("end") or section.get("finish")
            )

        # æ ¼å¼åŒ–è¾“å‡º
        if start_time is not None and end_time is not None:
            return f"({format_seconds(start_time)}â€“{format_seconds(end_time)})"
        elif start_time is not None:
            return f"({format_seconds(start_time)})"
        elif end_time is not None:
            return f"({format_seconds(end_time)})"

        return ""

    @staticmethod
    def _render_section(
        lines: list[str],
        num: int,
        section: dict[str, Any],
        self_check_mode: str,
    ) -> list[str]:
        """æ¸²æŸ“å•ä¸ªçŸ¥è¯†ç‚¹ï¼ˆv2 ä¸»åŠ¨å­¦ä¹ æ ¼å¼ä¼˜å…ˆï¼Œv1 å…œåº•ï¼‰"""
        topic = section.get("topic", "æœªçŸ¥ä¸»é¢˜")
        explanation = section.get("explanation", "")
        example = section.get("example", "")
        code = section.get("code", "")
        connections = section.get("connections", [])
        answer_lines: list[str] = []

        # æ–°å­—æ®µï¼ˆv2ï¼‰
        challenge = KnowledgeDocument._coerce_list(section.get("challenge", []))
        common_mistakes = KnowledgeDocument._coerce_list(
            section.get("common_mistakes", [])
        )
        raw_self_check = section.get("self_check", [])
        self_check: list[dict[str, str]] = []
        if isinstance(raw_self_check, list):
            for item in raw_self_check:
                if isinstance(item, dict) and "q" in item and "a" in item:
                    self_check.append(item)

        use_v2 = bool(challenge or self_check or common_mistakes)

        timestamp_str = KnowledgeDocument._format_timestamp_for_display(section)
        if timestamp_str:
            lines.append(f"#### {num}. {topic} {timestamp_str}")
        else:
            lines.append(f"#### {num}. {topic}")

        if use_v2:
            # === v2: ä¸»åŠ¨å­¦ä¹ æ ¼å¼ ===
            if challenge:
                lines.append("")
                lines.append("**ğŸ§© æŒ‘æˆ˜ï¼ˆå…ˆæƒ³ 20 ç§’å†å¾€ä¸‹çœ‹ï¼‰**ï¼š")
                for c in challenge:
                    lines.append(f"- {c}")
                lines.append("")

            if code:
                lines.append("**ğŸ’» ä»£ç å…ˆè¡Œ**ï¼š")
                lines.append("```python")
                lines.append(f"{code}")
                lines.append("```")
                lines.append("")

            if explanation:
                lines.append("**ğŸ’¡ åŸç†æ‹†è§£**ï¼š")
                lines.append(f"{explanation}")
                lines.append("")

            if example:
                lines.append("**ğŸŒ° è‡ªåŒ…å«ç¤ºä¾‹ï¼ˆè¾“å…¥ â†’ è¿‡ç¨‹ â†’ è¾“å‡ºï¼‰**ï¼š")
                lines.append(f"> {example}")
                lines.append("")

            if common_mistakes:
                lines.append("**âš ï¸ å¸¸è§è¯¯åŒº**ï¼š")
                for m in common_mistakes:
                    lines.append(f"- {m}")
                lines.append("")

            if self_check:
                lines.append("**âœ… è‡ªæµ‹ï¼ˆåšå®Œå†çœ‹ç­”æ¡ˆï¼‰**ï¼š")

                question_lines: list[str] = []
                include_answers = self_check_mode in {"static", "interactive"}

                for idx, qa in enumerate(self_check, 1):
                    label = f"Q{num}.{idx}"
                    question_text = str(qa["q"]).strip()
                    question_lines.append(f"- {label}ï¼š{question_text}")

                    if include_answers:
                        answer_lines.append(f"- {label}ï¼ˆ{topic}ï¼‰ï¼š{question_text}")
                        answer_lines.append(f"  ç­”æ¡ˆï¼š{qa['a']}")
                        answer_lines.append("")

                lines.extend(question_lines)
                lines.append("")

                if self_check_mode == "interactive" and answer_lines:
                    lines.append("<details>")
                    lines.append("<summary>ç‚¹å‡»å±•å¼€ç­”æ¡ˆ</summary>")
                    lines.append("")
                    lines.extend(answer_lines)
                    lines.append("</details>")
                    lines.append("")

            if connections:
                lines.append("**ğŸ”— å…³è”çŸ¥è¯†**ï¼š")
                for conn in connections:
                    lines.append(f"- {conn}")
                lines.append("")
        else:
            # === v1: æ—§æ ¼å¼å…œåº•ï¼ˆå‘åå…¼å®¹ï¼‰ ===
            lines.append("**ğŸ’¡ åŸç†è§£æ**ï¼š")
            lines.append(f"{explanation}")
            lines.append("")
            if example:
                lines.append("**ğŸŒ° ä¸¾ä¸ªæ —å­**ï¼š")
                lines.append(f"> {example}")
                lines.append("")
            if code:
                lines.append("**ğŸ’» ä»£ç æ¼”ç¤º**ï¼š")
                lines.append("```python")
                lines.append(f"{code}")
                lines.append("```")
                lines.append("")
            if connections:
                lines.append("**ğŸ”— å…³è”çŸ¥è¯†**ï¼š")
                for conn in connections:
                    lines.append(f"- {conn}")
                lines.append("")

        if self_check_mode == "static":
            return answer_lines

        return []

    @staticmethod
    def _render_section_compact(
        lines: list[str],
        num: int,
        section: dict[str, Any],
    ) -> None:
        topic = section.get("topic", "æœªçŸ¥ä¸»é¢˜")
        explanation = section.get("explanation", "")
        example = section.get("example", "")

        timestamp_str = KnowledgeDocument._format_timestamp_for_display(section)
        if timestamp_str:
            lines.append(f"#### {num}. {topic} {timestamp_str}")
        else:
            lines.append(f"#### {num}. {topic}")
        lines.append("")
        if explanation:
            lines.append("**ğŸ’¡ åŸç†è§£æ**ï¼š")
            lines.append(f"{explanation}")
            lines.append("")
        if example:
            lines.append("**ğŸŒ° ç¤ºä¾‹**ï¼š")
            lines.append(f"> {example}")
            lines.append("")

    @staticmethod
    def _render_section_appendix(
        num: int,
        section: dict[str, Any],
    ) -> list[str]:
        lines: list[str] = []
        topic = section.get("topic", "æœªçŸ¥ä¸»é¢˜")
        explanation = section.get("explanation", "")
        example = section.get("example", "")
        code = section.get("code", "")
        connections = section.get("connections", [])
        common_mistakes = KnowledgeDocument._coerce_list(
            section.get("common_mistakes", [])
        )

        timestamp_str = KnowledgeDocument._format_timestamp_for_display(section)
        if timestamp_str:
            lines.append(f"#### {num}. {topic} {timestamp_str}")
        else:
            lines.append(f"#### {num}. {topic}")
        lines.append("")

        if explanation:
            lines.append("**ğŸ’¡ åŸç†æ‹†è§£**ï¼š")
            lines.append(f"{explanation}")
            lines.append("")
        if example:
            lines.append("**ğŸŒ° è‡ªåŒ…å«ç¤ºä¾‹**ï¼š")
            lines.append(f"> {example}")
            lines.append("")
        if code:
            lines.append("**ğŸ’» å®Œæ•´ä»£ç **ï¼š")
            lines.append("```python")
            lines.append(f"{code}")
            lines.append("```")
            lines.append("")
        if common_mistakes:
            lines.append("**âš ï¸ å¸¸è§è¯¯åŒº**ï¼š")
            for mistake in common_mistakes:
                lines.append(f"- {mistake}")
            lines.append("")
        if connections:
            lines.append("**ğŸ”— å…³è”çŸ¥è¯†**ï¼š")
            for conn in connections:
                lines.append(f"- {conn}")
            lines.append("")

        return lines

    @staticmethod
    def _normalize_self_check_mode(mode: str) -> str:
        normalized = (mode or "").strip().lower()
        if normalized in {
            "static",
            "interactive",
            "questions_only",
            "default",
            "lecture",
        }:
            return normalized
        return "static"

    @staticmethod
    def _normalize_chapters(deep_dive: list[dict[str, Any]]) -> list[dict[str, Any]]:
        if any("chapter_title" in item for item in deep_dive):
            return deep_dive
        return [
            {
                "chapter_title": "æ ¸å¿ƒè¦ç‚¹",
                "chapter_summary": "",
                "sections": deep_dive,
            }
        ]

    @staticmethod
    def _sanitize_lecture_text(text: Any) -> str:
        if text is None:
            return ""
        if not isinstance(text, str):
            text = str(text)

        patterns = [
            r"\$[^$]+\$",
            r"\\\([^)]+\\\)",
            r"\\\[[^\]]+\\\]",
            r"<details>",
            r"</details>",
            r"<summary>",
            r"</summary>",
            r"\b\d{1,2}:\d{2}\b",
            r"\(\d{1,2}:\d{2}[â€“â€”-]\d{1,2}:\d{2}\)",
        ]

        cleaned_lines: list[str] = []
        for line in text.splitlines():
            cleaned = line
            for pattern in patterns:
                cleaned = re.sub(pattern, "", cleaned)
            cleaned = " ".join(cleaned.split())
            if cleaned:
                cleaned_lines.append(cleaned)

        return "\n".join(cleaned_lines).strip()

    @staticmethod
    def _render_chapter_exercises(
        chapter: dict[str, Any],
        chapter_title: str,
        topics: list[str],
    ) -> tuple[list[str], list[str]]:
        raw_questions = chapter.get("chapter_self_check", [])
        questions: list[str] = []
        answers: list[str] = []

        if isinstance(raw_questions, list):
            for item in raw_questions:
                if isinstance(item, dict) and "q" in item and "a" in item:
                    q = KnowledgeDocument._sanitize_lecture_text(item.get("q", ""))
                    a = KnowledgeDocument._sanitize_lecture_text(item.get("a", ""))
                    if q and a:
                        questions.append(q)
                        answers.append(a)

        fallback_pairs = KnowledgeDocument._generate_fallback_exercises(
            chapter_title, topics
        )
        while len(questions) < 3 and fallback_pairs:
            q, a = fallback_pairs.pop(0)
            questions.append(q)
            answers.append(a)

        if not questions:
            questions.append(f"ç”¨ä¸€å¥è¯æ¦‚æ‹¬ {chapter_title} çš„æ ¸å¿ƒä¸»é¢˜ã€‚")
            answers.append(f"æ ¸å¿ƒä¸»é¢˜æ˜¯ï¼š{chapter_title}ã€‚")

        question_lines = [f"{idx}. {q}" for idx, q in enumerate(questions, 1)]
        answer_lines = [f"{idx}. {a}" for idx, a in enumerate(answers, 1)]

        return question_lines, answer_lines

    @staticmethod
    def _generate_fallback_exercises(
        chapter_title: str,
        topics: list[str],
    ) -> list[tuple[str, str]]:
        topic_hint = "ã€".join(topics[:2]) if topics else chapter_title
        return [
            (
                "ç”¨ä¸€å¥è¯æ¦‚æ‹¬æœ¬ç« æ ¸å¿ƒä¸»é¢˜ã€‚",
                f"æœ¬ç« æ ¸å¿ƒä¸»é¢˜æ˜¯ï¼š{chapter_title}ã€‚",
            ),
            (
                "åˆ—å‡ºæœ¬ç« æ¶‰åŠçš„ä¸¤ä¸ªå…³é”®æ¦‚å¿µã€‚",
                f"å…³é”®æ¦‚å¿µåŒ…æ‹¬ï¼š{topic_hint}ã€‚",
            ),
            (
                "ç»™å‡ºä¸€ä¸ªæœ¬ç« çš„åº”ç”¨åœºæ™¯ã€‚",
                f"å¯ç”¨äºä¸ {topic_hint} ç›¸å…³çš„å®é™…å»ºæ¨¡ä¸è¯„ä¼°ä»»åŠ¡ã€‚",
            ),
            (
                "è¯´æ˜ä¸€ä¸ªå¸¸è§è¯¯åŒºå¹¶ç»™å‡ºæ”¹è¿›æ–¹å‘ã€‚",
                "å¸¸è§è¯¯åŒºæ˜¯åªçœ‹è®­ç»ƒé›†æŒ‡æ ‡ï¼Œæ”¹è¿›æ–¹å‘æ˜¯åŠ å…¥éªŒè¯é›†ç›‘æ§ã€‚",
            ),
        ]


def _format_bulleted_item(text: str) -> list[str]:
    lines = [line for line in text.splitlines() if line.strip()]
    if not lines:
        return []
    formatted = [f"- {lines[0]}"]
    for line in lines[1:]:
        formatted.append(f"  {line}")
    return formatted


def _format_ordered_list(items: list[str]) -> list[str]:
    lines: list[str] = []
    for index, item in enumerate(items, 1):
        item_lines = [line for line in item.splitlines() if line.strip()]
        if not item_lines:
            continue
        lines.append(f"{index}. {item_lines[0]}")
        for line in item_lines[1:]:
            lines.append(f"   {line}")
    return lines


def _normalize_steps_value(value: object) -> list[str]:
    if value is None:
        return []
    if isinstance(value, list):
        steps: list[str] = []
        for item in value:
            normalized = _normalize_field_value(item)
            if normalized:
                steps.append(normalized)
        return steps
    if isinstance(value, str):
        return [line.strip() for line in value.splitlines() if line.strip()]
    normalized = _normalize_field_value(value)
    return [normalized] if normalized else []


def _normalize_field_value(value: object) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, dict):
        if not value:
            return ""
        io_keys = {"input", "steps", "output"}
        has_io_keys = any(key in value for key in io_keys)
        if has_io_keys:
            parts: list[str] = []
            input_value = _normalize_field_value(value.get("input"))
            if input_value:
                parts.append(f"è¾“å…¥ï¼š{input_value}")
            steps_value = _normalize_steps_value(value.get("steps"))
            if steps_value:
                parts.append("æ­¥éª¤ï¼š")
                parts.extend(_format_ordered_list(steps_value))
            output_value = _normalize_field_value(value.get("output"))
            if output_value:
                parts.append(f"è¾“å‡ºï¼š{output_value}")

            extra_keys = sorted(key for key in value.keys() if key not in io_keys)
            if extra_keys:
                parts.append("å…¶ä»–ï¼š")
                for key in extra_keys:
                    normalized = _normalize_field_value(value.get(key))
                    label = f"{key}: {normalized}" if normalized else f"{key}:"
                    parts.extend(_format_bulleted_item(label))
            return "\n".join(parts).strip()

        lines: list[str] = []
        for key in sorted(value.keys()):
            normalized = _normalize_field_value(value.get(key))
            label = f"{key}: {normalized}" if normalized else f"{key}:"
            lines.extend(_format_bulleted_item(label))
        return "\n".join(lines).strip()

    if isinstance(value, list):
        lines: list[str] = []
        for item in value:
            normalized = _normalize_field_value(item)
            if not normalized:
                continue
            item_lines = [line for line in normalized.splitlines() if line.strip()]
            if not item_lines:
                continue
            if all(line.startswith("- ") for line in item_lines):
                lines.extend(item_lines)
            else:
                lines.extend(_format_bulleted_item("\n".join(item_lines)))
        return "\n".join(lines).strip()

    return str(value).strip()


def _normalize_list_field(value: object) -> list[str]:
    if value is None:
        return []
    if isinstance(value, list):
        items: list[str] = []
        for item in value:
            normalized = _normalize_field_value(item)
            if not normalized:
                continue
            lines = [line.strip() for line in normalized.splitlines() if line.strip()]
            if lines and all(line.startswith("- ") for line in lines):
                items.extend([line[2:].strip() for line in lines])
            else:
                items.append("\n".join(lines))
        return items
    if isinstance(value, str):
        return [line.strip() for line in value.splitlines() if line.strip()]
    normalized = _normalize_field_value(value)
    return [normalized] if normalized else []


def _normalize_section_fields(section: dict[str, object]) -> dict[str, object]:
    normalized = dict(section)
    normalized["explanation"] = _normalize_field_value(section.get("explanation"))
    normalized["example"] = _normalize_field_value(section.get("example"))
    normalized["code"] = _normalize_field_value(section.get("code"))
    normalized["common_mistakes"] = _normalize_list_field(
        section.get("common_mistakes")
    )
    normalized["connections"] = _normalize_list_field(section.get("connections"))
    return normalized


def _normalize_deep_dive(deep_dive: object) -> list[dict[str, object]]:
    if not isinstance(deep_dive, list):
        return []
    normalized: list[dict[str, object]] = []
    for item in deep_dive:
        if not isinstance(item, dict):
            continue
        if "sections" in item and isinstance(item.get("sections"), list):
            chapter = dict(item)
            chapter_sections: list[dict[str, object]] = []
            for section in item.get("sections", []):
                if isinstance(section, dict):
                    chapter_sections.append(_normalize_section_fields(section))
            chapter["sections"] = chapter_sections
            normalized.append(chapter)
        else:
            normalized.append(_normalize_section_fields(item))
    return normalized


@dataclass
class AnalysisResult:
    """è§†é¢‘åˆ†æç»“æœçš„å®Œæ•´æ•°æ®ç»“æ„"""

    video_path: str | Path
    """è§†é¢‘æ–‡ä»¶è·¯å¾„"""

    knowledge_doc: KnowledgeDocument
    """çŸ¥è¯†ç¬”è®°å¯¹è±¡"""

    metadata: dict[str, Any] = field(default_factory=dict)
    """å…ƒæ•°æ®ï¼ˆè§†é¢‘æ ‡é¢˜ã€æ—¶é•¿ç­‰ï¼‰"""

    @property
    def title(self) -> str:
        """è·å–æ–‡æ¡£æ ‡é¢˜"""
        return self.knowledge_doc.title

    @property
    def glossary(self) -> dict[str, str]:
        """è·å–æœ¯è¯­è¡¨"""
        return self.knowledge_doc.glossary

    def to_markdown(
        self,
        image_paths: list[str] | None = None,
        self_check_mode: str = "static",
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„ Markdown æ–‡æ¡£

        Args:
            image_paths: çŸ¥è¯†è“å›¾å›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„åˆ—è¡¨(å¯é€‰)
            self_check_mode: è‡ªæµ‹é¢˜æ¸²æŸ“æ¨¡å¼(static/interactive/questions_only)

        Returns:
            åŒ…å«çŸ¥è¯†ç¬”è®°å’ŒçŸ¥è¯†è“å›¾ç»“æ„çš„å®Œæ•´ Markdown æ–‡æ¡£
        """
        markdown = self.knowledge_doc.to_markdown(
            image_paths=image_paths,
            self_check_mode=self_check_mode,
        )

        normalized_mode = (self_check_mode or "").strip().lower()
        if normalized_mode not in {
            "static",
            "interactive",
            "questions_only",
            "default",
        }:
            normalized_mode = "static"
        errors: list[str] = []

        if detect_stub_output(markdown):
            errors.append("æ£€æµ‹åˆ°ç–‘ä¼¼å ä½/ç©ºå†…å®¹è¾“å‡º")

        _, structure_errors = validate_markdown_structure(markdown, normalized_mode)
        errors.extend(structure_errors)

        if errors:
            message = "Markdown æ ¡éªŒå¤±è´¥: " + "; ".join(errors)
            if normalized_mode == "default":
                raise ValueError(message)
            warnings.warn(f"Markdown æ ¡éªŒè­¦å‘Š(legacy æ¨¡å¼): {message}")

        return markdown

    @classmethod
    def from_api_response(
        cls,
        video_path: str | Path,
        response_data: dict[str, Any],
        metadata: dict[str, Any] | None = None,
    ) -> AnalysisResult:
        """
        ä» API å“åº”æ•°æ®æ„å»º AnalysisResult å¯¹è±¡

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            response_data: API è¿”å›çš„ JSON æ•°æ®
            metadata: å¯é€‰çš„å…ƒæ•°æ®

        Returns:
            AnalysisResult å¯¹è±¡

        Raises:
            ValueError: å¦‚æœå“åº”æ•°æ®æ ¼å¼ä¸æ­£ç¡®
        """
        # æ ¸å¿ƒå­—æ®µï¼šç¼ºå¤±åˆ™æ— æ³•æ„å»ºæœ‰æ„ä¹‰çš„æ–‡æ¡£ï¼Œå¿…é¡»ä¸¥æ ¼æ ¡éªŒ
        critical_fields = {
            "title",
            "one_sentence_summary",
            "key_takeaways",
            "deep_dive",
        }
        # å¯é€‰å­—æ®µ
        optional_defaults: dict[str, Any] = {
            "glossary": {},
        }

        missing_critical = critical_fields - response_data.keys()
        if missing_critical:
            raise ValueError(
                f"API å“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(sorted(missing_critical))}"
            )

        # è§£æ visual_schemasï¼ˆæ”¯æŒæ–°æ ¼å¼æ•°ç»„å’Œæ—§æ ¼å¼å•å­—ç¬¦ä¸²ï¼‰
        visual_schemas: list[VisualSchemaItem] = []
        raw_schemas = response_data.get("visual_schemas", [])
        if isinstance(raw_schemas, list) and len(raw_schemas) > 0:
            for item in raw_schemas:
                if isinstance(item, dict):
                    visual_schemas.append(
                        VisualSchemaItem(
                            type=item.get("type", "overview"),
                            description=item.get("description", ""),
                            schema=item.get("schema", ""),
                        )
                    )
                elif isinstance(item, str):
                    visual_schemas.append(
                        VisualSchemaItem(
                            type="overview",
                            description="",
                            schema=item,
                        )
                    )
        else:
            # å…¼å®¹æ—§æ ¼å¼: visual_schema å•å­—ç¬¦ä¸²
            old_schema = response_data.get("visual_schema", "")
            if old_schema:
                visual_schemas.append(
                    VisualSchemaItem(
                        type="overview",
                        description="æ€»è§ˆçŸ¥è¯†å¯¼å›¾",
                        schema=old_schema,
                    )
                )

        knowledge_doc = KnowledgeDocument(
            title=response_data["title"],
            one_sentence_summary=response_data["one_sentence_summary"],
            key_takeaways=response_data["key_takeaways"],
            deep_dive=_normalize_deep_dive(response_data["deep_dive"]),
            glossary=response_data.get("glossary", optional_defaults["glossary"]),
            visual_schemas=visual_schemas,
        )

        return cls(
            video_path=video_path,
            knowledge_doc=knowledge_doc,
            metadata=metadata or {},
        )
