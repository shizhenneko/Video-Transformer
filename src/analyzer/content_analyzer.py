"""
å†…å®¹åˆ†æä¸æ–‡æ¡£ç”Ÿæˆæ¨¡å—

ä½¿ç”¨ Gemini 2.5 Flash å¯¹è§†é¢‘è¿›è¡Œå¤šæ¨¡æ€åˆ†æï¼Œç”Ÿæˆä¸­æ–‡ç²¾è‹±çŸ¥è¯†ç¬”è®°ã€‚
é€šè¿‡ä»£ç†å·æ± æœåŠ¡çš„ /sdk/allocate-key API è·å–çœŸå® API Keyï¼Œ
ç›´æ¥ä½¿ç”¨ google-genai SDK è°ƒç”¨ Gemini APIã€‚
"""

from __future__ import annotations

import re
import json
import logging
import subprocess
import tempfile
import time
from pathlib import Path
from types import SimpleNamespace
from typing import Any, cast

import google.genai as genai  # type: ignore[reportMissingImports]
from google.genai import types  # type: ignore[reportMissingImports]
import requests

from utils.counter import APICounter, APILimitExceeded
from utils.gemini_throttle import GeminiThrottle
from utils.budget_planner import SegmentPlan, plan_segments_with_budget
from utils.video_segmenter import (
    extract_segment,
    load_or_create_manifest,
    save_manifest,
    update_segment_status,
)
from utils.video_utils import probe_duration
from analyzer.models import AnalysisResult
from analyzer.prompt_loader import load_prompts, render_prompt


class ContentAnalyzer:
    """
    è§†é¢‘å†…å®¹åˆ†æå™¨

    è´Ÿè´£ä½¿ç”¨ Gemini 2.5 Flash åˆ†æè§†é¢‘å†…å®¹ï¼Œç”Ÿæˆç²¾è‹±çŸ¥è¯†ç¬”è®°ã€æœ¯è¯­è¡¨å’ŒçŸ¥è¯†è“å›¾ç»“æ„ã€‚
    é€šè¿‡ä»£ç†å·æ± æœåŠ¡åˆ†é…çœŸå® API Keyï¼ŒSDK ç›´è¿ Google APIã€‚
    """

    def __init__(
        self,
        config: dict[str, Any],
        api_counter: APICounter,
        logger: logging.Logger,
        throttle: GeminiThrottle,
        api_key: str | None = None,
    ):
        """
        åˆå§‹åŒ–å†…å®¹åˆ†æå™¨

        Args:
            config: ç³»ç»Ÿé…ç½®å­—å…¸
            api_counter: API è°ƒç”¨è®¡æ•°å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            api_key: Gemini API å¯†é’¥ï¼ˆå¯é€‰ï¼Œè‹¥æä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼Œä¸èµ°ä»£ç†å·æ± ï¼‰
        """
        self.config = config
        self.api_counter = api_counter
        self.logger = logger

        # åŠ è½½åˆ†æå™¨é…ç½®
        self.analyzer_config = config.get("analyzer", {})
        self.model_name = self.analyzer_config.get("model", "gemini-2.5-flash")
        self.temperature = self.analyzer_config.get("temperature", 0.7)
        self.max_output_tokens = self.analyzer_config.get("max_output_tokens", 65536)
        self.timeout = self.analyzer_config.get("timeout", 120)
        self.max_continuations = self.analyzer_config.get("max_continuations", 3)

        # é™æµå™¨
        self.throttle = throttle

        proxy_config = config.get("proxy", {})
        self.proxy_base_url = proxy_config.get("base_url", "http://localhost:8000")
        self.proxy_timeout = proxy_config.get("timeout", 10)

        self._fixed_api_key = api_key
        self._allocated_key_id = None  # å¦‚æœæ˜¯ä»å¤–éƒ¨ä¼ å…¥ä¸”å·²çŸ¥ ID,å¯ä»¥æ‰©å±•æ¥å£ä¼ è¾“ ID
        self._allocated_api_key = api_key
        self._client: genai.Client | None = None
        self._llm_repair_used = False

        http_proxy = proxy_config.get("http")

        if http_proxy:
            import os

            os.environ["HTTP_PROXY"] = http_proxy
            os.environ["HTTPS_PROXY"] = http_proxy
            # ç¡®ä¿æœ¬åœ°æœåŠ¡ä¸èµ°ä»£ç†
            os.environ["NO_PROXY"] = "localhost,127.0.0.1"
            self.logger.info(f"å·²è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡: {http_proxy}")

        if self._allocated_api_key:
            self._client = genai.Client(
                api_key=self._allocated_api_key,
                http_options={"timeout": 600_000},
            )
            self.logger.info("Gemini SDK é…ç½®å®Œæˆ(ä½¿ç”¨å¤–éƒ¨åˆ†é…çš„ API Key)")
        else:
            self.logger.warning("æœªæä¾› Gemini API Key,ContentAnalyzer å°†æ— æ³•æ­£å¸¸å·¥ä½œ")

        # åŠ è½½ Prompt æ¨¡æ¿
        self.prompts = load_prompts()
        self.logger.info("ContentAnalyzer åˆå§‹åŒ–å®Œæˆ")

    # _allocate_key_from_pool å·²ç§»é™¤,å¯†é’¥åˆ†é…é€»è¾‘å·²ç§»è‡³ VideoPipeline

    def _report_usage_to_pool(self) -> None:
        """å‘ä»£ç†å·æ± æŠ¥å‘Šä¸€æ¬¡æˆåŠŸçš„ API è°ƒç”¨ã€‚"""
        if not self._allocated_key_id:
            return
        url = f"{self.proxy_base_url.rstrip('/')}/sdk/report-usage"
        try:
            requests.post(
                url,
                json={"key_id": self._allocated_key_id},
                timeout=self.proxy_timeout,
            )
        except requests.RequestException as e:
            self.logger.warning(f"å‘å·æ± æŠ¥å‘Šç”¨é‡å¤±è´¥: {e}")

    def _report_error_to_pool(self, is_rpd_limit: bool = False) -> None:
        """å‘ä»£ç†å·æ± æŠ¥å‘Š API è°ƒç”¨é”™è¯¯ã€‚"""
        if not self._allocated_key_id:
            return
        url = f"{self.proxy_base_url.rstrip('/')}/sdk/report-error"
        try:
            requests.post(
                url,
                json={
                    "key_id": self._allocated_key_id,
                    "is_rpd_limit": is_rpd_limit,
                },
                timeout=self.proxy_timeout,
            )
        except requests.RequestException as e:
            self.logger.warning(f"å‘å·æ± æŠ¥å‘Šé”™è¯¯å¤±è´¥: {e}")

    @staticmethod
    def _classify_429_is_daily(exc: Exception) -> bool:
        """ä»…åœ¨å¼‚å¸¸æ˜ç¡®æç¤ºæ¯æ—¥é…é¢è€—å°½æ—¶è¿”å› Trueã€‚"""
        message = str(exc).lower()
        if not message:
            return False
        daily_markers = ("per day", "daily", "quota exceeded per day")
        return any(marker in message for marker in daily_markers)

    def _delete_remote_file(self, file_name: str) -> None:
        """åˆ é™¤ Gemini Files å­˜å‚¨ä¸­çš„è¿œç¨‹æ–‡ä»¶ï¼Œé‡Šæ”¾é…é¢ç©ºé—´ã€‚"""
        if not self._client:
            return
        try:
            self.throttle.wait_for_files_op()
            self._client.files.delete(name=file_name)
            self.logger.info(f"å·²æ¸…ç† Gemini è¿œç¨‹æ–‡ä»¶: {file_name}")
        except Exception as e:
            self.logger.warning(f"æ¸…ç† Gemini è¿œç¨‹æ–‡ä»¶å¤±è´¥: {e}")

    def _compress_video_for_upload(self, video_path: Path) -> Path:
        """ç”¨ ffmpeg å‹ç¼©è§†é¢‘ä»¥å‡å°ä¸Šä¼ ä½“ç§¯ï¼Œè¿”å›å‹ç¼©åçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„ã€‚

        å‹ç¼©ç­–ç•¥: 360p + CRF 28ï¼Œå¯¹ Gemini å†…å®¹åˆ†æè¶³å¤Ÿï¼Œä½“ç§¯å¯é™è‡³åŸæ¥çš„ 1/5~1/10ã€‚
        å¦‚æœ ffmpeg ä¸å¯ç”¨æˆ–å‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€‚
        """
        max_size_mb = 30
        file_size_mb = video_path.stat().st_size / (1024 * 1024)
        if file_size_mb <= max_size_mb:
            self.logger.info(
                f"è§†é¢‘ä½“ç§¯ {file_size_mb:.1f}MB <= {max_size_mb}MBï¼Œè·³è¿‡å‹ç¼©"
            )
            return video_path

        self.logger.info(
            f"è§†é¢‘ä½“ç§¯ {file_size_mb:.1f}MB > {max_size_mb}MBï¼Œå¼€å§‹ ffmpeg å‹ç¼©..."
        )

        compressed_path = video_path.parent / f"compressed_{video_path.name}"

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‹ç¼©æ–‡ä»¶
        if compressed_path.exists() and compressed_path.stat().st_size > 0:
            self.logger.info(f"å‘ç°å·²å­˜åœ¨çš„å‹ç¼©æ–‡ä»¶: {compressed_path.name}ï¼Œè·³è¿‡å‹ç¼©")
            return compressed_path

        try:
            cmd = [
                "ffmpeg",
                "-y",
                "-i",
                str(video_path),
                "-vf",
                "scale=-2:360",
                "-c:v",
                "libx264",
                "-crf",
                "28",
                "-preset",
                "fast",
                "-c:a",
                "aac",
                "-b:a",
                "64k",
                str(compressed_path),
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,
            )
            if result.returncode != 0:
                self.logger.warning(f"ffmpeg å‹ç¼©å¤±è´¥: {(result.stderr or '')[:200]}")
                return video_path

            new_size_mb = compressed_path.stat().st_size / (1024 * 1024)
            self.logger.info(
                f"å‹ç¼©å®Œæˆ: {file_size_mb:.1f}MB -> {new_size_mb:.1f}MB "
                f"(å‹ç¼©ç‡ {new_size_mb / file_size_mb * 100:.0f}%)"
            )
            return compressed_path

        except FileNotFoundError:
            self.logger.warning("ffmpeg æœªå®‰è£…ï¼Œè·³è¿‡å‹ç¼©")
            return video_path
        except subprocess.TimeoutExpired:
            self.logger.warning("ffmpeg å‹ç¼©è¶…æ—¶(5åˆ†é’Ÿ)ï¼Œè·³è¿‡å‹ç¼©")
            if compressed_path.exists():
                compressed_path.unlink()
            return video_path

    def _upload_video(self, video_path: Path) -> Any:
        """
        å¤„ç†è§†é¢‘å‹ç¼©å’Œä¸Šä¼ 

        Returns:
             Gemini File å¯¹è±¡
        """
        upload_path = self._compress_video_for_upload(video_path)
        video_file = None
        key_id = self._allocated_key_id or "unknown"

        try:
            if not self._client:
                raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")
            self.logger.info(f"ä¸Šä¼ è§†é¢‘æ–‡ä»¶: {upload_path.name}")
            self.logger.info(
                f"API call | op=files.upload | key_id={key_id} | file={upload_path.name}"
            )
            self.throttle.wait_for_files_op()
            video_file = self._client.files.upload(file=str(upload_path))
            if video_file is None or not getattr(video_file, "name", None):
                raise RuntimeError("Gemini æ–‡ä»¶ä¸Šä¼ æœªè¿”å›æœ‰æ•ˆæ–‡ä»¶å")
            video_file = cast(Any, video_file)
            video_file_name = cast(str, video_file.name)
            self.logger.info(
                f"API call complete | op=files.upload | key_id={key_id} "
                + f"| file={upload_path.name} | name={video_file_name}"
            )

            wait_time = 3
            max_wait_time = 30
            max_total_wait = 300
            start_time = time.monotonic()

            while video_file.state.name == "PROCESSING":
                elapsed = time.monotonic() - start_time
                remaining = max_total_wait - elapsed
                if remaining <= 0:
                    raise TimeoutError("è§†é¢‘å¤„ç†è¶…æ—¶(5åˆ†é’Ÿ)ï¼Œåœæ­¢ç­‰å¾…")

                sleep_time = min(wait_time, remaining)
                self.logger.info(f"ç­‰å¾…è§†é¢‘å¤„ç†... (ä¸‹æ¬¡æ£€æŸ¥: {sleep_time:.1f}s)")
                time.sleep(sleep_time)
                self.throttle.wait_for_files_op()
                self.logger.debug(
                    f"API call | op=files.get | key_id={key_id} | file={video_file_name}"
                )
                video_file = self._client.files.get(name=video_file_name)
                video_file = cast(Any, video_file)
                self.logger.info(
                    f"API call complete | op=files.get | key_id={key_id} "
                    + f"| name={video_file_name} | state={video_file.state.name}"
                )
                wait_time = min(wait_time * 2, max_wait_time)

            if video_file.state.name == "FAILED":
                raise RuntimeError(f"è§†é¢‘æ–‡ä»¶å¤„ç†å¤±è´¥: {video_file.state.name}")

            self.logger.info(f"è§†é¢‘æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {video_file.name}")
            return video_file

        except Exception as e:
            self.logger.error(f"è§†é¢‘ä¸Šä¼ å¤±è´¥: {e}")
            # å¦‚æœä¸Šä¼ å¤±è´¥ï¼Œæ¸…ç†è¿œç¨‹æ–‡ä»¶ï¼ˆå¦‚æœå·²åˆ›å»ºï¼‰
            if video_file:
                video_file_name = getattr(video_file, "name", None)
                if video_file_name:
                    self._delete_remote_file(video_file_name)
            raise

    @staticmethod
    def _format_timecode(seconds: float) -> str:
        total_seconds = max(int(seconds), 0)
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        secs = total_seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    @staticmethod
    def _coerce_list(value: Any) -> list[str]:
        if isinstance(value, list):
            items = [str(item).strip() for item in value]
            return [item for item in items if item]
        if isinstance(value, str) and value.strip():
            return [item.strip() for item in value.split("\n") if item.strip()]
        return []

    @staticmethod
    def _normalize_text(value: str) -> str:
        return re.sub(r"\s+", " ", value).strip().lower()

    @classmethod
    def _section_signature(cls, section: dict[str, Any]) -> str:
        topic = cls._normalize_text(str(section.get("topic", "")))
        explanation = cls._normalize_text(str(section.get("explanation", "")))
        return f"{topic}|{explanation}"

    @classmethod
    def _parse_time_value(cls, value: Any) -> float | None:
        if value is None:
            return None
        if isinstance(value, (int, float)):
            num = float(value)
            if num > 1000:
                return num / 1000.0
            return num
        if not isinstance(value, str):
            return None

        raw = value.strip()
        if not raw:
            return None
        if raw.isdigit():
            num = float(raw)
            if num > 1000:
                return num / 1000.0
            return num
        if ":" in raw:
            parts = raw.split(":")
            if len(parts) == 3:
                hours, minutes, seconds = parts
            elif len(parts) == 2:
                hours = "0"
                minutes, seconds = parts
            else:
                return None
            try:
                return int(hours) * 3600 + int(minutes) * 60 + float(seconds)
            except ValueError:
                return None
        return None

    @classmethod
    def _extract_time_range_from_text(
        cls, text: str
    ) -> tuple[float | None, float | None]:
        matches = re.findall(r"\d{1,2}:\d{2}:\d{2}|\d{1,2}:\d{2}", text)
        if not matches:
            return None, None
        if len(matches) == 1:
            return cls._parse_time_value(matches[0]), None
        start = cls._parse_time_value(matches[0])
        end = cls._parse_time_value(matches[1])
        return start, end

    @classmethod
    def _parse_time_range(cls, value: Any) -> tuple[float | None, float | None]:
        if isinstance(value, dict):
            start = cls._parse_time_value(
                value.get("start") or value.get("start_time") or value.get("begin")
            )
            end = cls._parse_time_value(
                value.get("end") or value.get("end_time") or value.get("finish")
            )
            return start, end
        if isinstance(value, str):
            return cls._extract_time_range_from_text(value)
        start = cls._parse_time_value(value)
        return start, None

    @classmethod
    def _extract_section_time_range(
        cls,
        section: dict[str, Any],
        fallback_range: tuple[float | None, float | None] | None = None,
    ) -> tuple[float | None, float | None]:
        for key in ("timestamp", "time_range", "timecode", "time"):
            if key in section:
                start, end = cls._parse_time_range(section.get(key))
                if start is not None or end is not None:
                    return start, end

        start = cls._parse_time_value(
            section.get("start_time") or section.get("start") or section.get("begin")
        )
        end = cls._parse_time_value(
            section.get("end_time") or section.get("end") or section.get("finish")
        )
        if start is not None or end is not None:
            return start, end

        if fallback_range is not None:
            return fallback_range
        return None, None

    @classmethod
    def _normalize_chapters(cls, deep_dive: Any) -> list[dict[str, Any]]:
        if not isinstance(deep_dive, list):
            return []
        has_chapter = any(
            isinstance(item, dict) and ("chapter_title" in item or "sections" in item)
            for item in deep_dive
        )
        if has_chapter:
            return [item for item in deep_dive if isinstance(item, dict)]
        sections = [item for item in deep_dive if isinstance(item, dict)]
        if not sections:
            return []
        return [
            {
                "chapter_title": "æ ¸å¿ƒè¦ç‚¹",
                "chapter_summary": "",
                "sections": sections,
            }
        ]

    def _build_segment_prompt_parts(
        self, segment_index: int, total_segments: int, start: float, end: float
    ) -> list[str]:
        start_code = self._format_timecode(start)
        end_code = self._format_timecode(end)
        segment_context = (
            f"This is segment {segment_index} of {total_segments}, "
            f"covering time range {start_code} to {end_code}. "
            "Output timestamps as absolute video time (HH:MM:SS or milliseconds). "
            "Structure deep_dive as a single chapter with chapter_title indicating the time range."
        )
        return [segment_context]

    @staticmethod
    def _coerce_float(value: object) -> float | None:
        if isinstance(value, (int, float, str)):
            try:
                return float(value)
            except ValueError:
                return None
        return None

    def _should_use_segmentation(
        self, duration: float, plan: SegmentPlan, long_video_config: dict[str, Any]
    ) -> bool:
        if duration <= 0:
            return False
        if not long_video_config.get("enabled", True):
            return False
        threshold = self._coerce_float(
            long_video_config.get("duration_threshold_seconds")
        )
        if threshold is not None and duration >= threshold:
            return True
        return plan.num_segments > 1

    def _call_analysis_json(
        self,
        video_file: Any,
        system_role: str,
        main_prompt: str,
        extra_text_parts: list[str] | None = None,
    ) -> dict[str, Any]:
        reported_retry = False

        def _on_retry(attempt: int, exc: Exception) -> None:
            nonlocal reported_retry
            if reported_retry:
                return
            reported_retry = True
            is_daily_limit = self._classify_429_is_daily(exc)
            self._report_error_to_pool(is_rpd_limit=is_daily_limit)

        json_parse_max_retries = 2
        response_data = None

        for json_attempt in range(1, json_parse_max_retries + 2):
            try:
                reported_retry = False
                response_data = self.throttle.call_with_retry(
                    self._generate_content,
                    video_file,
                    system_role,
                    main_prompt,
                    extra_text_parts,
                    on_retry_callback=_on_retry,
                    log_context={
                        "endpoint": "models.generate_content_stream",
                        "model": self.model_name,
                        "key_id": self._allocated_key_id or "unknown",
                    },
                )
                break
            except ValueError as ve:
                if json_attempt <= json_parse_max_retries:
                    self.logger.warning(
                        f"âš ï¸ JSON è§£æå¤±è´¥ (ç¬¬ {json_attempt} æ¬¡)ï¼Œé‡æ–°è¯·æ±‚ API: {ve}"
                    )
                else:
                    raise

        self.api_counter.increment("Gemini")

        if response_data is None:
            raise RuntimeError("Gemini å“åº”ä¸ºç©º")

        return response_data

    def analyze_video(self, video_path: str | Path) -> AnalysisResult:
        """
        åˆ†æè§†é¢‘å†…å®¹ï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ
        """
        self._llm_repair_used = False
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

        self.logger.info(f"å¼€å§‹åˆ†æè§†é¢‘: {video_path.name}")

        duration = probe_duration(video_path)
        long_video_config = self.analyzer_config.get("long_video", {})
        segment_plan = plan_segments_with_budget(
            duration, self.config, self.api_counter.current_count
        )

        if self._should_use_segmentation(duration, segment_plan, long_video_config):
            if not self._client:
                raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")
            return self._analyze_video_segments(video_path, duration, segment_plan)

        # æ£€æŸ¥ API è°ƒç”¨æ¬¡æ•° (é¢„ç•™ 2 æ¬¡: 1æ¬¡å†…å®¹åˆ†æ, 1æ¬¡ Schema ç”Ÿæˆ)
        if self.api_counter.current_count + 2 > self.api_counter.max_calls:
            raise APILimitExceeded(
                f"API è°ƒç”¨æ¬¡æ•°ä¸è¶³ä»¥å®Œæˆå…¨æµç¨‹: {self.api_counter.current_count}/{self.api_counter.max_calls}"
            )

        # Step 0: å‡†å¤‡è§†é¢‘æ–‡ä»¶ (å‹ç¼© + ä¸Šä¼ )
        # æ³¨æ„: è¿™é‡Œä¸è¿›è¡Œé‡è¯•ï¼Œå¦‚æœä¸Šä¼ å¤±è´¥é€šå¸¸æ˜¯ç½‘ç»œæˆ–æ–‡ä»¶é—®é¢˜ï¼Œé‡è¯•æ„ä¹‰ä¸å¤§æˆ–ç”±å¤–éƒ¨æ§åˆ¶
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")

        video_file = self._upload_video(video_path)

        try:
            # Step 1: è§†é¢‘å†…å®¹åˆ†æ (é€šè¿‡é™æµå™¨è‡ªåŠ¨é‡è¯• 429)
            self.logger.info("Step 1: æ‰§è¡Œè§†é¢‘å†…å®¹åˆ†æ...")
            system_role = self.prompts.get("gemini_analysis", {}).get("system_role", "")
            main_prompt = self.prompts.get("gemini_analysis", {}).get("main_prompt", "")
            try:
                response_data = self._call_analysis_json(
                    video_file, system_role, main_prompt
                )
            except Exception as exc:
                if self._is_input_token_overflow_error(exc):
                    self.logger.warning("æ£€æµ‹åˆ°è¾“å…¥ token è¶…é™ï¼Œåˆ‡æ¢ä¸ºåˆ†æ®µåˆ†ææ¨¡å¼")
                    return self._analyze_video_segments(
                        video_path, duration, segment_plan
                    )
                raise

            # Step 2: ç”Ÿæˆ Visual Schema
            # å¦‚æœ Step 1 å·²ç»ç”Ÿæˆäº† Visual Schemaï¼Œåˆ™è·³è¿‡ Step 2
            # å¦åˆ™å°è¯•å•ç‹¬ç”Ÿæˆ (Fallback)

            raw_schemas = response_data.get("visual_schemas", [])
            has_valid_schema = any(
                isinstance(s, dict) and "---BEGIN PROMPT---" in s.get("schema", "")
                for s in (raw_schemas if isinstance(raw_schemas, list) else [])
            )

            if has_valid_schema:
                self.logger.info("Visual Schema å·²åœ¨ Step 1 ä¸­ç”Ÿæˆï¼Œè·³è¿‡ç‹¬ç«‹ç”Ÿæˆæ­¥éª¤")
            else:
                self.logger.info(
                    "Step 1 æœªç”Ÿæˆæœ‰æ•ˆ Visual Schemaï¼Œå°è¯•ç‹¬ç«‹ç”Ÿæˆ (Step 2)..."
                )
                if self.api_counter.current_count + 1 <= self.api_counter.max_calls:
                    try:
                        self.logger.info("Step 2: ç”ŸæˆçŸ¥è¯†è“å›¾ Visual Schema...")
                        deep_dive_content = json.dumps(
                            response_data.get("deep_dive", []),
                            ensure_ascii=False,
                            indent=2,
                        )
                        fallback_schema = self._generate_visual_schema(
                            deep_dive_content
                        )
                        self.logger.info("Visual Schema ç”ŸæˆæˆåŠŸ")
                        response_data["visual_schemas"] = [
                            {
                                "type": "overview",
                                "description": "æ€»è§ˆçŸ¥è¯†å¯¼å›¾",
                                "schema": fallback_schema,
                            }
                        ]
                    except Exception as e:
                        self.logger.error(
                            f"Visual Schema ç”Ÿæˆå¤±è´¥: {e}ï¼Œå°†ç”Ÿæˆä¸å¸¦å›¾çš„æŠ¥å‘Š"
                        )
                else:
                    self.logger.warning(
                        "API é…é¢ä¸è¶³ä»¥æ‰§è¡Œ Step 2ï¼Œè·³è¿‡ Visual Schema ç”Ÿæˆ"
                    )

            metadata = {
                "video_name": video_path.name,
                "video_size": video_path.stat().st_size,
            }

            try:
                result = AnalysisResult.from_api_response(
                    video_path=video_path,
                    response_data=response_data,
                    metadata=metadata,
                )
                self.logger.info(f"è§†é¢‘åˆ†æå…¨æµç¨‹å®Œæˆ: {result.title}")
                return result
            except ValueError as e:
                self.logger.error(f"API å“åº”æ ¼å¼é”™è¯¯: {e}")
                raise

        finally:
            # æ¸…ç†èµ„æº
            if video_file:
                video_file_name = getattr(video_file, "name", None)
                if video_file_name:
                    self._delete_remote_file(video_file_name)

            # æ¸…ç†å‹ç¼©æ–‡ä»¶ (å¯é€‰: å¦‚æœå¸Œæœ›ä¸‹æ¬¡è¿è¡Œå¤ç”¨ï¼Œå¯ä»¥æ³¨é‡Šæ‰è¿™è¡Œï¼Œæˆ–è€…ä¿ç•™ä»¥èŠ‚çœç©ºé—´)
            # æ ¹æ®ç”¨æˆ·éœ€æ±‚ "æ–­ç‚¹å¤„ç»§ç»­"ï¼Œæˆ‘ä»¬åº”è¯¥ä¿ç•™å‹ç¼©æ–‡ä»¶ï¼Œæˆ–è€…åœ¨æˆåŠŸåæ‰æ¸…ç†ï¼Ÿ
            # ç°åœ¨çš„é€»è¾‘æ˜¯ `_compress_video_for_upload` ä¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚
            # å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œåˆ é™¤äº†ï¼Œé‚£ä¹ˆä¸‹æ¬¡åˆè¦é‡æ–°å‹ç¼©ã€‚
            # ä¸ºäº†æ”¯æŒ"æ–­ç‚¹å‹ç¼©"ï¼Œæˆ‘ä»¬åº”è¯¥ä»…åœ¨å®Œå…¨æˆåŠŸååˆ é™¤ï¼Œæˆ–è€…ä¿ç•™åœ¨ temp ç›®å½•ç”±ç³»ç»Ÿæ¸…ç†ï¼Ÿ
            # ä¹‹å‰çš„é€»è¾‘æ˜¯ `finally` ä¸­åˆ é™¤ã€‚
            # ç”¨æˆ·æŠ±æ€¨çš„æ˜¯ "å¤±è´¥å...é‡æ–°ä¸‹è½½å¹¶å‹ç¼©"ã€‚
            # å¦‚æœæˆ‘ä»¬åˆ é™¤äº†ï¼Œä¸‹æ¬¡ç¡®å®è¦é‡æ–°å‹ç¼©ã€‚
            # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬åº”è¯¥ç§»é™¤åˆ é™¤å‹ç¼©æ–‡ä»¶çš„é€»è¾‘ï¼Œæˆ–è€…åªåœ¨æˆåŠŸæ—¶åˆ é™¤ï¼Ÿ
            # å®é™…ä¸Šï¼Œ`VideoDownloader` æœ‰ä¸ª temp ç›®å½•ï¼Œå¯èƒ½æ›´é€‚åˆæ”¾é‚£é‡Œã€‚
            # è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯å…ˆä¿ç•™åˆ é™¤é€»è¾‘ï¼Œä½†æ˜¯å› ä¸º `_compress_video_for_upload` ç°åœ¨æ£€æŸ¥äº†å­˜åœ¨æ€§ï¼Œ
            # åªè¦åœ¨"æœ¬æ¬¡å…ƒæ“ä½œ"ä¸­ä¸åˆ é™¤ï¼Œä¸‹æ¬¡é‡è¯•ï¼ˆåŒä¸€è¿›ç¨‹å†…ï¼‰è¿˜æ˜¯ä¼šé€šè¿‡ path ä¼ é€’ã€‚
            # ä½†å¦‚æœæ˜¯è¿›ç¨‹å´©æºƒé‡å¯ï¼Œ`upload_path` æ˜¯ä¸´æ—¶ç”Ÿæˆçš„å—ï¼Ÿ
            # `video_path.parent / f"compressed_{video_path.name}"` æ˜¯åœ¨åŸç›®å½•ä¸‹ã€‚
            # å¦‚æœæˆ‘åˆ é™¤äº†ï¼Œä¸‹æ¬¡è¿›ç¨‹å¯åŠ¨è¿˜æ˜¯è¦å‹ç¼©ã€‚
            # é‰´äºç”¨æˆ·çš„éœ€æ±‚ï¼Œæˆ‘å°†æ³¨é‡Šæ‰åˆ é™¤å‹ç¼©æ–‡ä»¶çš„ä»£ç ï¼Œè®©å®ƒä¿ç•™ï¼Œ
            # æˆ–è€…å°†å…¶ç§»åŠ¨åˆ° VideoPipeline çš„æ¸…ç†é˜¶æ®µï¼Ÿ
            # ç®€å•èµ·è§ï¼Œæˆ‘å…ˆä¸åˆ é™¤å‹ç¼©æ–‡ä»¶ï¼Œè®©å®ƒå˜æˆ"ç¼“å­˜"ã€‚
            # ä¸ºäº†é¿å…åƒåœ¾å †ç§¯ï¼Œå¯ä»¥åœ¨ AnalysisResult æˆåŠŸè¿”å›å‰åˆ é™¤ï¼Ÿ
            # è¿˜æ˜¯è¯´ï¼Œç”¨æˆ·å¸Œæœ›çš„æ˜¯"å¤±è´¥é‡è¯•"æ—¶ä¸é‡æ–°å‹ç¼©ã€‚
            # å½“å‰çš„ `finally` å—æ˜¯åœ¨ `analyze_video` ç»“æŸæ—¶æ‰§è¡Œã€‚
            # å¦‚æœ `analyze_video` å¤±è´¥æŠ›å‡ºå¼‚å¸¸ï¼Œ`finally` æ‰§è¡Œï¼Œæ–‡ä»¶è¢«åˆ ã€‚
            # ä¸‹æ¬¡è°ƒç”¨ `analyze_video` (æ¯”å¦‚å¤–éƒ¨é‡è¯•)ï¼Œæ–‡ä»¶æ²¡äº†ï¼Œåˆè¦å‹ç¼©ã€‚
            # æ‰€ä»¥å¿…é¡» **ä¸åˆ é™¤** å‹ç¼©æ–‡ä»¶ï¼Œæˆ–è€…åªåœ¨ **æˆåŠŸ** ååˆ é™¤ã€‚

            compressed_file = video_path.parent / f"compressed_{video_path.name}"
            if compressed_file.exists():
                # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆç»“æœåæ‰åˆ é™¤ï¼Ÿæˆ–è€…å¹²è„†ä¸åˆ ï¼Œç•™ç»™ç”¨æˆ·æ‰‹åŠ¨æ¸…ç†/å®šæœŸæ¸…ç†ï¼Ÿ
                # ä¸ºäº†é˜²æ­¢ç£ç›˜çˆ†æ»¡ï¼Œæˆ‘ä»¬è¿˜æ˜¯å°è¯•åˆ é™¤ï¼Œä½†æ˜¯å‰ææ˜¯å¿…é¡»åŒºåˆ†"å®Œå…¨å¤±è´¥"å’Œ"æˆåŠŸ"ã€‚
                # ç”±äº `finally` ä¸åŒºåˆ†ï¼Œæˆ‘ä»¬å¾ˆéš¾åšã€‚
                # æœ€å¥½çš„æ–¹å¼æ˜¯ï¼šä¸åœ¨è¿™é‡Œåˆ é™¤ã€‚è®©ä¸Šå±‚ `VideoPipeline` æˆ–è€… `cleanup` æ–¹æ³•æ¥å¤„ç†ã€‚
                # æˆ–è€…ï¼Œä»…ä»…è®°å½•æ—¥å¿—è¯´"ä¿ç•™å‹ç¼©æ–‡ä»¶ä»¥ä¾¿é‡è¯•"ã€‚
                self.logger.info(f"ä¿ç•™å‹ç¼©æ–‡ä»¶ä»¥å¤‡é‡è¯•: {compressed_file.name}")
                pass

    def _analyze_segment_range(
        self,
        *,
        video_path: Path,
        segment_id: int,
        segment_index: int,
        total_segments: int,
        start: float,
        end: float,
        prompt_start: float,
        prompt_end: float,
        segment_dir: Path,
        segment_path: Path | None,
        min_segment_seconds: float,
        system_role: str,
        main_prompt: str,
    ) -> list[dict[str, Any]]:
        duration = end - start
        if duration <= 0:
            return []

        if not self.api_counter.can_call():
            raise APILimitExceeded("API è°ƒç”¨æ¬¡æ•°ä¸è¶³ï¼Œåœæ­¢åˆ†æ®µåˆ†æ")

        if segment_path is None:
            segment_path = segment_dir / (
                f"segment_{segment_id:04d}_{int(start * 1000):010d}_{int(end * 1000):010d}.mp4"
            )
        if not segment_path.exists() or segment_path.stat().st_size <= 0:
            extracted = extract_segment(
                input_path=video_path,
                start=start,
                end=end,
                output_path=segment_path,
                stream_copy=True,
            )
            if not extracted:
                raise RuntimeError("åˆ†æ®µè§†é¢‘åˆ‡å‰²å¤±è´¥")

        video_file = self._upload_video(segment_path)
        try:
            self._llm_repair_used = False
            segment_parts = self._build_segment_prompt_parts(
                segment_index, total_segments, prompt_start, prompt_end
            )
            response_data = self._call_analysis_json(
                video_file,
                system_role,
                main_prompt,
                extra_text_parts=segment_parts,
            )
            return [
                {
                    "start": prompt_start,
                    "end": prompt_end,
                    "data": response_data,
                }
            ]
        except Exception as exc:
            if self._is_input_token_overflow_error(exc):
                if duration / 2 < min_segment_seconds:
                    raise
                mid = (start + end) / 2
                left = self._analyze_segment_range(
                    video_path=video_path,
                    segment_id=segment_id,
                    segment_index=segment_index,
                    total_segments=total_segments,
                    start=start,
                    end=mid,
                    prompt_start=start,
                    prompt_end=mid,
                    segment_dir=segment_dir,
                    segment_path=None,
                    min_segment_seconds=min_segment_seconds,
                    system_role=system_role,
                    main_prompt=main_prompt,
                )
                right = self._analyze_segment_range(
                    video_path=video_path,
                    segment_id=segment_id,
                    segment_index=segment_index,
                    total_segments=total_segments,
                    start=mid,
                    end=end,
                    prompt_start=mid,
                    prompt_end=end,
                    segment_dir=segment_dir,
                    segment_path=None,
                    min_segment_seconds=min_segment_seconds,
                    system_role=system_role,
                    main_prompt=main_prompt,
                )
                return left + right
            raise
        finally:
            if video_file:
                video_file_name = getattr(video_file, "name", None)
                if video_file_name:
                    self._delete_remote_file(video_file_name)

    def _analyze_video_segments(
        self, video_path: Path, duration: float, plan: SegmentPlan
    ) -> AnalysisResult:
        if duration <= 0:
            raise RuntimeError("æ— æ³•è·å–è§†é¢‘æ—¶é•¿ï¼Œæ— æ³•åˆ†æ®µåˆ†æ")

        long_video_config = self.analyzer_config.get("long_video", {})
        segment_seconds = plan.segment_duration
        overlap_seconds = plan.overlap
        if segment_seconds <= 0:
            segment_seconds = int(long_video_config.get("min_segment_seconds") or 90)
            overlap_seconds = 0

        min_segment_seconds = float(long_video_config.get("min_segment_seconds") or 90)

        if plan.hard_max_calls:
            self.api_counter.set_max_calls(
                self.api_counter.max_calls, plan.hard_max_calls
            )

        if not self.api_counter.can_call():
            raise APILimitExceeded("API è°ƒç”¨æ¬¡æ•°ä¸è¶³ä»¥æ‰§è¡Œåˆ†æ®µåˆ†æ")

        system_config = self.config.get("system", {})
        temp_dir = system_config.get("temp_dir", "./data/temp")
        video_id = video_path.stem

        manifest = load_or_create_manifest(
            video_id=video_id,
            duration=duration,
            segment_seconds=segment_seconds,
            overlap_seconds=overlap_seconds,
            temp_dir=temp_dir,
        )
        segment_dir = Path(temp_dir) / "segments" / video_id
        manifest_path = segment_dir / "manifest.json"

        segments = sorted(manifest["segments"], key=lambda item: item["id"])
        total_segments = len(segments)
        if total_segments == 0:
            raise RuntimeError("æ— æ³•ç”Ÿæˆåˆ†æ®µè®¡åˆ’ï¼Œç¼ºå°‘å¯åˆ†æçš„ç‰‡æ®µ")

        system_role = self.prompts.get("gemini_analysis", {}).get("system_role", "")
        main_prompt = self.prompts.get("gemini_analysis", {}).get("main_prompt", "")

        segment_outputs: list[dict[str, Any]] = []
        gap_notes: list[str] = []

        for entry in segments:
            segment_id = entry["id"]
            effective_start = float(entry.get("effective_start", entry["start"]))
            effective_end = float(entry.get("effective_end", entry["end"]))

            if not self.api_counter.can_call():
                gap_notes.append(
                    f"{self._format_timecode(effective_start)}-{self._format_timecode(effective_end)}"
                )
                remaining = [item for item in segments if item["id"] > segment_id]
                for item in remaining:
                    gap_notes.append(
                        f"{self._format_timecode(float(item.get('effective_start', item['start'])))}-{self._format_timecode(float(item.get('effective_end', item['end'])))}"
                    )
                break

            update_segment_status(
                manifest, segment_id, "processing", increment_attempts=True
            )
            save_manifest(manifest_path, manifest)

            try:
                segment_results = self._analyze_segment_range(
                    video_path=video_path,
                    segment_id=segment_id,
                    segment_index=segment_id + 1,
                    total_segments=total_segments,
                    start=float(entry["start"]),
                    end=float(entry["end"]),
                    prompt_start=effective_start,
                    prompt_end=effective_end,
                    segment_dir=segment_dir,
                    segment_path=Path(entry["file_path"]),
                    min_segment_seconds=min_segment_seconds,
                    system_role=system_role,
                    main_prompt=main_prompt,
                )
                if segment_results:
                    segment_outputs.extend(segment_results)
                    update_segment_status(manifest, segment_id, "completed")
                else:
                    update_segment_status(
                        manifest,
                        segment_id,
                        "failed",
                        error="segment returned empty",
                    )
                    gap_notes.append(
                        f"{self._format_timecode(effective_start)}-{self._format_timecode(effective_end)}"
                    )
            except APILimitExceeded:
                update_segment_status(
                    manifest,
                    segment_id,
                    "skipped",
                    error="api budget exhausted",
                )
                gap_notes.append(
                    f"{self._format_timecode(effective_start)}-{self._format_timecode(effective_end)}"
                )
                remaining = [item for item in segments if item["id"] > segment_id]
                for item in remaining:
                    gap_notes.append(
                        f"{self._format_timecode(float(item.get('effective_start', item['start'])))}-{self._format_timecode(float(item.get('effective_end', item['end'])))}"
                    )
                break
            except Exception as exc:
                update_segment_status(manifest, segment_id, "failed", error=str(exc))
                gap_notes.append(
                    f"{self._format_timecode(effective_start)}-{self._format_timecode(effective_end)}"
                )
            finally:
                save_manifest(manifest_path, manifest)

        if not segment_outputs:
            raise RuntimeError("åˆ†æ®µåˆ†æå¤±è´¥ï¼Œæœªè·å¾—ä»»ä½•æœ‰æ•ˆç»“æœ")

        merged = self._merge_segment_outputs(segment_outputs, gap_notes)
        metadata = {
            "video_name": video_path.name,
            "video_size": video_path.stat().st_size,
            "duration": duration,
            "segments": total_segments,
            "segment_gaps": gap_notes,
        }

        return AnalysisResult.from_api_response(
            video_path=video_path,
            response_data=merged,
            metadata=metadata,
        )

    def _merge_segment_outputs(
        self, segment_outputs: list[dict[str, Any]], gap_notes: list[str]
    ) -> dict[str, Any]:
        ordered = sorted(segment_outputs, key=lambda item: item.get("start", 0.0))
        first_data = ordered[0]["data"]

        merged_key_takeaways: list[str] = []
        seen_takeaways: set[str] = set()

        merged_glossary: dict[str, str] = {}
        seen_terms: dict[str, str] = {}

        merged_deep_dive: list[dict[str, Any]] = []
        seen_sections: set[str] = set()
        last_end_time: float | None = None

        for item in ordered:
            data = item["data"]
            key_takeaways = self._coerce_list(data.get("key_takeaways", []))
            for takeaway in key_takeaways:
                norm = self._normalize_text(takeaway)
                if norm in seen_takeaways:
                    continue
                merged_key_takeaways.append(takeaway)
                seen_takeaways.add(norm)

            glossary = data.get("glossary", {})
            if isinstance(glossary, dict):
                for term, definition in glossary.items():
                    term_text = str(term).strip()
                    if not term_text:
                        continue
                    norm = self._normalize_text(term_text)
                    if norm in seen_terms:
                        existing_key = seen_terms[norm]
                        if not merged_glossary.get(existing_key) and definition:
                            merged_glossary[existing_key] = str(definition)
                        continue
                    merged_glossary[term_text] = str(definition)
                    seen_terms[norm] = term_text

            chapters = self._normalize_chapters(data.get("deep_dive", []))
            for chapter in chapters:
                chapter_title = str(chapter.get("chapter_title", ""))
                chapter_time_range = self._extract_time_range_from_text(chapter_title)
                sections = chapter.get("sections", [])
                if not isinstance(sections, list):
                    continue

                kept_sections: list[dict[str, Any]] = []
                for section in sections:
                    if not isinstance(section, dict):
                        continue
                    signature = self._section_signature(section)
                    if signature in seen_sections:
                        continue

                    start_time, end_time = self._extract_section_time_range(
                        section, chapter_time_range
                    )
                    if (
                        start_time is not None
                        and last_end_time is not None
                        and start_time <= last_end_time
                    ):
                        continue

                    kept_sections.append(section)
                    seen_sections.add(signature)
                    if end_time is not None:
                        last_end_time = max(last_end_time or 0.0, end_time)
                    elif start_time is not None:
                        last_end_time = max(last_end_time or 0.0, start_time)

                if kept_sections:
                    merged_deep_dive.append(
                        {
                            "chapter_title": chapter.get("chapter_title", ""),
                            "chapter_summary": chapter.get("chapter_summary", ""),
                            "sections": kept_sections,
                        }
                    )

        if gap_notes:
            gap_text = "ã€".join(gap_notes)
            merged_key_takeaways.append(f"æ³¨æ„ï¼šä»¥ä¸‹ç‰‡æ®µæœªè¦†ç›–æˆ–åˆ†æå¤±è´¥ï¼š{gap_text}")

        merged: dict[str, Any] = {
            "title": first_data.get("title", ""),
            "one_sentence_summary": first_data.get("one_sentence_summary", ""),
            "key_takeaways": merged_key_takeaways,
            "deep_dive": merged_deep_dive,
            "glossary": merged_glossary,
        }

        if "visual_schemas" in first_data:
            merged["visual_schemas"] = first_data.get("visual_schemas", [])
        elif "visual_schema" in first_data:
            merged["visual_schema"] = first_data.get("visual_schema", "")

        return merged

    def _generate_visual_schema(self, deep_dive_content: str) -> str:
        """æ ¹æ®æ·±åº¦è§£æå†…å®¹ç”Ÿæˆ Visual Schema"""

        schema_config = self.prompts.get("gemini_visual_schema", {})
        system_role = schema_config.get("system_role", "")
        main_prompt_template = schema_config.get("main_prompt", "")

        full_prompt = render_prompt(
            main_prompt_template, deep_dive_content=deep_dive_content
        )

        # ä½¿ç”¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæ¥å£
        response_text = self._call_gemini_text_api(
            system_role=system_role, user_prompt=full_prompt, temperature=0.7
        )

        # æå– Markdown ä»£ç å—
        if "```markdown" in response_text:
            start = response_text.find("```markdown") + 11
            end = response_text.find("```", start)
            return response_text[start:end].strip()
        elif "```" in response_text:
            start = response_text.find("```") + 3
            end = response_text.find("```", start)
            return response_text[start:end].strip()

        return response_text.strip()

    def _stream_response(
        self,
        contents: list[dict[str, Any]],
        gen_config: types.GenerateContentConfig,
    ) -> tuple[str, str]:
        """æµå¼æ¥æ”¶ Gemini å“åº”ï¼Œè¿”å› (response_text, finish_reason_name)ã€‚

        è¿™æ˜¯ _generate_content å’Œ _call_gemini_text_api å…±ç”¨çš„åº•å±‚æµå¼æ¥æ”¶æ–¹æ³•ã€‚

        Args:
            contents: å¤šè½®å¯¹è¯å†…å®¹åˆ—è¡¨
            gen_config: ç”Ÿæˆé…ç½®

        Returns:
            (response_text, finish_reason_name) å…ƒç»„ã€‚
            finish_reason_name ä¸º "STOP"ã€"MAX_TOKENS" ç­‰å­—ç¬¦ä¸²ï¼Œ
            å¦‚æœæœªè·å–åˆ° finish_reason åˆ™è¿”å› "UNKNOWN"ã€‚
        """
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")
        response_text_parts: list[str] = []
        thinking_logged = False
        finish_reason_name = "UNKNOWN"
        key_id = self._allocated_key_id or "unknown"
        usage_metadata = None

        self.logger.info(
            f"API call | op=generate_content | key_id={key_id} | model={self.model_name}"
        )
        self.logger.info(
            f"API call | op=models.generate_content_stream | key_id={key_id} "
            + f"| model={self.model_name}"
        )
        contents_any = cast(Any, contents)

        for chunk in self._client.models.generate_content_stream(
            model=self.model_name,
            contents=contents_any,
            config=gen_config,
        ):
            usage_metadata = (
                getattr(chunk, "usage_metadata", None)
                or getattr(chunk, "usage", None)
                or usage_metadata
            )
            if not chunk.candidates:
                continue

            candidate = chunk.candidates[0]

            if candidate.finish_reason:
                finish_reason_name = candidate.finish_reason.name or "UNKNOWN"

            if not candidate.content or not candidate.content.parts:
                continue

            for part in candidate.content.parts:
                if part.thought:
                    if not thinking_logged:
                        self.logger.info("ğŸ’­ Gemini æ€è€ƒä¸­...")
                        thinking_logged = True
                    snippet = (part.text or "")[:200] if part.text else ""
                    if snippet:
                        self.logger.info(f"  ğŸ’­ {snippet}")
                else:
                    if part.text:
                        response_text_parts.append(part.text)
                        snippet = (part.text or "")[:100].replace("\n", " ")
                        self.logger.info(f"  ğŸ“ ç”Ÿæˆä¸­: {snippet}...")

        response_text = "".join(response_text_parts)
        self.logger.info(
            f"API call complete | op=models.generate_content_stream | key_id={key_id} "
            + f"| model={self.model_name} | finish_reason={finish_reason_name}"
        )
        usage = self._extract_usage_metadata(usage_metadata)
        if usage:
            self.logger.info(
                f"Token usage | prompt={usage.prompt_tokens} output={usage.output_tokens} total={usage.total_tokens}"
            )
        return response_text, finish_reason_name

    @staticmethod
    def _extract_usage_metadata(usage: Any) -> SimpleNamespace | None:
        if usage is None:
            return None
        prompt_tokens = getattr(usage, "prompt_tokens", None)
        output_tokens = getattr(usage, "output_tokens", None)
        total_tokens = getattr(usage, "total_tokens", None)

        if prompt_tokens is None:
            prompt_tokens = getattr(usage, "prompt_token_count", None)
        if output_tokens is None:
            output_tokens = getattr(usage, "candidates_token_count", None)
        if total_tokens is None:
            total_tokens = getattr(usage, "total_token_count", None)

        if prompt_tokens is None or output_tokens is None or total_tokens is None:
            return None

        return SimpleNamespace(
            prompt_tokens=prompt_tokens,
            output_tokens=output_tokens,
            total_tokens=total_tokens,
        )

    @staticmethod
    def _is_input_token_overflow_error(exc: Exception) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºè¾“å…¥ token è¶…é™é”™è¯¯ï¼ˆ400 INVALID_ARGUMENTï¼‰ã€‚

        Args:
            exc: æ•è·çš„å¼‚å¸¸å¯¹è±¡

        Returns:
            True å¦‚æœæ˜¯è¾“å…¥ token è¶…è¿‡ 1048576 çš„é”™è¯¯
        """
        error_msg = str(exc).lower()
        return (
            "400" in error_msg
            and "invalid_argument" in error_msg
            and "input token count exceeds" in error_msg
            and "1048576" in error_msg
        )

    def _stream_with_continuation(
        self,
        contents: list[dict[str, Any]],
        gen_config: types.GenerateContentConfig,
        continuation_prompt: str,
    ) -> str:
        """æµå¼æ¥æ”¶ + MAX_TOKENS è‡ªåŠ¨ç»­ä¼ ã€‚

        å½“ finish_reason ä¸º MAX_TOKENS æ—¶ï¼Œå°†å·²æœ‰å†…å®¹ä½œä¸º model å›å¤è¿½åŠ åˆ°å¯¹è¯å†å²ï¼Œ
        å†å‘é€ç»­ä¼ æŒ‡ä»¤ï¼Œè®© Gemini ä»æˆªæ–­å¤„ç»§ç»­è¾“å‡ºã€‚æœ€å¤šç»­ä¼  max_continuations è½®ã€‚

        ç»­ä¼ ç­–ç•¥ï¼šç¬¬ 1 è½®ä½¿ç”¨åŸå§‹ contentsï¼ˆå« file_dataï¼‰ï¼Œåç»­è½®æ¬¡ä½¿ç”¨çº¯æ–‡æœ¬å†å²
        ï¼ˆä¸å« file_dataï¼‰ï¼Œé¿å…é‡å¤å‘é€è§†é¢‘å¯¼è‡´è¾“å…¥ token è¶…é™ã€‚

        Args:
            contents: åˆå§‹å¯¹è¯å†…å®¹åˆ—è¡¨ï¼ˆç¬¬ 1 è½®ä½¿ç”¨ï¼Œåç»­è½®æ¬¡æ„å»ºæ–°çš„çº¯æ–‡æœ¬å†å²ï¼‰
            gen_config: ç”Ÿæˆé…ç½®
            continuation_prompt: ç»­ä¼ æ—¶å‘é€ç»™ Gemini çš„ç”¨æˆ·æŒ‡ä»¤

        Returns:
            æ‹¼æ¥åçš„å®Œæ•´å“åº”æ–‡æœ¬
        """
        all_text_parts: list[str] = []
        text_only_history: list[dict[str, Any]] = []

        for round_idx in range(self.max_continuations + 1):
            round_label = f"ç¬¬ {round_idx + 1} è½®" if round_idx > 0 else "é¦–æ¬¡è¯·æ±‚"
            self.logger.info(f"å¼€å§‹æµå¼æ¥æ”¶ Gemini å“åº” ({round_label})...")

            current_contents = contents if round_idx == 0 else text_only_history
            response_text, finish_reason = self._stream_response(
                current_contents, gen_config
            )

            self.logger.info(
                f"æµå¼æ¥æ”¶å®Œæˆ ({round_label})ï¼Œ"
                f"æœ¬è½®é•¿åº¦: {len(response_text)} å­—ç¬¦ï¼Œ"
                f"finish_reason: {finish_reason}"
            )

            all_text_parts.append(response_text)
            self._report_usage_to_pool()

            if finish_reason != "MAX_TOKENS":
                if finish_reason == "STOP":
                    self.logger.info("Gemini ç”Ÿæˆæ­£å¸¸ç»“æŸ (STOP)")
                else:
                    self.logger.warning(
                        f"Gemini ç”Ÿæˆç»“æŸåŸå› é STOP: {finish_reason} (å¯èƒ½å‘ç”Ÿæˆªæ–­)"
                    )
                break

            if round_idx >= self.max_continuations:
                self.logger.warning(
                    f"å·²è¾¾æœ€å¤§ç»­ä¼ è½®æ•° ({self.max_continuations})ï¼Œåœæ­¢ç»­ä¼ "
                )
                break

            self.logger.info(
                f"âš ï¸ æ£€æµ‹åˆ° MAX_TOKENS æˆªæ–­ï¼Œå‘èµ·ç»­ä¼  "
                f"(ç¬¬ {round_idx + 2}/{self.max_continuations + 1} è½®)..."
            )

            if round_idx == 0:
                text_only_history = self._extract_text_only_prompt(contents)

            text_only_history.append(
                {"role": "model", "parts": [{"text": response_text}]}
            )
            text_only_history.append(
                {"role": "user", "parts": [{"text": continuation_prompt}]}
            )

            self.throttle.wait_before_call()

        total_text = "".join(all_text_parts)
        self.logger.info(
            f"å“åº”æ€»é•¿åº¦: {len(total_text)} å­—ç¬¦ (å…± {len(all_text_parts)} è½®)"
        )
        return total_text

    @staticmethod
    def _extract_text_only_prompt(
        contents: list[dict[str, Any]],
    ) -> list[dict[str, Any]]:
        """ä»åŸå§‹ contents ä¸­æå–çº¯æ–‡æœ¬éƒ¨åˆ†ï¼ˆç§»é™¤ file_dataï¼‰ã€‚

        Args:
            contents: åŸå§‹å¯¹è¯å†…å®¹åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å« file_dataï¼‰

        Returns:
            ä»…åŒ…å«æ–‡æœ¬éƒ¨åˆ†çš„å¯¹è¯å†…å®¹åˆ—è¡¨
        """
        text_only: list[dict[str, Any]] = []
        for msg in contents:
            text_parts = [part for part in msg.get("parts", []) if "text" in part]
            if text_parts:
                text_only.append({"role": msg["role"], "parts": text_parts})
        return text_only

    def _generate_content(
        self,
        video_file: Any,
        system_role: str,
        main_prompt: str,
        extra_text_parts: list[str] | None = None,
    ) -> dict[str, Any]:
        """
        ä½¿ç”¨å·²ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶ç”Ÿæˆå†…å®¹
        """
        # Disable thinking for JSON mode to avoid empty-response edge case
        gen_config = types.GenerateContentConfig(
            temperature=self.temperature,
            max_output_tokens=self.max_output_tokens,
            response_mime_type="application/json",
            system_instruction=system_role,
            thinking_config=None,
        )

        parts: list[dict[str, Any]] = [
            {
                "file_data": {
                    "file_uri": video_file.uri,
                    "mime_type": video_file.mime_type,
                }
            }
        ]
        if extra_text_parts:
            for text_part in extra_text_parts:
                if text_part:
                    parts.append({"text": text_part})
        parts.append({"text": main_prompt})

        contents: list[dict[str, Any]] = [{"role": "user", "parts": parts}]

        continuation_prompt = (
            "ä½ çš„ä¸Šä¸€æ¬¡è¾“å‡ºå› é•¿åº¦é™åˆ¶è¢«æˆªæ–­äº†ã€‚"
            "è¯·ä»ä¸Šæ¬¡æˆªæ–­å¤„ç»§ç»­è¾“å‡ºï¼Œç›´æ¥ç»­å†™ JSON å†…å®¹ï¼Œ"
            "ä¸è¦é‡å¤å·²è¾“å‡ºçš„éƒ¨åˆ†ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€è¯´æ˜ã€‚"
        )

        response_text = self._stream_with_continuation(
            contents, gen_config, continuation_prompt
        )

        response_text = response_text.strip()

        # ç©ºå“åº”æ£€æµ‹ï¼šGemini å¯èƒ½åªè¿”å› thinking å†…å®¹è€Œæ— å®é™…æ–‡æœ¬
        if not response_text:
            raise ValueError("Gemini è¿”å›äº†ç©ºå“åº”ï¼ˆ0 å­—ç¬¦ï¼‰ï¼Œå¯èƒ½ä»…åŒ…å« thinking å†…å®¹")

        self.logger.debug(f"API å“åº”: {response_text[:200]}...")

        # è§£æ JSON å“åº”
        # 1. ä¼˜å…ˆå°è¯•æ ‡å‡†çš„ ```json ... ``` å—
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        else:
            # 2. å°è¯•ä¸å¸¦ json æ ‡ç­¾çš„ ``` ... ``` å—ï¼Œä½†æ’é™¤é JSON çš„ä»£ç å—
            code_block_match = re.search(
                r"```\s*(\{.*?\})\s*```", response_text, re.DOTALL
            )
            if code_block_match:
                response_text = code_block_match.group(1)
            else:
                # 3. å…œåº•ï¼šå¯»æ‰¾æœ€å¤–å±‚çš„ {}
                start_idx = response_text.find("{")
                end_idx = response_text.rfind("}")
                if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                    response_text = response_text[start_idx : end_idx + 1]

        # å¤šè½® JSON ä¿®å¤
        cleaned_text, stripped_count = self._strip_stray_token_prefixes(response_text)
        if stripped_count > 0:
            self.logger.debug(f"event=json_stray_token_strip count={stripped_count}")
        repaired = self._try_repair_json(cleaned_text)
        if repaired is not None:
            self.logger.info("API å“åº”è§£ææˆåŠŸ")
            response_data = repaired
        else:
            if not self._llm_repair_used:
                llm_repaired = self._llm_repair_json(cleaned_text)
                self._llm_repair_used = True
                if llm_repaired is not None:
                    response_data = llm_repaired
                else:
                    self.logger.error(
                        "event=json_parse_failed reason=llm_repair_exhausted"
                    )
                    self._dump_failed_json(response_text)
                    raise ValueError("JSON parse failed after LLM repair")
            else:
                self.logger.error(
                    "event=json_parse_failed reason=llm_repair_already_used"
                )
                self._dump_failed_json(response_text)
                raise ValueError("LLM repair already used for this video")

        # å¿…éœ€å­—æ®µï¼šç¼ºå¤±åˆ™æ— æ³•æ„å»ºæœ‰æ„ä¹‰çš„æ–‡æ¡£
        required_fields = {
            "title",
            "one_sentence_summary",
            "key_takeaways",
            "deep_dive",
            "glossary",
        }
        # å¯é€‰å­—æ®µï¼švisual_schemas ç¼ºå¤±æ—¶ç”± Step 2 fallback å•ç‹¬ç”Ÿæˆ
        missing = required_fields - response_data.keys()
        if missing:
            self.logger.error(
                f"event=validation_failed reason=missing_required_fields fields={','.join(sorted(missing))}"
            )
            self.logger.warning(
                f"API å“åº” JSON ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(sorted(missing))}ï¼Œè§¦å‘é‡è¯•"
            )
            raise ValueError(
                f"API å“åº” JSON ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(sorted(missing))}"
            )

        return response_data

    def _llm_repair_json(self, invalid_json: str) -> dict[str, Any] | None:
        prompt = (
            "The following is invalid JSON. Output ONLY the corrected JSON "
            "with no markdown fences, explanations, or other text. "
            "Preserve all content and meaning exactly.\n\n"
            f"{invalid_json}"
        )

        self.logger.info("event=llm_json_repair_attempt")
        try:
            response_text = self._call_gemini_text_api(
                system_role="",
                user_prompt=prompt,
                temperature=0.0,
                max_output_tokens=8192,
            )
            parsed = json.loads(response_text)
        except Exception:
            self.logger.warning("event=llm_json_repair_failed")
            return None

        if isinstance(parsed, dict):
            self.logger.info("event=llm_json_repair_success")
            return parsed

        self.logger.warning("event=llm_json_repair_failed")
        return None

    @staticmethod
    def _dump_failed_json(response_text: str) -> None:
        # ä¿®å¤å¤±è´¥ï¼šä¿å­˜åŸå§‹å“åº”åˆ°ç£ç›˜ä»¥ä¾¿äº‹åè°ƒè¯•
        try:
            dump_path = Path("data/output/logs") / f"failed_json_{int(time.time())}.txt"
            dump_path.parent.mkdir(parents=True, exist_ok=True)
            dump_path.write_text(response_text, encoding="utf-8")
            logging.getLogger(__name__).error(
                f"JSON ä¿®å¤å¤±è´¥ï¼Œå·²ä¿å­˜åŸå§‹å“åº”åˆ°: {dump_path}"
            )
        except Exception:
            logging.getLogger(__name__).error("JSON ä¿®å¤å¤±è´¥ï¼Œä¸”æ— æ³•ä¿å­˜åŸå§‹å“åº”åˆ°ç£ç›˜")

    def _call_gemini_text_api(
        self,
        system_role: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_output_tokens: int = 8192,
    ) -> str:
        """è°ƒç”¨ Gemini çº¯æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼ˆé€šè¿‡é™æµå™¨è‡ªåŠ¨å¤„ç† 429ï¼‰"""
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")

        def _do_text_call() -> str:
            if not self._client:
                if self._allocated_api_key:
                    self._client = genai.Client(
                        api_key=self._allocated_api_key,
                        http_options={"timeout": 600_000},
                    )
                else:
                    raise RuntimeError("Gemini Client æœªåˆå§‹åŒ–ä¸”æ— å¯ç”¨ Key")

            gen_config = types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_output_tokens,
                system_instruction=system_role if system_role else None,
                thinking_config=types.ThinkingConfig(
                    thinking_budget=4096,
                ),
            )

            contents: list[dict[str, Any]] = [
                {"role": "user", "parts": [{"text": user_prompt}]}
            ]

            continuation_prompt = (
                "ä½ çš„ä¸Šä¸€æ¬¡è¾“å‡ºå› é•¿åº¦é™åˆ¶è¢«æˆªæ–­äº†ã€‚"
                "è¯·ä»ä¸Šæ¬¡æˆªæ–­å¤„ç»§ç»­è¾“å‡ºï¼Œç›´æ¥ç»­å†™å†…å®¹ï¼Œ"
                "ä¸è¦é‡å¤å·²è¾“å‡ºçš„éƒ¨åˆ†ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€è¯´æ˜ã€‚"
            )

            result = self._stream_with_continuation(
                contents, gen_config, continuation_prompt
            )

            self.api_counter.increment("Gemini")
            return result.strip()

        def _on_retry(attempt: int, exc: Exception) -> None:
            nonlocal reported_retry
            if reported_retry:
                return
            reported_retry = True
            is_daily_limit = self._classify_429_is_daily(exc)
            self._report_error_to_pool(is_rpd_limit=is_daily_limit)

        reported_retry = False
        return self.throttle.call_with_retry(
            _do_text_call,
            on_retry_callback=_on_retry,
            log_context={
                "endpoint": "models.generate_content_stream",
                "model": self.model_name,
                "key_id": self._allocated_key_id or "unknown",
            },
        )

    @staticmethod
    def _strip_stray_token_prefixes(text: str) -> tuple[str, int]:
        start_idx = text.find("{")
        end_idx = text.rfind("}")
        if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
            return text, 0

        prefix = text[: start_idx + 1]
        body = text[start_idx + 1 : end_idx]
        suffix = text[end_idx:]

        cleaned_body, count = re.subn(
            r'(?<=\n)\s*[A-Za-z0-9]\s+(?=")',
            "",
            body,
        )
        if count == 0:
            return text, 0
        return f"{prefix}{cleaned_body}{suffix}", count

    @staticmethod
    def _sanitize_json_escapes(text: str) -> str:
        """ä¿®å¤ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„éæ³•è½¬ä¹‰åºåˆ—ã€‚

        å¸¸è§åœºæ™¯: Gemini è¿”å›çš„ JSON ä¸­åŒ…å« LaTeX å…¬å¼ (\\frac, \\sum, \\ln ç­‰),
        è¿™äº›åœ¨ JSON è§„èŒƒä¸­æ˜¯éæ³•çš„è½¬ä¹‰åºåˆ—ã€‚

        ç­–ç•¥: ä»…åœ¨å­—ç¬¦ä¸²å€¼å†…éƒ¨, å°† `\\X` (X é JSON åˆæ³•è½¬ä¹‰å­—ç¬¦) æ›¿æ¢ä¸º `\\\\X`ã€‚
        JSON åˆæ³•è½¬ä¹‰å­—ç¬¦: " \\ / b f n r t u
        """
        legal_escapes = set('"\\/ b f n r t u'.split() + ['"', "\\", "/"])
        # æ›´ç²¾ç¡®: JSON å…è®¸ \" \\\\ \/ \b \f \n \r \t \uXXXX
        legal_escape_chars = set('"\\/ bfnrtu')

        result: list[str] = []
        i = 0
        in_string = False
        length = len(text)

        while i < length:
            ch = text[i]

            if not in_string:
                result.append(ch)
                if ch == '"':
                    in_string = True
                i += 1
            else:
                # åœ¨å­—ç¬¦ä¸²å†…éƒ¨
                if ch == "\\":
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦
                    if i + 1 < length:
                        next_ch = text[i + 1]
                        if next_ch in legal_escape_chars:
                            # åˆæ³•è½¬ä¹‰, åŸæ ·ä¿ç•™
                            result.append(ch)
                            result.append(next_ch)
                            i += 2
                        else:
                            # éæ³•è½¬ä¹‰ (å¦‚ \frac, \sum): æ›¿æ¢ä¸º \\\\
                            result.append("\\\\")
                            # ä¸è·³è¿‡ next_ch, è®©å®ƒä½œä¸ºæ™®é€šå­—ç¬¦å¤„ç†
                            i += 1
                    else:
                        # åæ–œæ åœ¨æœ«å°¾, è½¬ä¹‰å®ƒ
                        result.append("\\\\")
                        i += 1
                elif ch == '"':
                    result.append(ch)
                    in_string = False
                    i += 1
                else:
                    result.append(ch)
                    i += 1

        return "".join(result)

    @staticmethod
    def _close_truncated_json(text: str) -> str:
        """é—­åˆè¢«æˆªæ–­çš„ JSON: æœªé—­åˆçš„å­—ç¬¦ä¸²ã€å°¾éƒ¨é€—å·ã€æœªé—­åˆçš„æ‹¬å·ã€‚"""
        text = text.rstrip()

        # 1. é—­åˆæœªé—­åˆçš„å­—ç¬¦ä¸²
        in_string = False
        escape_next = False
        for ch in text:
            if escape_next:
                escape_next = False
                continue
            if ch == "\\":
                escape_next = True
                continue
            if ch == '"':
                in_string = not in_string

        if in_string:
            text += '"'

        # 2. å»é™¤å°¾éƒ¨é€—å·
        text = text.rstrip().rstrip(",")

        # 3. ç»Ÿè®¡æœªé—­åˆçš„æ‹¬å·å¹¶é€†åºé—­åˆ
        bracket_stack: list[str] = []
        in_str = False
        esc = False
        for ch in text:
            if esc:
                esc = False
                continue
            if ch == "\\" and in_str:
                esc = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch in ("{", "["):
                bracket_stack.append(ch)
            elif ch == "}" and bracket_stack and bracket_stack[-1] == "{":
                bracket_stack.pop()
            elif ch == "]" and bracket_stack and bracket_stack[-1] == "[":
                bracket_stack.pop()

        closing_map = {"[": "]", "{": "}"}
        suffix = "".join(closing_map[b] for b in reversed(bracket_stack))
        return text + suffix

    @staticmethod
    def _truncate_to_last_complete_item(text: str) -> str | None:
        """æˆªæ–­åˆ°æœ€åä¸€ä¸ªé€—å·å¤„, ç„¶åé‡æ–°é—­åˆæ‹¬å·ã€‚ç”¨äºä¸¢å¼ƒè¢«æˆªæ–­çš„æœ€åä¸€ä¸ªå…ƒç´ ã€‚"""
        last_comma = text.rfind(",")
        if last_comma <= 0:
            return None

        truncated = text[:last_comma]

        # é‡æ–°æ‰«æå¹¶é—­åˆ
        bracket_stack: list[str] = []
        in_str = False
        esc = False
        for ch in truncated:
            if esc:
                esc = False
                continue
            if ch == "\\" and in_str:
                esc = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch in ("{", "["):
                bracket_stack.append(ch)
            elif ch == "}" and bracket_stack and bracket_stack[-1] == "{":
                bracket_stack.pop()
            elif ch == "]" and bracket_stack and bracket_stack[-1] == "[":
                bracket_stack.pop()

        closing_map = {"[": "]", "{": "}"}
        suffix = "".join(closing_map[b] for b in reversed(bracket_stack))
        return truncated + suffix

    @staticmethod
    def _fix_unquoted_keys(text: str) -> str:
        """ç»™æœªåŠ å¼•å·çš„ JSON key è¡¥ä¸ŠåŒå¼•å·ã€‚
        ä¾‹: MINDMAP: "..." â†’ "MINDMAP": "..."
        ä»…å¤„ç†å­—ç¬¦ä¸²å¤–éƒ¨ã€çœ‹èµ·æ¥åƒ JSON key çš„è£¸å•è¯ã€‚"""
        result: list[str] = []
        i = 0
        in_string = False
        esc = False
        length = len(text)

        while i < length:
            ch = text[i]
            if esc:
                result.append(ch)
                esc = False
                i += 1
                continue
            if in_string:
                result.append(ch)
                if ch == "\\":
                    esc = True
                elif ch == '"':
                    in_string = False
                i += 1
                continue

            if ch == '"':
                result.append(ch)
                in_string = True
                i += 1
                continue

            # å­—ç¬¦ä¸²å¤–éƒ¨ï¼šæ£€æµ‹è£¸ keyï¼ˆå­—æ¯/ä¸‹åˆ’çº¿å¼€å¤´ï¼Œåè·Ÿå†’å·ï¼‰
            if ch.isalpha() or ch == "_":
                j = i + 1
                while j < length and (text[j].isalnum() or text[j] == "_"):
                    j += 1
                # è·³è¿‡ç©ºç™½åæ£€æŸ¥æ˜¯å¦ç´§è·Ÿå†’å·
                k = j
                while k < length and text[k] in (" ", "\t"):
                    k += 1
                if k < length and text[k] == ":":
                    bare_key = text[i:j]
                    result.append(f'"{bare_key}"')
                    i = j
                    continue
            result.append(ch)
            i += 1

        return "".join(result)

    @staticmethod
    def _fix_backtick_as_quote(text: str) -> str:
        """ä¿®å¤åå¼•å·è¢«è¯¯ç”¨ä¸ºåŒå¼•å·çš„æƒ…å†µã€‚
        ä¾‹: "explanation`: "..." â†’ "explanation": "..."
        ä»…æ›¿æ¢ç´§é‚»å†’å·çš„åå¼•å·ï¼ˆkey å°¾éƒ¨ï¼‰å’Œç´§é‚»å†’å·åçš„åå¼•å·ï¼ˆvalue å¤´éƒ¨ï¼‰ã€‚"""
        # key å°¾éƒ¨: `": â†’ "":   (åå¼•å·æ›¿ä»£äº† key çš„é—­åˆå¼•å·)
        text = re.sub(r"`(\s*:)", r'"\1', text)
        # value å¤´éƒ¨: : `  â†’ : "  (åå¼•å·æ›¿ä»£äº† value çš„å¼€å¯å¼•å·)
        text = re.sub(r"(:\s*)`", r'\1"', text)
        return text

    @classmethod
    def _try_repair_json(cls, text: str) -> dict[str, Any] | None:
        """å¤šè½®å°è¯•ä¿®å¤ JSON å“åº”ã€‚

        ä¿®å¤ç­–ç•¥ (é€è½®å°è¯•, æˆåŠŸå³è¿”å›):
          ç¬¬ 0 è½®: ç›´æ¥è§£æ (å¿«é€Ÿè·¯å¾„)
          ç¬¬ 1 è½®: ä¿®å¤éæ³•è½¬ä¹‰åºåˆ— (LaTeX å…¬å¼ç­‰)
          ç¬¬ 1.5 è½®: ä¿®å¤åå¼•å·è¯¯ç”¨ + æœªåŠ å¼•å·çš„ key
          ç¬¬ 2 è½®: ä¿®å¤è½¬ä¹‰ + é—­åˆæˆªæ–­
          ç¬¬ 3 è½®: ä¿®å¤è½¬ä¹‰ + æˆªæ–­åˆ°æœ€åå®Œæ•´é¡¹
          ç¬¬ 4 è½®: ç§»é™¤æ— æ³•ä¿®å¤çš„æ§åˆ¶å­—ç¬¦åé‡è¯•
        """

        # --- ç¬¬ 0 è½®: ç›´æ¥è§£æ ---
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 1 è½®: ä»…ä¿®å¤éæ³•è½¬ä¹‰ ---
        sanitized = cls._sanitize_json_escapes(text)
        try:
            return json.loads(sanitized)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 1.5 è½®: ä¿®å¤åå¼•å·è¯¯ç”¨ + æœªåŠ å¼•å·çš„ key ---
        patched = cls._fix_backtick_as_quote(sanitized)
        patched = cls._fix_unquoted_keys(patched)
        try:
            return json.loads(patched)
        except json.JSONDecodeError:
            pass

        patched_closed = cls._close_truncated_json(patched)
        try:
            return json.loads(patched_closed)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 2 è½®: ä¿®å¤è½¬ä¹‰ + é—­åˆæˆªæ–­ ---
        closed = cls._close_truncated_json(sanitized)
        try:
            return json.loads(closed)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 3 è½®: ä¿®å¤è½¬ä¹‰ + æˆªæ–­åˆ°æœ€åå®Œæ•´é¡¹ ---
        truncated = cls._truncate_to_last_complete_item(sanitized)
        if truncated:
            try:
                return json.loads(truncated)
            except json.JSONDecodeError:
                pass

            # é—­åˆæˆªæ–­åçš„ç»“æœ
            closed_truncated = cls._close_truncated_json(truncated)
            try:
                return json.loads(closed_truncated)
            except json.JSONDecodeError:
                pass

        # --- ç¬¬ 4 è½®: æ¸…ç†æ§åˆ¶å­—ç¬¦ ---
        # ç§»é™¤ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„è£¸æ§åˆ¶å­—ç¬¦ (\x00-\x1f é™¤ \t \n \r)
        cleaned = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f]", "", sanitized)
        closed_cleaned = cls._close_truncated_json(cleaned)
        try:
            return json.loads(closed_cleaned)
        except json.JSONDecodeError:
            pass

        return None

    def generate_report(
        self,
        analysis: AnalysisResult,
        image_relative_path: str | None = None,
        self_check_mode: str = "static",
    ) -> str:
        """
        ç”ŸæˆçŸ¥è¯†ç¬”è®°æŠ¥å‘Šçš„ Markdown æ ¼å¼

        Args:
            analysis: åˆ†æç»“æœå¯¹è±¡
            image_relative_path: çŸ¥è¯†è“å›¾å›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„ï¼ˆç”¨äºåµŒå…¥æŠ¥å‘Šï¼‰
            self_check_mode: è‡ªæµ‹é¢˜æ¸²æŸ“æ¨¡å¼(static/interactive/questions_only)

        Returns:
            Markdown æ ¼å¼çš„çŸ¥è¯†ç¬”è®°æŠ¥å‘Š
        """
        return analysis.to_markdown(
            image_paths=[image_relative_path] if image_relative_path else None,
            self_check_mode=self_check_mode,
        )

    def rewrite_visual_schema(
        self,
        original_structure: str,
        feedback: str,
    ) -> str:
        """
        æ ¹æ®åé¦ˆæ”¹å†™ Visual Schema
        """
        rewrite_prompt_template = self.prompts.get("gemini_rewrite", {}).get(
            "prompt", ""
        )

        full_prompt = render_prompt(
            rewrite_prompt_template,
            original_structure=original_structure,
            feedback=feedback,
        )

        return self._call_gemini_text_api(
            system_role="",
            user_prompt=full_prompt,
        )
