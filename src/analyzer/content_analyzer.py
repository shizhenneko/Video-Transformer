"""
å†…å®¹åˆ†æä¸æ–‡æ¡£ç”Ÿæˆæ¨¡å—

ä½¿ç”¨ Gemini 2.5 Flash å¯¹è§†é¢‘è¿›è¡Œå¤šæ¨¡æ€åˆ†æï¼Œç”Ÿæˆä¸­æ–‡ç²¾è‹±çŸ¥è¯†ç¬”è®°ã€‚
é€šè¿‡ä»£ç†å·æ± æœåŠ¡çš„ /sdk/allocate-key API è·å–çœŸå® API Keyï¼Œ
ç›´æ¥ä½¿ç”¨ google-genai SDK è°ƒç”¨ Gemini APIã€‚
"""

from __future__ import annotations

import re
import json
import logging
import subprocess
import tempfile
import time
from pathlib import Path
from typing import Any

from google import genai
from google.genai import types
import requests

from utils.counter import APICounter, APILimitExceeded
from utils.gemini_throttle import GeminiThrottle
from analyzer.models import AnalysisResult
from analyzer.prompt_loader import load_prompts, render_prompt


class ContentAnalyzer:
    """
    è§†é¢‘å†…å®¹åˆ†æå™¨

    è´Ÿè´£ä½¿ç”¨ Gemini 2.5 Flash åˆ†æè§†é¢‘å†…å®¹ï¼Œç”Ÿæˆç²¾è‹±çŸ¥è¯†ç¬”è®°ã€æœ¯è¯­è¡¨å’ŒçŸ¥è¯†è“å›¾ç»“æ„ã€‚
    é€šè¿‡ä»£ç†å·æ± æœåŠ¡åˆ†é…çœŸå® API Keyï¼ŒSDK ç›´è¿ Google APIã€‚
    """

    def __init__(
        self,
        config: dict[str, Any],
        api_counter: APICounter,
        logger: logging.Logger,
        api_key: str | None = None,
        throttle: GeminiThrottle | None = None,
    ):
        """
        åˆå§‹åŒ–å†…å®¹åˆ†æå™¨

        Args:
            config: ç³»ç»Ÿé…ç½®å­—å…¸
            api_counter: API è°ƒç”¨è®¡æ•°å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            api_key: Gemini API å¯†é’¥ï¼ˆå¯é€‰ï¼Œè‹¥æä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼Œä¸èµ°ä»£ç†å·æ± ï¼‰
        """
        self.config = config
        self.api_counter = api_counter
        self.logger = logger

        # åŠ è½½åˆ†æå™¨é…ç½®
        self.analyzer_config = config.get("analyzer", {})
        self.model_name = self.analyzer_config.get("model", "gemini-2.5-flash")
        self.temperature = self.analyzer_config.get("temperature", 0.7)
        self.max_output_tokens = self.analyzer_config.get("max_output_tokens", 65536)
        self.retry_times = self.analyzer_config.get("retry_times", 10)
        self.timeout = self.analyzer_config.get("timeout", 120)
        self.max_continuations = self.analyzer_config.get("max_continuations", 3)

        # é™æµå™¨
        min_interval = self.analyzer_config.get("min_call_interval", 4.0)
        max_retry_wait = self.analyzer_config.get("max_retry_wait", 600.0)
        self.throttle = throttle or GeminiThrottle(
            min_interval=min_interval,
            max_retries=self.retry_times,
            max_total_wait=max_retry_wait,
            logger=logger,
        )

        proxy_config = config.get("proxy", {})
        self.proxy_base_url = proxy_config.get("base_url", "http://localhost:8000")
        self.proxy_timeout = proxy_config.get("timeout", 10)

        self._fixed_api_key = api_key
        self._allocated_key_id = None  # å¦‚æœæ˜¯ä»å¤–éƒ¨ä¼ å…¥ä¸”å·²çŸ¥ ID,å¯ä»¥æ‰©å±•æ¥å£ä¼ è¾“ ID
        self._allocated_api_key = api_key
        self._client: genai.Client | None = None
        self._llm_repair_used = False

        http_proxy = proxy_config.get("http")

        if http_proxy:
            import os

            os.environ["HTTP_PROXY"] = http_proxy
            os.environ["HTTPS_PROXY"] = http_proxy
            # ç¡®ä¿æœ¬åœ°æœåŠ¡ä¸èµ°ä»£ç†
            os.environ["NO_PROXY"] = "localhost,127.0.0.1"
            self.logger.info(f"å·²è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡: {http_proxy}")

        if self._allocated_api_key:
            self._client = genai.Client(
                api_key=self._allocated_api_key,
                http_options={"timeout": 600_000},
            )
            self.logger.info("Gemini SDK é…ç½®å®Œæˆ(ä½¿ç”¨å¤–éƒ¨åˆ†é…çš„ API Key)")
        else:
            self.logger.warning("æœªæä¾› Gemini API Key,ContentAnalyzer å°†æ— æ³•æ­£å¸¸å·¥ä½œ")

        # åŠ è½½ Prompt æ¨¡æ¿
        self.prompts = load_prompts()
        self.logger.info("ContentAnalyzer åˆå§‹åŒ–å®Œæˆ")

    # _allocate_key_from_pool å·²ç§»é™¤,å¯†é’¥åˆ†é…é€»è¾‘å·²ç§»è‡³ VideoPipeline

    def _report_usage_to_pool(self) -> None:
        """å‘ä»£ç†å·æ± æŠ¥å‘Šä¸€æ¬¡æˆåŠŸçš„ API è°ƒç”¨ã€‚"""
        if not self._allocated_key_id:
            return
        url = f"{self.proxy_base_url.rstrip('/')}/sdk/report-usage"
        try:
            requests.post(
                url,
                json={"key_id": self._allocated_key_id},
                timeout=self.proxy_timeout,
            )
        except requests.RequestException as e:
            self.logger.warning(f"å‘å·æ± æŠ¥å‘Šç”¨é‡å¤±è´¥: {e}")

    def _report_error_to_pool(self, is_rpd_limit: bool = False) -> None:
        """å‘ä»£ç†å·æ± æŠ¥å‘Š API è°ƒç”¨é”™è¯¯ã€‚"""
        if not self._allocated_key_id:
            return
        url = f"{self.proxy_base_url.rstrip('/')}/sdk/report-error"
        try:
            requests.post(
                url,
                json={
                    "key_id": self._allocated_key_id,
                    "is_rpd_limit": is_rpd_limit,
                },
                timeout=self.proxy_timeout,
            )
        except requests.RequestException as e:
            self.logger.warning(f"å‘å·æ± æŠ¥å‘Šé”™è¯¯å¤±è´¥: {e}")

    def _delete_remote_file(self, file_name: str) -> None:
        """åˆ é™¤ Gemini Files å­˜å‚¨ä¸­çš„è¿œç¨‹æ–‡ä»¶ï¼Œé‡Šæ”¾é…é¢ç©ºé—´ã€‚"""
        if not self._client:
            return
        try:
            self.throttle.wait_for_files_op()
            self._client.files.delete(name=file_name)
            self.logger.info(f"å·²æ¸…ç† Gemini è¿œç¨‹æ–‡ä»¶: {file_name}")
        except Exception as e:
            self.logger.warning(f"æ¸…ç† Gemini è¿œç¨‹æ–‡ä»¶å¤±è´¥: {e}")

    def _compress_video_for_upload(self, video_path: Path) -> Path:
        """ç”¨ ffmpeg å‹ç¼©è§†é¢‘ä»¥å‡å°ä¸Šä¼ ä½“ç§¯ï¼Œè¿”å›å‹ç¼©åçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„ã€‚

        å‹ç¼©ç­–ç•¥: 360p + CRF 28ï¼Œå¯¹ Gemini å†…å®¹åˆ†æè¶³å¤Ÿï¼Œä½“ç§¯å¯é™è‡³åŸæ¥çš„ 1/5~1/10ã€‚
        å¦‚æœ ffmpeg ä¸å¯ç”¨æˆ–å‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€‚
        """
        max_size_mb = 30
        file_size_mb = video_path.stat().st_size / (1024 * 1024)
        if file_size_mb <= max_size_mb:
            self.logger.info(
                f"è§†é¢‘ä½“ç§¯ {file_size_mb:.1f}MB <= {max_size_mb}MBï¼Œè·³è¿‡å‹ç¼©"
            )
            return video_path

        self.logger.info(
            f"è§†é¢‘ä½“ç§¯ {file_size_mb:.1f}MB > {max_size_mb}MBï¼Œå¼€å§‹ ffmpeg å‹ç¼©..."
        )

        compressed_path = video_path.parent / f"compressed_{video_path.name}"

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‹ç¼©æ–‡ä»¶
        if compressed_path.exists() and compressed_path.stat().st_size > 0:
            self.logger.info(f"å‘ç°å·²å­˜åœ¨çš„å‹ç¼©æ–‡ä»¶: {compressed_path.name}ï¼Œè·³è¿‡å‹ç¼©")
            return compressed_path

        try:
            cmd = [
                "ffmpeg",
                "-y",
                "-i",
                str(video_path),
                "-vf",
                "scale=-2:360",
                "-c:v",
                "libx264",
                "-crf",
                "28",
                "-preset",
                "fast",
                "-c:a",
                "aac",
                "-b:a",
                "64k",
                str(compressed_path),
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,
            )
            if result.returncode != 0:
                self.logger.warning(f"ffmpeg å‹ç¼©å¤±è´¥: {(result.stderr or '')[:200]}")
                return video_path

            new_size_mb = compressed_path.stat().st_size / (1024 * 1024)
            self.logger.info(
                f"å‹ç¼©å®Œæˆ: {file_size_mb:.1f}MB -> {new_size_mb:.1f}MB "
                f"(å‹ç¼©ç‡ {new_size_mb / file_size_mb * 100:.0f}%)"
            )
            return compressed_path

        except FileNotFoundError:
            self.logger.warning("ffmpeg æœªå®‰è£…ï¼Œè·³è¿‡å‹ç¼©")
            return video_path
        except subprocess.TimeoutExpired:
            self.logger.warning("ffmpeg å‹ç¼©è¶…æ—¶(5åˆ†é’Ÿ)ï¼Œè·³è¿‡å‹ç¼©")
            if compressed_path.exists():
                compressed_path.unlink()
            return video_path

    def _upload_video(self, video_path: Path) -> Any:
        """
        å¤„ç†è§†é¢‘å‹ç¼©å’Œä¸Šä¼ 

        Returns:
             Gemini File å¯¹è±¡
        """
        upload_path = self._compress_video_for_upload(video_path)
        video_file = None

        try:
            self.logger.info(f"ä¸Šä¼ è§†é¢‘æ–‡ä»¶: {upload_path.name}")
            self.throttle.wait_for_files_op()
            video_file = self._client.files.upload(file=str(upload_path))

            while video_file.state.name == "PROCESSING":
                self.logger.info("ç­‰å¾…è§†é¢‘å¤„ç†...")
                self.throttle.wait_for_files_op()
                video_file = self._client.files.get(name=video_file.name)

            if video_file.state.name == "FAILED":
                raise RuntimeError(f"è§†é¢‘æ–‡ä»¶å¤„ç†å¤±è´¥: {video_file.state.name}")

            self.logger.info(f"è§†é¢‘æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {video_file.name}")
            return video_file

        except Exception as e:
            self.logger.error(f"è§†é¢‘ä¸Šä¼ å¤±è´¥: {e}")
            # å¦‚æœä¸Šä¼ å¤±è´¥ï¼Œæ¸…ç†è¿œç¨‹æ–‡ä»¶ï¼ˆå¦‚æœå·²åˆ›å»ºï¼‰
            if video_file:
                self._delete_remote_file(video_file.name)
            raise

    def analyze_video(self, video_path: str | Path) -> AnalysisResult:
        """
        åˆ†æè§†é¢‘å†…å®¹ï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ
        """
        self._llm_repair_used = False
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

        self.logger.info(f"å¼€å§‹åˆ†æè§†é¢‘: {video_path.name}")

        # æ£€æŸ¥ API è°ƒç”¨æ¬¡æ•° (é¢„ç•™ 2 æ¬¡: 1æ¬¡å†…å®¹åˆ†æ, 1æ¬¡ Schema ç”Ÿæˆ)
        if self.api_counter.current_count + 2 > self.api_counter.max_calls:
            raise APILimitExceeded(
                f"API è°ƒç”¨æ¬¡æ•°ä¸è¶³ä»¥å®Œæˆå…¨æµç¨‹: {self.api_counter.current_count}/{self.api_counter.max_calls}"
            )

        # Step 0: å‡†å¤‡è§†é¢‘æ–‡ä»¶ (å‹ç¼© + ä¸Šä¼ )
        # æ³¨æ„: è¿™é‡Œä¸è¿›è¡Œé‡è¯•ï¼Œå¦‚æœä¸Šä¼ å¤±è´¥é€šå¸¸æ˜¯ç½‘ç»œæˆ–æ–‡ä»¶é—®é¢˜ï¼Œé‡è¯•æ„ä¹‰ä¸å¤§æˆ–ç”±å¤–éƒ¨æ§åˆ¶
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")

        video_file = self._upload_video(video_path)

        try:
            # Step 1: è§†é¢‘å†…å®¹åˆ†æ (é€šè¿‡é™æµå™¨è‡ªåŠ¨é‡è¯• 429)
            self.logger.info("Step 1: æ‰§è¡Œè§†é¢‘å†…å®¹åˆ†æ...")
            system_role = self.prompts.get("gemini_analysis", {}).get("system_role", "")
            main_prompt = self.prompts.get("gemini_analysis", {}).get("main_prompt", "")

            def _on_retry(attempt: int, exc: Exception) -> None:
                """429 é‡è¯•å›è°ƒ: å‘å·æ± æŠ¥å‘Šé”™è¯¯"""
                self._report_error_to_pool(is_rpd_limit=True)

            # å†…å±‚é‡è¯•: JSON è§£æå¤±è´¥æ—¶é‡æ–°è¯·æ±‚ API (æœ€å¤š 2 æ¬¡é¢å¤–å°è¯•)
            json_parse_max_retries = 2
            response_data = None
            for json_attempt in range(
                1, json_parse_max_retries + 2
            ):  # 1 æ¬¡æ­£å¸¸ + 2 æ¬¡é‡è¯•
                try:
                    response_data = self.throttle.call_with_retry(
                        self._generate_content,
                        video_file,
                        system_role,
                        main_prompt,
                        on_retry_callback=_on_retry,
                    )
                    break  # æˆåŠŸåˆ™è·³å‡º
                except ValueError as ve:
                    if json_attempt <= json_parse_max_retries:
                        self.logger.warning(
                            f"âš ï¸ JSON è§£æå¤±è´¥ (ç¬¬ {json_attempt} æ¬¡)ï¼Œé‡æ–°è¯·æ±‚ API: {ve}"
                        )
                    else:
                        raise  # æœ€åä¸€æ¬¡ä»å¤±è´¥åˆ™æŠ›å‡º

            # å¢åŠ  API è®¡æ•°
            self.api_counter.increment("Gemini")

            # Step 2: ç”Ÿæˆ Visual Schema
            # å¦‚æœ Step 1 å·²ç»ç”Ÿæˆäº† Visual Schemaï¼Œåˆ™è·³è¿‡ Step 2
            # å¦åˆ™å°è¯•å•ç‹¬ç”Ÿæˆ (Fallback)

            raw_schemas = response_data.get("visual_schemas", [])
            has_valid_schema = any(
                isinstance(s, dict) and "---BEGIN PROMPT---" in s.get("schema", "")
                for s in (raw_schemas if isinstance(raw_schemas, list) else [])
            )

            if has_valid_schema:
                self.logger.info("Visual Schema å·²åœ¨ Step 1 ä¸­ç”Ÿæˆï¼Œè·³è¿‡ç‹¬ç«‹ç”Ÿæˆæ­¥éª¤")
            else:
                self.logger.info(
                    "Step 1 æœªç”Ÿæˆæœ‰æ•ˆ Visual Schemaï¼Œå°è¯•ç‹¬ç«‹ç”Ÿæˆ (Step 2)..."
                )
                if self.api_counter.current_count + 1 <= self.api_counter.max_calls:
                    try:
                        self.logger.info("Step 2: ç”ŸæˆçŸ¥è¯†è“å›¾ Visual Schema...")
                        deep_dive_content = json.dumps(
                            response_data.get("deep_dive", []),
                            ensure_ascii=False,
                            indent=2,
                        )
                        fallback_schema = self._generate_visual_schema(
                            deep_dive_content
                        )
                        self.logger.info("Visual Schema ç”ŸæˆæˆåŠŸ")
                        response_data["visual_schemas"] = [
                            {
                                "type": "overview",
                                "description": "æ€»è§ˆçŸ¥è¯†å¯¼å›¾",
                                "schema": fallback_schema,
                            }
                        ]
                    except Exception as e:
                        self.logger.error(
                            f"Visual Schema ç”Ÿæˆå¤±è´¥: {e}ï¼Œå°†ç”Ÿæˆä¸å¸¦å›¾çš„æŠ¥å‘Š"
                        )
                else:
                    self.logger.warning(
                        "API é…é¢ä¸è¶³ä»¥æ‰§è¡Œ Step 2ï¼Œè·³è¿‡ Visual Schema ç”Ÿæˆ"
                    )

            metadata = {
                "video_name": video_path.name,
                "video_size": video_path.stat().st_size,
            }

            try:
                result = AnalysisResult.from_api_response(
                    video_path=video_path,
                    response_data=response_data,
                    metadata=metadata,
                )
                self.logger.info(f"è§†é¢‘åˆ†æå…¨æµç¨‹å®Œæˆ: {result.title}")
                return result
            except ValueError as e:
                self.logger.error(f"API å“åº”æ ¼å¼é”™è¯¯: {e}")
                raise

        finally:
            # æ¸…ç†èµ„æº
            if video_file:
                self._delete_remote_file(video_file.name)

            # æ¸…ç†å‹ç¼©æ–‡ä»¶ (å¯é€‰: å¦‚æœå¸Œæœ›ä¸‹æ¬¡è¿è¡Œå¤ç”¨ï¼Œå¯ä»¥æ³¨é‡Šæ‰è¿™è¡Œï¼Œæˆ–è€…ä¿ç•™ä»¥èŠ‚çœç©ºé—´)
            # æ ¹æ®ç”¨æˆ·éœ€æ±‚ "æ–­ç‚¹å¤„ç»§ç»­"ï¼Œæˆ‘ä»¬åº”è¯¥ä¿ç•™å‹ç¼©æ–‡ä»¶ï¼Œæˆ–è€…åœ¨æˆåŠŸåæ‰æ¸…ç†ï¼Ÿ
            # ç°åœ¨çš„é€»è¾‘æ˜¯ `_compress_video_for_upload` ä¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚
            # å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œåˆ é™¤äº†ï¼Œé‚£ä¹ˆä¸‹æ¬¡åˆè¦é‡æ–°å‹ç¼©ã€‚
            # ä¸ºäº†æ”¯æŒ"æ–­ç‚¹å‹ç¼©"ï¼Œæˆ‘ä»¬åº”è¯¥ä»…åœ¨å®Œå…¨æˆåŠŸååˆ é™¤ï¼Œæˆ–è€…ä¿ç•™åœ¨ temp ç›®å½•ç”±ç³»ç»Ÿæ¸…ç†ï¼Ÿ
            # ä¹‹å‰çš„é€»è¾‘æ˜¯ `finally` ä¸­åˆ é™¤ã€‚
            # ç”¨æˆ·æŠ±æ€¨çš„æ˜¯ "å¤±è´¥å...é‡æ–°ä¸‹è½½å¹¶å‹ç¼©"ã€‚
            # å¦‚æœæˆ‘ä»¬åˆ é™¤äº†ï¼Œä¸‹æ¬¡ç¡®å®è¦é‡æ–°å‹ç¼©ã€‚
            # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬åº”è¯¥ç§»é™¤åˆ é™¤å‹ç¼©æ–‡ä»¶çš„é€»è¾‘ï¼Œæˆ–è€…åªåœ¨æˆåŠŸæ—¶åˆ é™¤ï¼Ÿ
            # å®é™…ä¸Šï¼Œ`VideoDownloader` æœ‰ä¸ª temp ç›®å½•ï¼Œå¯èƒ½æ›´é€‚åˆæ”¾é‚£é‡Œã€‚
            # è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯å…ˆä¿ç•™åˆ é™¤é€»è¾‘ï¼Œä½†æ˜¯å› ä¸º `_compress_video_for_upload` ç°åœ¨æ£€æŸ¥äº†å­˜åœ¨æ€§ï¼Œ
            # åªè¦åœ¨"æœ¬æ¬¡å…ƒæ“ä½œ"ä¸­ä¸åˆ é™¤ï¼Œä¸‹æ¬¡é‡è¯•ï¼ˆåŒä¸€è¿›ç¨‹å†…ï¼‰è¿˜æ˜¯ä¼šé€šè¿‡ path ä¼ é€’ã€‚
            # ä½†å¦‚æœæ˜¯è¿›ç¨‹å´©æºƒé‡å¯ï¼Œ`upload_path` æ˜¯ä¸´æ—¶ç”Ÿæˆçš„å—ï¼Ÿ
            # `video_path.parent / f"compressed_{video_path.name}"` æ˜¯åœ¨åŸç›®å½•ä¸‹ã€‚
            # å¦‚æœæˆ‘åˆ é™¤äº†ï¼Œä¸‹æ¬¡è¿›ç¨‹å¯åŠ¨è¿˜æ˜¯è¦å‹ç¼©ã€‚
            # é‰´äºç”¨æˆ·çš„éœ€æ±‚ï¼Œæˆ‘å°†æ³¨é‡Šæ‰åˆ é™¤å‹ç¼©æ–‡ä»¶çš„ä»£ç ï¼Œè®©å®ƒä¿ç•™ï¼Œ
            # æˆ–è€…å°†å…¶ç§»åŠ¨åˆ° VideoPipeline çš„æ¸…ç†é˜¶æ®µï¼Ÿ
            # ç®€å•èµ·è§ï¼Œæˆ‘å…ˆä¸åˆ é™¤å‹ç¼©æ–‡ä»¶ï¼Œè®©å®ƒå˜æˆ"ç¼“å­˜"ã€‚
            # ä¸ºäº†é¿å…åƒåœ¾å †ç§¯ï¼Œå¯ä»¥åœ¨ AnalysisResult æˆåŠŸè¿”å›å‰åˆ é™¤ï¼Ÿ
            # è¿˜æ˜¯è¯´ï¼Œç”¨æˆ·å¸Œæœ›çš„æ˜¯"å¤±è´¥é‡è¯•"æ—¶ä¸é‡æ–°å‹ç¼©ã€‚
            # å½“å‰çš„ `finally` å—æ˜¯åœ¨ `analyze_video` ç»“æŸæ—¶æ‰§è¡Œã€‚
            # å¦‚æœ `analyze_video` å¤±è´¥æŠ›å‡ºå¼‚å¸¸ï¼Œ`finally` æ‰§è¡Œï¼Œæ–‡ä»¶è¢«åˆ ã€‚
            # ä¸‹æ¬¡è°ƒç”¨ `analyze_video` (æ¯”å¦‚å¤–éƒ¨é‡è¯•)ï¼Œæ–‡ä»¶æ²¡äº†ï¼Œåˆè¦å‹ç¼©ã€‚
            # æ‰€ä»¥å¿…é¡» **ä¸åˆ é™¤** å‹ç¼©æ–‡ä»¶ï¼Œæˆ–è€…åªåœ¨ **æˆåŠŸ** ååˆ é™¤ã€‚

            compressed_file = video_path.parent / f"compressed_{video_path.name}"
            if compressed_file.exists():
                # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆç»“æœåæ‰åˆ é™¤ï¼Ÿæˆ–è€…å¹²è„†ä¸åˆ ï¼Œç•™ç»™ç”¨æˆ·æ‰‹åŠ¨æ¸…ç†/å®šæœŸæ¸…ç†ï¼Ÿ
                # ä¸ºäº†é˜²æ­¢ç£ç›˜çˆ†æ»¡ï¼Œæˆ‘ä»¬è¿˜æ˜¯å°è¯•åˆ é™¤ï¼Œä½†æ˜¯å‰ææ˜¯å¿…é¡»åŒºåˆ†"å®Œå…¨å¤±è´¥"å’Œ"æˆåŠŸ"ã€‚
                # ç”±äº `finally` ä¸åŒºåˆ†ï¼Œæˆ‘ä»¬å¾ˆéš¾åšã€‚
                # æœ€å¥½çš„æ–¹å¼æ˜¯ï¼šä¸åœ¨è¿™é‡Œåˆ é™¤ã€‚è®©ä¸Šå±‚ `VideoPipeline` æˆ–è€… `cleanup` æ–¹æ³•æ¥å¤„ç†ã€‚
                # æˆ–è€…ï¼Œä»…ä»…è®°å½•æ—¥å¿—è¯´"ä¿ç•™å‹ç¼©æ–‡ä»¶ä»¥ä¾¿é‡è¯•"ã€‚
                self.logger.info(f"ä¿ç•™å‹ç¼©æ–‡ä»¶ä»¥å¤‡é‡è¯•: {compressed_file.name}")
                pass

    def _generate_visual_schema(self, deep_dive_content: str) -> str:
        """æ ¹æ®æ·±åº¦è§£æå†…å®¹ç”Ÿæˆ Visual Schema"""

        schema_config = self.prompts.get("gemini_visual_schema", {})
        system_role = schema_config.get("system_role", "")
        main_prompt_template = schema_config.get("main_prompt", "")

        full_prompt = render_prompt(
            main_prompt_template, deep_dive_content=deep_dive_content
        )

        # ä½¿ç”¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæ¥å£
        response_text = self._call_gemini_text_api(
            system_role=system_role, user_prompt=full_prompt, temperature=0.7
        )

        # æå– Markdown ä»£ç å—
        if "```markdown" in response_text:
            start = response_text.find("```markdown") + 11
            end = response_text.find("```", start)
            return response_text[start:end].strip()
        elif "```" in response_text:
            start = response_text.find("```") + 3
            end = response_text.find("```", start)
            return response_text[start:end].strip()

        return response_text.strip()

    def _stream_response(
        self,
        contents: list[dict[str, Any]],
        gen_config: types.GenerateContentConfig,
    ) -> tuple[str, str]:
        """æµå¼æ¥æ”¶ Gemini å“åº”ï¼Œè¿”å› (response_text, finish_reason_name)ã€‚

        è¿™æ˜¯ _generate_content å’Œ _call_gemini_text_api å…±ç”¨çš„åº•å±‚æµå¼æ¥æ”¶æ–¹æ³•ã€‚

        Args:
            contents: å¤šè½®å¯¹è¯å†…å®¹åˆ—è¡¨
            gen_config: ç”Ÿæˆé…ç½®

        Returns:
            (response_text, finish_reason_name) å…ƒç»„ã€‚
            finish_reason_name ä¸º "STOP"ã€"MAX_TOKENS" ç­‰å­—ç¬¦ä¸²ï¼Œ
            å¦‚æœæœªè·å–åˆ° finish_reason åˆ™è¿”å› "UNKNOWN"ã€‚
        """
        response_text_parts: list[str] = []
        thinking_logged = False
        finish_reason_name = "UNKNOWN"

        for chunk in self._client.models.generate_content_stream(
            model=self.model_name,
            contents=contents,
            config=gen_config,
        ):
            if not chunk.candidates:
                continue

            candidate = chunk.candidates[0]

            if candidate.finish_reason:
                finish_reason_name = candidate.finish_reason.name or "UNKNOWN"

            if not candidate.content or not candidate.content.parts:
                continue

            for part in candidate.content.parts:
                if part.thought:
                    if not thinking_logged:
                        self.logger.info("ğŸ’­ Gemini æ€è€ƒä¸­...")
                        thinking_logged = True
                    snippet = (part.text or "")[:200] if part.text else ""
                    if snippet:
                        self.logger.info(f"  ğŸ’­ {snippet}")
                else:
                    if part.text:
                        response_text_parts.append(part.text)
                        snippet = (part.text or "")[:100].replace("\n", " ")
                        self.logger.info(f"  ğŸ“ ç”Ÿæˆä¸­: {snippet}...")

        response_text = "".join(response_text_parts)
        return response_text, finish_reason_name

    def _stream_with_continuation(
        self,
        contents: list[dict[str, Any]],
        gen_config: types.GenerateContentConfig,
        continuation_prompt: str,
    ) -> str:
        """æµå¼æ¥æ”¶ + MAX_TOKENS è‡ªåŠ¨ç»­ä¼ ã€‚

        å½“ finish_reason ä¸º MAX_TOKENS æ—¶ï¼Œå°†å·²æœ‰å†…å®¹ä½œä¸º model å›å¤è¿½åŠ åˆ°å¯¹è¯å†å²ï¼Œ
        å†å‘é€ç»­ä¼ æŒ‡ä»¤ï¼Œè®© Gemini ä»æˆªæ–­å¤„ç»§ç»­è¾“å‡ºã€‚æœ€å¤šç»­ä¼  max_continuations è½®ã€‚

        Args:
            contents: åˆå§‹å¯¹è¯å†…å®¹åˆ—è¡¨ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ä»¥è¿½åŠ ç»­ä¼ è½®æ¬¡ï¼‰
            gen_config: ç”Ÿæˆé…ç½®
            continuation_prompt: ç»­ä¼ æ—¶å‘é€ç»™ Gemini çš„ç”¨æˆ·æŒ‡ä»¤

        Returns:
            æ‹¼æ¥åçš„å®Œæ•´å“åº”æ–‡æœ¬
        """
        all_text_parts: list[str] = []

        for round_idx in range(self.max_continuations + 1):
            round_label = f"ç¬¬ {round_idx + 1} è½®" if round_idx > 0 else "é¦–æ¬¡è¯·æ±‚"
            self.logger.info(f"å¼€å§‹æµå¼æ¥æ”¶ Gemini å“åº” ({round_label})...")

            response_text, finish_reason = self._stream_response(contents, gen_config)

            self.logger.info(
                f"æµå¼æ¥æ”¶å®Œæˆ ({round_label})ï¼Œ"
                f"æœ¬è½®é•¿åº¦: {len(response_text)} å­—ç¬¦ï¼Œ"
                f"finish_reason: {finish_reason}"
            )

            all_text_parts.append(response_text)
            self._report_usage_to_pool()

            if finish_reason != "MAX_TOKENS":
                if finish_reason == "STOP":
                    self.logger.info("Gemini ç”Ÿæˆæ­£å¸¸ç»“æŸ (STOP)")
                else:
                    self.logger.warning(
                        f"Gemini ç”Ÿæˆç»“æŸåŸå› é STOP: {finish_reason} (å¯èƒ½å‘ç”Ÿæˆªæ–­)"
                    )
                break

            if round_idx >= self.max_continuations:
                self.logger.warning(
                    f"å·²è¾¾æœ€å¤§ç»­ä¼ è½®æ•° ({self.max_continuations})ï¼Œåœæ­¢ç»­ä¼ "
                )
                break

            self.logger.info(
                f"âš ï¸ æ£€æµ‹åˆ° MAX_TOKENS æˆªæ–­ï¼Œå‘èµ·ç»­ä¼  "
                f"(ç¬¬ {round_idx + 2}/{self.max_continuations + 1} è½®)..."
            )

            contents.append({"role": "model", "parts": [{"text": response_text}]})
            contents.append({"role": "user", "parts": [{"text": continuation_prompt}]})

            self.throttle.wait_before_call()

        total_text = "".join(all_text_parts)
        self.logger.info(
            f"å“åº”æ€»é•¿åº¦: {len(total_text)} å­—ç¬¦ (å…± {len(all_text_parts)} è½®)"
        )
        return total_text

    def _generate_content(
        self,
        video_file: Any,
        system_role: str,
        main_prompt: str,
    ) -> dict[str, Any]:
        """
        ä½¿ç”¨å·²ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶ç”Ÿæˆå†…å®¹
        """
        # Disable thinking for JSON mode to avoid empty-response edge case
        gen_config = types.GenerateContentConfig(
            temperature=self.temperature,
            max_output_tokens=self.max_output_tokens,
            response_mime_type="application/json",
            system_instruction=system_role,
            thinking_config=None,
        )

        contents: list[dict[str, Any]] = [
            {
                "role": "user",
                "parts": [
                    {
                        "file_data": {
                            "file_uri": video_file.uri,
                            "mime_type": video_file.mime_type,
                        }
                    },
                    {"text": main_prompt},
                ],
            }
        ]

        continuation_prompt = (
            "ä½ çš„ä¸Šä¸€æ¬¡è¾“å‡ºå› é•¿åº¦é™åˆ¶è¢«æˆªæ–­äº†ã€‚"
            "è¯·ä»ä¸Šæ¬¡æˆªæ–­å¤„ç»§ç»­è¾“å‡ºï¼Œç›´æ¥ç»­å†™ JSON å†…å®¹ï¼Œ"
            "ä¸è¦é‡å¤å·²è¾“å‡ºçš„éƒ¨åˆ†ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€è¯´æ˜ã€‚"
        )

        response_text = self._stream_with_continuation(
            contents, gen_config, continuation_prompt
        )

        response_text = response_text.strip()

        # ç©ºå“åº”æ£€æµ‹ï¼šGemini å¯èƒ½åªè¿”å› thinking å†…å®¹è€Œæ— å®é™…æ–‡æœ¬
        if not response_text:
            raise ValueError("Gemini è¿”å›äº†ç©ºå“åº”ï¼ˆ0 å­—ç¬¦ï¼‰ï¼Œå¯èƒ½ä»…åŒ…å« thinking å†…å®¹")

        self.logger.debug(f"API å“åº”: {response_text[:200]}...")

        # è§£æ JSON å“åº”
        # 1. ä¼˜å…ˆå°è¯•æ ‡å‡†çš„ ```json ... ``` å—
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        else:
            # 2. å°è¯•ä¸å¸¦ json æ ‡ç­¾çš„ ``` ... ``` å—ï¼Œä½†æ’é™¤é JSON çš„ä»£ç å—
            code_block_match = re.search(
                r"```\s*(\{.*?\})\s*```", response_text, re.DOTALL
            )
            if code_block_match:
                response_text = code_block_match.group(1)
            else:
                # 3. å…œåº•ï¼šå¯»æ‰¾æœ€å¤–å±‚çš„ {}
                start_idx = response_text.find("{")
                end_idx = response_text.rfind("}")
                if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                    response_text = response_text[start_idx : end_idx + 1]

        # å¤šè½® JSON ä¿®å¤
        cleaned_text, stripped_count = self._strip_stray_token_prefixes(response_text)
        if stripped_count > 0:
            self.logger.debug(f"event=json_stray_token_strip count={stripped_count}")
        repaired = self._try_repair_json(cleaned_text)
        if repaired is not None:
            self.logger.info("API å“åº”è§£ææˆåŠŸ")
            response_data = repaired
        else:
            if not self._llm_repair_used:
                llm_repaired = self._llm_repair_json(cleaned_text)
                self._llm_repair_used = True
                if llm_repaired is not None:
                    response_data = llm_repaired
                else:
                    self.logger.error(
                        "event=json_parse_failed reason=llm_repair_exhausted"
                    )
                    self._dump_failed_json(response_text)
                    raise ValueError("JSON parse failed after LLM repair")
            else:
                self.logger.error(
                    "event=json_parse_failed reason=llm_repair_already_used"
                )
                self._dump_failed_json(response_text)
                raise ValueError("LLM repair already used for this video")

        # å¿…éœ€å­—æ®µï¼šç¼ºå¤±åˆ™æ— æ³•æ„å»ºæœ‰æ„ä¹‰çš„æ–‡æ¡£
        required_fields = {
            "title",
            "one_sentence_summary",
            "key_takeaways",
            "deep_dive",
            "glossary",
        }
        # å¯é€‰å­—æ®µï¼švisual_schemas ç¼ºå¤±æ—¶ç”± Step 2 fallback å•ç‹¬ç”Ÿæˆ
        missing = required_fields - response_data.keys()
        if missing:
            self.logger.error(
                f"event=validation_failed reason=missing_required_fields fields={','.join(sorted(missing))}"
            )
            self.logger.warning(
                f"API å“åº” JSON ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(sorted(missing))}ï¼Œè§¦å‘é‡è¯•"
            )
            raise ValueError(
                f"API å“åº” JSON ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(sorted(missing))}"
            )

        return response_data

    def _llm_repair_json(self, invalid_json: str) -> dict[str, Any] | None:
        prompt = (
            "The following is invalid JSON. Output ONLY the corrected JSON "
            "with no markdown fences, explanations, or other text. "
            "Preserve all content and meaning exactly.\n\n"
            f"{invalid_json}"
        )

        self.logger.info("event=llm_json_repair_attempt")
        try:
            response_text = self._call_gemini_text_api(
                system_role="",
                user_prompt=prompt,
                temperature=0.0,
                max_output_tokens=8192,
            )
            parsed = json.loads(response_text)
        except Exception:
            self.logger.warning("event=llm_json_repair_failed")
            return None

        if isinstance(parsed, dict):
            self.logger.info("event=llm_json_repair_success")
            return parsed

        self.logger.warning("event=llm_json_repair_failed")
        return None

    @staticmethod
    def _dump_failed_json(response_text: str) -> None:
        # ä¿®å¤å¤±è´¥ï¼šä¿å­˜åŸå§‹å“åº”åˆ°ç£ç›˜ä»¥ä¾¿äº‹åè°ƒè¯•
        try:
            dump_path = Path("data/output/logs") / f"failed_json_{int(time.time())}.txt"
            dump_path.parent.mkdir(parents=True, exist_ok=True)
            dump_path.write_text(response_text, encoding="utf-8")
            logging.getLogger(__name__).error(
                f"JSON ä¿®å¤å¤±è´¥ï¼Œå·²ä¿å­˜åŸå§‹å“åº”åˆ°: {dump_path}"
            )
        except Exception:
            logging.getLogger(__name__).error("JSON ä¿®å¤å¤±è´¥ï¼Œä¸”æ— æ³•ä¿å­˜åŸå§‹å“åº”åˆ°ç£ç›˜")

    def _call_gemini_text_api(
        self,
        system_role: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_output_tokens: int = 8192,
    ) -> str:
        """è°ƒç”¨ Gemini çº¯æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼ˆé€šè¿‡é™æµå™¨è‡ªåŠ¨å¤„ç† 429ï¼‰"""
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")

        def _do_text_call() -> str:
            if not self._client:
                if self._allocated_api_key:
                    self._client = genai.Client(
                        api_key=self._allocated_api_key,
                        http_options={"timeout": 600_000},
                    )
                else:
                    raise RuntimeError("Gemini Client æœªåˆå§‹åŒ–ä¸”æ— å¯ç”¨ Key")

            gen_config = types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_output_tokens,
                system_instruction=system_role if system_role else None,
                thinking_config=types.ThinkingConfig(
                    thinking_budget=4096,
                ),
            )

            contents: list[dict[str, Any]] = [
                {"role": "user", "parts": [{"text": user_prompt}]}
            ]

            continuation_prompt = (
                "ä½ çš„ä¸Šä¸€æ¬¡è¾“å‡ºå› é•¿åº¦é™åˆ¶è¢«æˆªæ–­äº†ã€‚"
                "è¯·ä»ä¸Šæ¬¡æˆªæ–­å¤„ç»§ç»­è¾“å‡ºï¼Œç›´æ¥ç»­å†™å†…å®¹ï¼Œ"
                "ä¸è¦é‡å¤å·²è¾“å‡ºçš„éƒ¨åˆ†ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€è¯´æ˜ã€‚"
            )

            result = self._stream_with_continuation(
                contents, gen_config, continuation_prompt
            )

            self.api_counter.increment("Gemini")
            return result.strip()

        def _on_retry(attempt: int, exc: Exception) -> None:
            self._report_error_to_pool(is_rpd_limit=True)

        return self.throttle.call_with_retry(
            _do_text_call,
            on_retry_callback=_on_retry,
        )

    @staticmethod
    def _strip_stray_token_prefixes(text: str) -> tuple[str, int]:
        start_idx = text.find("{")
        end_idx = text.rfind("}")
        if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
            return text, 0

        prefix = text[: start_idx + 1]
        body = text[start_idx + 1 : end_idx]
        suffix = text[end_idx:]

        cleaned_body, count = re.subn(
            r'(?<=\n)\s*[A-Za-z0-9]\s+(?=")',
            "",
            body,
        )
        if count == 0:
            return text, 0
        return f"{prefix}{cleaned_body}{suffix}", count

    @staticmethod
    def _sanitize_json_escapes(text: str) -> str:
        """ä¿®å¤ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„éæ³•è½¬ä¹‰åºåˆ—ã€‚

        å¸¸è§åœºæ™¯: Gemini è¿”å›çš„ JSON ä¸­åŒ…å« LaTeX å…¬å¼ (\frac, \sum, \ln ç­‰),
        è¿™äº›åœ¨ JSON è§„èŒƒä¸­æ˜¯éæ³•çš„è½¬ä¹‰åºåˆ—ã€‚

        ç­–ç•¥: ä»…åœ¨å­—ç¬¦ä¸²å€¼å†…éƒ¨, å°† `\X` (X é JSON åˆæ³•è½¬ä¹‰å­—ç¬¦) æ›¿æ¢ä¸º `\\X`ã€‚
        JSON åˆæ³•è½¬ä¹‰å­—ç¬¦: " \\ / b f n r t u
        """
        legal_escapes = set('"\\/ b f n r t u'.split() + ['"', "\\", "/"])
        # æ›´ç²¾ç¡®: JSON å…è®¸ \" \\\\ \/ \b \f \n \r \t \uXXXX
        legal_escape_chars = set('"\\/ bfnrtu')

        result: list[str] = []
        i = 0
        in_string = False
        length = len(text)

        while i < length:
            ch = text[i]

            if not in_string:
                result.append(ch)
                if ch == '"':
                    in_string = True
                i += 1
            else:
                # åœ¨å­—ç¬¦ä¸²å†…éƒ¨
                if ch == "\\":
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦
                    if i + 1 < length:
                        next_ch = text[i + 1]
                        if next_ch in legal_escape_chars:
                            # åˆæ³•è½¬ä¹‰, åŸæ ·ä¿ç•™
                            result.append(ch)
                            result.append(next_ch)
                            i += 2
                        else:
                            # éæ³•è½¬ä¹‰ (å¦‚ \frac, \sum): æ›¿æ¢ä¸º \\\\
                            result.append("\\\\")
                            # ä¸è·³è¿‡ next_ch, è®©å®ƒä½œä¸ºæ™®é€šå­—ç¬¦å¤„ç†
                            i += 1
                    else:
                        # åæ–œæ åœ¨æœ«å°¾, è½¬ä¹‰å®ƒ
                        result.append("\\\\")
                        i += 1
                elif ch == '"':
                    result.append(ch)
                    in_string = False
                    i += 1
                else:
                    result.append(ch)
                    i += 1

        return "".join(result)

    @staticmethod
    def _close_truncated_json(text: str) -> str:
        """é—­åˆè¢«æˆªæ–­çš„ JSON: æœªé—­åˆçš„å­—ç¬¦ä¸²ã€å°¾éƒ¨é€—å·ã€æœªé—­åˆçš„æ‹¬å·ã€‚"""
        text = text.rstrip()

        # 1. é—­åˆæœªé—­åˆçš„å­—ç¬¦ä¸²
        in_string = False
        escape_next = False
        for ch in text:
            if escape_next:
                escape_next = False
                continue
            if ch == "\\":
                escape_next = True
                continue
            if ch == '"':
                in_string = not in_string

        if in_string:
            text += '"'

        # 2. å»é™¤å°¾éƒ¨é€—å·
        text = text.rstrip().rstrip(",")

        # 3. ç»Ÿè®¡æœªé—­åˆçš„æ‹¬å·å¹¶é€†åºé—­åˆ
        bracket_stack: list[str] = []
        in_str = False
        esc = False
        for ch in text:
            if esc:
                esc = False
                continue
            if ch == "\\" and in_str:
                esc = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch in ("{", "["):
                bracket_stack.append(ch)
            elif ch == "}" and bracket_stack and bracket_stack[-1] == "{":
                bracket_stack.pop()
            elif ch == "]" and bracket_stack and bracket_stack[-1] == "[":
                bracket_stack.pop()

        closing_map = {"[": "]", "{": "}"}
        suffix = "".join(closing_map[b] for b in reversed(bracket_stack))
        return text + suffix

    @staticmethod
    def _truncate_to_last_complete_item(text: str) -> str | None:
        """æˆªæ–­åˆ°æœ€åä¸€ä¸ªé€—å·å¤„, ç„¶åé‡æ–°é—­åˆæ‹¬å·ã€‚ç”¨äºä¸¢å¼ƒè¢«æˆªæ–­çš„æœ€åä¸€ä¸ªå…ƒç´ ã€‚"""
        last_comma = text.rfind(",")
        if last_comma <= 0:
            return None

        truncated = text[:last_comma]

        # é‡æ–°æ‰«æå¹¶é—­åˆ
        bracket_stack: list[str] = []
        in_str = False
        esc = False
        for ch in truncated:
            if esc:
                esc = False
                continue
            if ch == "\\" and in_str:
                esc = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch in ("{", "["):
                bracket_stack.append(ch)
            elif ch == "}" and bracket_stack and bracket_stack[-1] == "{":
                bracket_stack.pop()
            elif ch == "]" and bracket_stack and bracket_stack[-1] == "[":
                bracket_stack.pop()

        closing_map = {"[": "]", "{": "}"}
        suffix = "".join(closing_map[b] for b in reversed(bracket_stack))
        return truncated + suffix

    @staticmethod
    def _fix_unquoted_keys(text: str) -> str:
        """ç»™æœªåŠ å¼•å·çš„ JSON key è¡¥ä¸ŠåŒå¼•å·ã€‚
        ä¾‹: MINDMAP: "..." â†’ "MINDMAP": "..."
        ä»…å¤„ç†å­—ç¬¦ä¸²å¤–éƒ¨ã€çœ‹èµ·æ¥åƒ JSON key çš„è£¸å•è¯ã€‚"""
        result: list[str] = []
        i = 0
        in_string = False
        esc = False
        length = len(text)

        while i < length:
            ch = text[i]
            if esc:
                result.append(ch)
                esc = False
                i += 1
                continue
            if in_string:
                result.append(ch)
                if ch == "\\":
                    esc = True
                elif ch == '"':
                    in_string = False
                i += 1
                continue

            if ch == '"':
                result.append(ch)
                in_string = True
                i += 1
                continue

            # å­—ç¬¦ä¸²å¤–éƒ¨ï¼šæ£€æµ‹è£¸ keyï¼ˆå­—æ¯/ä¸‹åˆ’çº¿å¼€å¤´ï¼Œåè·Ÿå†’å·ï¼‰
            if ch.isalpha() or ch == "_":
                j = i + 1
                while j < length and (text[j].isalnum() or text[j] == "_"):
                    j += 1
                # è·³è¿‡ç©ºç™½åæ£€æŸ¥æ˜¯å¦ç´§è·Ÿå†’å·
                k = j
                while k < length and text[k] in (" ", "\t"):
                    k += 1
                if k < length and text[k] == ":":
                    bare_key = text[i:j]
                    result.append(f'"{bare_key}"')
                    i = j
                    continue
            result.append(ch)
            i += 1

        return "".join(result)

    @staticmethod
    def _fix_backtick_as_quote(text: str) -> str:
        """ä¿®å¤åå¼•å·è¢«è¯¯ç”¨ä¸ºåŒå¼•å·çš„æƒ…å†µã€‚
        ä¾‹: "explanation`: "..." â†’ "explanation": "..."
        ä»…æ›¿æ¢ç´§é‚»å†’å·çš„åå¼•å·ï¼ˆkey å°¾éƒ¨ï¼‰å’Œç´§é‚»å†’å·åçš„åå¼•å·ï¼ˆvalue å¤´éƒ¨ï¼‰ã€‚"""
        # key å°¾éƒ¨: `": â†’ "":   (åå¼•å·æ›¿ä»£äº† key çš„é—­åˆå¼•å·)
        text = re.sub(r"`(\s*:)", r'"\1', text)
        # value å¤´éƒ¨: : `  â†’ : "  (åå¼•å·æ›¿ä»£äº† value çš„å¼€å¯å¼•å·)
        text = re.sub(r"(:\s*)`", r'\1"', text)
        return text

    @classmethod
    def _try_repair_json(cls, text: str) -> dict[str, Any] | None:
        """å¤šè½®å°è¯•ä¿®å¤ JSON å“åº”ã€‚

        ä¿®å¤ç­–ç•¥ (é€è½®å°è¯•, æˆåŠŸå³è¿”å›):
          ç¬¬ 0 è½®: ç›´æ¥è§£æ (å¿«é€Ÿè·¯å¾„)
          ç¬¬ 1 è½®: ä¿®å¤éæ³•è½¬ä¹‰åºåˆ— (LaTeX å…¬å¼ç­‰)
          ç¬¬ 1.5 è½®: ä¿®å¤åå¼•å·è¯¯ç”¨ + æœªåŠ å¼•å·çš„ key
          ç¬¬ 2 è½®: ä¿®å¤è½¬ä¹‰ + é—­åˆæˆªæ–­
          ç¬¬ 3 è½®: ä¿®å¤è½¬ä¹‰ + æˆªæ–­åˆ°æœ€åå®Œæ•´é¡¹
          ç¬¬ 4 è½®: ç§»é™¤æ— æ³•ä¿®å¤çš„æ§åˆ¶å­—ç¬¦åé‡è¯•
        """

        # --- ç¬¬ 0 è½®: ç›´æ¥è§£æ ---
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 1 è½®: ä»…ä¿®å¤éæ³•è½¬ä¹‰ ---
        sanitized = cls._sanitize_json_escapes(text)
        try:
            return json.loads(sanitized)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 1.5 è½®: ä¿®å¤åå¼•å·è¯¯ç”¨ + æœªåŠ å¼•å·çš„ key ---
        patched = cls._fix_backtick_as_quote(sanitized)
        patched = cls._fix_unquoted_keys(patched)
        try:
            return json.loads(patched)
        except json.JSONDecodeError:
            pass

        patched_closed = cls._close_truncated_json(patched)
        try:
            return json.loads(patched_closed)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 2 è½®: ä¿®å¤è½¬ä¹‰ + é—­åˆæˆªæ–­ ---
        closed = cls._close_truncated_json(sanitized)
        try:
            return json.loads(closed)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 3 è½®: ä¿®å¤è½¬ä¹‰ + æˆªæ–­åˆ°æœ€åå®Œæ•´é¡¹ ---
        truncated = cls._truncate_to_last_complete_item(sanitized)
        if truncated:
            try:
                return json.loads(truncated)
            except json.JSONDecodeError:
                pass

            # é—­åˆæˆªæ–­åçš„ç»“æœ
            closed_truncated = cls._close_truncated_json(truncated)
            try:
                return json.loads(closed_truncated)
            except json.JSONDecodeError:
                pass

        # --- ç¬¬ 4 è½®: æ¸…ç†æ§åˆ¶å­—ç¬¦ ---
        # ç§»é™¤ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„è£¸æ§åˆ¶å­—ç¬¦ (\x00-\x1f é™¤ \t \n \r)
        cleaned = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f]", "", sanitized)
        closed_cleaned = cls._close_truncated_json(cleaned)
        try:
            return json.loads(closed_cleaned)
        except json.JSONDecodeError:
            pass

        return None

    def generate_report(
        self,
        analysis: AnalysisResult,
        image_relative_path: str | None = None,
        self_check_mode: str = "static",
    ) -> str:
        """
        ç”ŸæˆçŸ¥è¯†ç¬”è®°æŠ¥å‘Šçš„ Markdown æ ¼å¼

        Args:
            analysis: åˆ†æç»“æœå¯¹è±¡
            image_relative_path: çŸ¥è¯†è“å›¾å›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„ï¼ˆç”¨äºåµŒå…¥æŠ¥å‘Šï¼‰
            self_check_mode: è‡ªæµ‹é¢˜æ¸²æŸ“æ¨¡å¼(static/interactive/questions_only)

        Returns:
            Markdown æ ¼å¼çš„çŸ¥è¯†ç¬”è®°æŠ¥å‘Š
        """
        return analysis.to_markdown(
            image_paths=[image_relative_path] if image_relative_path else None,
            self_check_mode=self_check_mode,
        )

    def rewrite_visual_schema(
        self,
        original_structure: str,
        feedback: str,
    ) -> str:
        """
        æ ¹æ®åé¦ˆæ”¹å†™ Visual Schema
        """
        rewrite_prompt_template = self.prompts.get("gemini_rewrite", {}).get(
            "prompt", ""
        )

        full_prompt = render_prompt(
            rewrite_prompt_template,
            original_structure=original_structure,
            feedback=feedback,
        )

        return self._call_gemini_text_api(
            system_role="",
            user_prompt=full_prompt,
        )
