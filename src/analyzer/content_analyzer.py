"""
å†…å®¹åˆ†æä¸æ–‡æ¡£ç”Ÿæˆæ¨¡å—

ä½¿ç”¨ Gemini 2.5 Flash å¯¹è§†é¢‘è¿›è¡Œå¤šæ¨¡æ€åˆ†æï¼Œç”Ÿæˆä¸­æ–‡ç²¾è‹±çŸ¥è¯†ç¬”è®°ã€‚
é€šè¿‡ä»£ç†å·æ± æœåŠ¡çš„ /sdk/allocate-key API è·å–çœŸå® API Keyï¼Œ
ç›´æ¥ä½¿ç”¨ google-genai SDK è°ƒç”¨ Gemini APIã€‚
"""

from __future__ import annotations

import re
import json
import logging
import subprocess
import tempfile
import time
from pathlib import Path
from typing import Any

from google import genai
from google.genai import types
import requests

from utils.counter import APICounter, APILimitExceeded
from utils.gemini_throttle import GeminiThrottle
from analyzer.models import AnalysisResult
from analyzer.prompt_loader import load_prompts, render_prompt


class ContentAnalyzer:
    """
    è§†é¢‘å†…å®¹åˆ†æå™¨

    è´Ÿè´£ä½¿ç”¨ Gemini 2.5 Flash åˆ†æè§†é¢‘å†…å®¹ï¼Œç”Ÿæˆç²¾è‹±çŸ¥è¯†ç¬”è®°ã€æœ¯è¯­è¡¨å’ŒçŸ¥è¯†è“å›¾ç»“æ„ã€‚
    é€šè¿‡ä»£ç†å·æ± æœåŠ¡åˆ†é…çœŸå® API Keyï¼ŒSDK ç›´è¿ Google APIã€‚
    """

    def __init__(
        self,
        config: dict[str, Any],
        api_counter: APICounter,
        logger: logging.Logger,
        api_key: str | None = None,
        throttle: GeminiThrottle | None = None,
    ):
        """
        åˆå§‹åŒ–å†…å®¹åˆ†æå™¨

        Args:
            config: ç³»ç»Ÿé…ç½®å­—å…¸
            api_counter: API è°ƒç”¨è®¡æ•°å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            api_key: Gemini API å¯†é’¥ï¼ˆå¯é€‰ï¼Œè‹¥æä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼Œä¸èµ°ä»£ç†å·æ± ï¼‰
        """
        self.config = config
        self.api_counter = api_counter
        self.logger = logger

        # åŠ è½½åˆ†æå™¨é…ç½®
        self.analyzer_config = config.get("analyzer", {})
        self.model_name = self.analyzer_config.get("model", "gemini-2.5-flash")
        self.temperature = self.analyzer_config.get("temperature", 0.7)
        self.max_output_tokens = self.analyzer_config.get("max_output_tokens", 65536)
        self.retry_times = self.analyzer_config.get("retry_times", 10)
        self.timeout = self.analyzer_config.get("timeout", 120)

        # é™æµå™¨
        min_interval = self.analyzer_config.get("min_call_interval", 4.0)
        max_retry_wait = self.analyzer_config.get("max_retry_wait", 600.0)
        self.throttle = throttle or GeminiThrottle(
            min_interval=min_interval,
            max_retries=self.retry_times,
            max_total_wait=max_retry_wait,
            logger=logger,
        )

        proxy_config = config.get("proxy", {})
        self.proxy_base_url = proxy_config.get("base_url", "http://localhost:8000")
        self.proxy_timeout = proxy_config.get("timeout", 10)

        self._fixed_api_key = api_key
        self._allocated_key_id = None  # å¦‚æœæ˜¯ä»å¤–éƒ¨ä¼ å…¥ä¸”å·²çŸ¥ ID,å¯ä»¥æ‰©å±•æ¥å£ä¼ è¾“ ID
        self._allocated_api_key = api_key
        self._client: genai.Client | None = None

        http_proxy = proxy_config.get("http")
        
        if http_proxy:
            import os
            os.environ["HTTP_PROXY"] = http_proxy
            os.environ["HTTPS_PROXY"] = http_proxy
            # ç¡®ä¿æœ¬åœ°æœåŠ¡ä¸èµ°ä»£ç†
            os.environ["NO_PROXY"] = "localhost,127.0.0.1"
            self.logger.info(f"å·²è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡: {http_proxy}")

        if self._allocated_api_key:
            self._client = genai.Client(
                api_key=self._allocated_api_key,
                http_options={"timeout": 600_000},
            )
            self.logger.info("Gemini SDK é…ç½®å®Œæˆ(ä½¿ç”¨å¤–éƒ¨åˆ†é…çš„ API Key)")
        else:
            self.logger.warning("æœªæä¾› Gemini API Key,ContentAnalyzer å°†æ— æ³•æ­£å¸¸å·¥ä½œ")

        # åŠ è½½ Prompt æ¨¡æ¿
        self.prompts = load_prompts()
        self.logger.info("ContentAnalyzer åˆå§‹åŒ–å®Œæˆ")

    # _allocate_key_from_pool å·²ç§»é™¤,å¯†é’¥åˆ†é…é€»è¾‘å·²ç§»è‡³ VideoPipeline

    def _report_usage_to_pool(self) -> None:
        """å‘ä»£ç†å·æ± æŠ¥å‘Šä¸€æ¬¡æˆåŠŸçš„ API è°ƒç”¨ã€‚"""
        if not self._allocated_key_id:
            return
        url = f"{self.proxy_base_url.rstrip('/')}/sdk/report-usage"
        try:
            requests.post(
                url,
                json={"key_id": self._allocated_key_id},
                timeout=self.proxy_timeout,
            )
        except requests.RequestException as e:
            self.logger.warning(f"å‘å·æ± æŠ¥å‘Šç”¨é‡å¤±è´¥: {e}")

    def _report_error_to_pool(self, is_rpd_limit: bool = False) -> None:
        """å‘ä»£ç†å·æ± æŠ¥å‘Š API è°ƒç”¨é”™è¯¯ã€‚"""
        if not self._allocated_key_id:
            return
        url = f"{self.proxy_base_url.rstrip('/')}/sdk/report-error"
        try:
            requests.post(
                url,
                json={
                    "key_id": self._allocated_key_id,
                    "is_rpd_limit": is_rpd_limit,
                },
                timeout=self.proxy_timeout,
            )
        except requests.RequestException as e:
            self.logger.warning(f"å‘å·æ± æŠ¥å‘Šé”™è¯¯å¤±è´¥: {e}")

    def _delete_remote_file(self, file_name: str) -> None:
        """åˆ é™¤ Gemini Files å­˜å‚¨ä¸­çš„è¿œç¨‹æ–‡ä»¶ï¼Œé‡Šæ”¾é…é¢ç©ºé—´ã€‚"""
        if not self._client:
            return
        try:
            self.throttle.wait_for_files_op()
            self._client.files.delete(name=file_name)
            self.logger.info(f"å·²æ¸…ç† Gemini è¿œç¨‹æ–‡ä»¶: {file_name}")
        except Exception as e:
            self.logger.warning(f"æ¸…ç† Gemini è¿œç¨‹æ–‡ä»¶å¤±è´¥: {e}")

    def _compress_video_for_upload(self, video_path: Path) -> Path:
        """ç”¨ ffmpeg å‹ç¼©è§†é¢‘ä»¥å‡å°ä¸Šä¼ ä½“ç§¯ï¼Œè¿”å›å‹ç¼©åçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„ã€‚

        å‹ç¼©ç­–ç•¥: 360p + CRF 28ï¼Œå¯¹ Gemini å†…å®¹åˆ†æè¶³å¤Ÿï¼Œä½“ç§¯å¯é™è‡³åŸæ¥çš„ 1/5~1/10ã€‚
        å¦‚æœ ffmpeg ä¸å¯ç”¨æˆ–å‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€‚
        """
        max_size_mb = 30
        file_size_mb = video_path.stat().st_size / (1024 * 1024)
        if file_size_mb <= max_size_mb:
            self.logger.info(
                f"è§†é¢‘ä½“ç§¯ {file_size_mb:.1f}MB <= {max_size_mb}MBï¼Œè·³è¿‡å‹ç¼©"
            )
            return video_path

        self.logger.info(
            f"è§†é¢‘ä½“ç§¯ {file_size_mb:.1f}MB > {max_size_mb}MBï¼Œå¼€å§‹ ffmpeg å‹ç¼©..."
        )

        compressed_path = video_path.parent / f"compressed_{video_path.name}"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‹ç¼©æ–‡ä»¶
        if compressed_path.exists() and compressed_path.stat().st_size > 0:
            self.logger.info(f"å‘ç°å·²å­˜åœ¨çš„å‹ç¼©æ–‡ä»¶: {compressed_path.name}ï¼Œè·³è¿‡å‹ç¼©")
            return compressed_path

        try:
            cmd = [
                "ffmpeg",
                "-y",
                "-i",
                str(video_path),
                "-vf",
                "scale=-2:360",
                "-c:v",
                "libx264",
                "-crf",
                "28",
                "-preset",
                "fast",
                "-c:a",
                "aac",
                "-b:a",
                "64k",
                str(compressed_path),
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,
            )
            if result.returncode != 0:
                self.logger.warning(f"ffmpeg å‹ç¼©å¤±è´¥: {(result.stderr or '')[:200]}")
                return video_path

            new_size_mb = compressed_path.stat().st_size / (1024 * 1024)
            self.logger.info(
                f"å‹ç¼©å®Œæˆ: {file_size_mb:.1f}MB -> {new_size_mb:.1f}MB "
                f"(å‹ç¼©ç‡ {new_size_mb / file_size_mb * 100:.0f}%)"
            )
            return compressed_path

        except FileNotFoundError:
            self.logger.warning("ffmpeg æœªå®‰è£…ï¼Œè·³è¿‡å‹ç¼©")
            return video_path
        except subprocess.TimeoutExpired:
            self.logger.warning("ffmpeg å‹ç¼©è¶…æ—¶(5åˆ†é’Ÿ)ï¼Œè·³è¿‡å‹ç¼©")
            if compressed_path.exists():
                compressed_path.unlink()
            return video_path

    def _upload_video(self, video_path: Path) -> Any:
        """
        å¤„ç†è§†é¢‘å‹ç¼©å’Œä¸Šä¼ 
        
        Returns:
             Gemini File å¯¹è±¡
        """
        upload_path = self._compress_video_for_upload(video_path)
        video_file = None
        
        try:
            self.logger.info(f"ä¸Šä¼ è§†é¢‘æ–‡ä»¶: {upload_path.name}")
            self.throttle.wait_for_files_op()
            video_file = self._client.files.upload(file=str(upload_path))

            while video_file.state.name == "PROCESSING":
                self.logger.info("ç­‰å¾…è§†é¢‘å¤„ç†...")
                self.throttle.wait_for_files_op()
                video_file = self._client.files.get(name=video_file.name)

            if video_file.state.name == "FAILED":
                raise RuntimeError(f"è§†é¢‘æ–‡ä»¶å¤„ç†å¤±è´¥: {video_file.state.name}")

            self.logger.info(f"è§†é¢‘æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {video_file.name}")
            return video_file

        except Exception as e:
            self.logger.error(f"è§†é¢‘ä¸Šä¼ å¤±è´¥: {e}")
            # å¦‚æœä¸Šä¼ å¤±è´¥ï¼Œæ¸…ç†è¿œç¨‹æ–‡ä»¶ï¼ˆå¦‚æœå·²åˆ›å»ºï¼‰
            if video_file:
                self._delete_remote_file(video_file.name)
            raise

    def analyze_video(self, video_path: str | Path) -> AnalysisResult:
        """
        åˆ†æè§†é¢‘å†…å®¹ï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

        self.logger.info(f"å¼€å§‹åˆ†æè§†é¢‘: {video_path.name}")

        # æ£€æŸ¥ API è°ƒç”¨æ¬¡æ•° (é¢„ç•™ 2 æ¬¡: 1æ¬¡å†…å®¹åˆ†æ, 1æ¬¡ Schema ç”Ÿæˆ)
        if self.api_counter.current_count + 2 > self.api_counter.max_calls:
             raise APILimitExceeded(
                f"API è°ƒç”¨æ¬¡æ•°ä¸è¶³ä»¥å®Œæˆå…¨æµç¨‹: {self.api_counter.current_count}/{self.api_counter.max_calls}"
            )

        # Step 0: å‡†å¤‡è§†é¢‘æ–‡ä»¶ (å‹ç¼© + ä¸Šä¼ )
        # æ³¨æ„: è¿™é‡Œä¸è¿›è¡Œé‡è¯•ï¼Œå¦‚æœä¸Šä¼ å¤±è´¥é€šå¸¸æ˜¯ç½‘ç»œæˆ–æ–‡ä»¶é—®é¢˜ï¼Œé‡è¯•æ„ä¹‰ä¸å¤§æˆ–ç”±å¤–éƒ¨æ§åˆ¶
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")

        video_file = self._upload_video(video_path)
        
        try:
            # Step 1: è§†é¢‘å†…å®¹åˆ†æ (é€šè¿‡é™æµå™¨è‡ªåŠ¨é‡è¯• 429)
            self.logger.info("Step 1: æ‰§è¡Œè§†é¢‘å†…å®¹åˆ†æ...")
            system_role = self.prompts.get("gemini_analysis", {}).get("system_role", "")
            main_prompt = self.prompts.get("gemini_analysis", {}).get("main_prompt", "")

            def _on_retry(attempt: int, exc: Exception) -> None:
                """429 é‡è¯•å›è°ƒ: å‘å·æ± æŠ¥å‘Šé”™è¯¯"""
                self._report_error_to_pool(is_rpd_limit=True)

            # å†…å±‚é‡è¯•: JSON è§£æå¤±è´¥æ—¶é‡æ–°è¯·æ±‚ API (æœ€å¤š 2 æ¬¡é¢å¤–å°è¯•)
            json_parse_max_retries = 2
            response_data = None
            for json_attempt in range(1, json_parse_max_retries + 2):  # 1 æ¬¡æ­£å¸¸ + 2 æ¬¡é‡è¯•
                try:
                    response_data = self.throttle.call_with_retry(
                        self._generate_content,
                        video_file,
                        system_role,
                        main_prompt,
                        on_retry_callback=_on_retry,
                    )
                    break  # æˆåŠŸåˆ™è·³å‡º
                except ValueError as ve:
                    if json_attempt <= json_parse_max_retries:
                        self.logger.warning(
                            f"âš ï¸ JSON è§£æå¤±è´¥ (ç¬¬ {json_attempt} æ¬¡)ï¼Œé‡æ–°è¯·æ±‚ API: {ve}"
                        )
                    else:
                        raise  # æœ€åä¸€æ¬¡ä»å¤±è´¥åˆ™æŠ›å‡º

            # å¢åŠ  API è®¡æ•°
            self.api_counter.increment("Gemini")
            
            # Step 2: ç”Ÿæˆ Visual Schema
            # å¦‚æœ Step 1 å·²ç»ç”Ÿæˆäº† Visual Schemaï¼Œåˆ™è·³è¿‡ Step 2
            # å¦åˆ™å°è¯•å•ç‹¬ç”Ÿæˆ (Fallback)
        
            visual_schema = response_data.get("visual_schema", "")
            
            if visual_schema and "---BEGIN PROMPT---" in visual_schema:
                self.logger.info("Visual Schema å·²åœ¨ Step 1 ä¸­ç”Ÿæˆï¼Œè·³è¿‡ç‹¬ç«‹ç”Ÿæˆæ­¥éª¤")
            else:
                self.logger.info("Step 1 æœªç”Ÿæˆæœ‰æ•ˆ Visual Schemaï¼Œå°è¯•ç‹¬ç«‹ç”Ÿæˆ (Step 2)...")
                # æ£€æŸ¥ API è°ƒç”¨æ¬¡æ•° (å¦‚æœæ²¡æœ‰é…é¢ï¼Œåˆ™æ”¾å¼ƒç”Ÿæˆ)
                if self.api_counter.current_count + 1 <= self.api_counter.max_calls:
                     try:
                        self.logger.info("Step 2: ç”ŸæˆçŸ¥è¯†è“å›¾ Visual Schema...")
                        deep_dive_content = json.dumps(
                            response_data.get("deep_dive", []), ensure_ascii=False, indent=2
                        )
                        visual_schema = self._generate_visual_schema(deep_dive_content)
                        self.logger.info("Visual Schema ç”ŸæˆæˆåŠŸ")
                        # æ›´æ–° response_dataï¼Œä»¥ä¾¿åç»­ AnalysisResult.from_api_response ä½¿ç”¨
                        response_data["visual_schema"] = visual_schema
                     except Exception as e:
                        self.logger.error(f"Visual Schema ç”Ÿæˆå¤±è´¥: {e}ï¼Œå°†ç”Ÿæˆä¸å¸¦å›¾çš„æŠ¥å‘Š")
                        # visual_schema ä¿æŒä¸ºç©º
                else:
                     self.logger.warning("API é…é¢ä¸è¶³ä»¥æ‰§è¡Œ Step 2ï¼Œè·³è¿‡ Visual Schema ç”Ÿæˆ")

            # æ„å»º AnalysisResult å¯¹è±¡
            metadata = {
                "video_name": video_path.name,
                "video_size": video_path.stat().st_size,
            }

            try:
                result = AnalysisResult.from_api_response(
                    video_path=video_path,
                    response_data=response_data,
                    metadata=metadata,
                )
                # å¡«å…… Visual Schema
                result.knowledge_doc.visual_schema = visual_schema
                
                self.logger.info(f"è§†é¢‘åˆ†æå…¨æµç¨‹å®Œæˆ: {result.title}")
                return result
            except ValueError as e:
                self.logger.error(f"API å“åº”æ ¼å¼é”™è¯¯: {e}")
                raise

        finally:
            # æ¸…ç†èµ„æº
            if video_file:
                self._delete_remote_file(video_file.name)
            
            # æ¸…ç†å‹ç¼©æ–‡ä»¶ (å¯é€‰: å¦‚æœå¸Œæœ›ä¸‹æ¬¡è¿è¡Œå¤ç”¨ï¼Œå¯ä»¥æ³¨é‡Šæ‰è¿™è¡Œï¼Œæˆ–è€…ä¿ç•™ä»¥èŠ‚çœç©ºé—´)
            # æ ¹æ®ç”¨æˆ·éœ€æ±‚ "æ–­ç‚¹å¤„ç»§ç»­"ï¼Œæˆ‘ä»¬åº”è¯¥ä¿ç•™å‹ç¼©æ–‡ä»¶ï¼Œæˆ–è€…åœ¨æˆåŠŸåæ‰æ¸…ç†ï¼Ÿ
            # ç°åœ¨çš„é€»è¾‘æ˜¯ `_compress_video_for_upload` ä¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚
            # å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œåˆ é™¤äº†ï¼Œé‚£ä¹ˆä¸‹æ¬¡åˆè¦é‡æ–°å‹ç¼©ã€‚
            # ä¸ºäº†æ”¯æŒ"æ–­ç‚¹å‹ç¼©"ï¼Œæˆ‘ä»¬åº”è¯¥ä»…åœ¨å®Œå…¨æˆåŠŸååˆ é™¤ï¼Œæˆ–è€…ä¿ç•™åœ¨ temp ç›®å½•ç”±ç³»ç»Ÿæ¸…ç†ï¼Ÿ
            # ä¹‹å‰çš„é€»è¾‘æ˜¯ `finally` ä¸­åˆ é™¤ã€‚
            # ç”¨æˆ·æŠ±æ€¨çš„æ˜¯ "å¤±è´¥å...é‡æ–°ä¸‹è½½å¹¶å‹ç¼©"ã€‚
            # å¦‚æœæˆ‘ä»¬åˆ é™¤äº†ï¼Œä¸‹æ¬¡ç¡®å®è¦é‡æ–°å‹ç¼©ã€‚
            # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬åº”è¯¥ç§»é™¤åˆ é™¤å‹ç¼©æ–‡ä»¶çš„é€»è¾‘ï¼Œæˆ–è€…åªåœ¨æˆåŠŸæ—¶åˆ é™¤ï¼Ÿ
            # å®é™…ä¸Šï¼Œ`VideoDownloader` æœ‰ä¸ª temp ç›®å½•ï¼Œå¯èƒ½æ›´é€‚åˆæ”¾é‚£é‡Œã€‚
            # è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯å…ˆä¿ç•™åˆ é™¤é€»è¾‘ï¼Œä½†æ˜¯å› ä¸º `_compress_video_for_upload` ç°åœ¨æ£€æŸ¥äº†å­˜åœ¨æ€§ï¼Œ
            # åªè¦åœ¨"æœ¬æ¬¡å…ƒæ“ä½œ"ä¸­ä¸åˆ é™¤ï¼Œä¸‹æ¬¡é‡è¯•ï¼ˆåŒä¸€è¿›ç¨‹å†…ï¼‰è¿˜æ˜¯ä¼šé€šè¿‡ path ä¼ é€’ã€‚
            # ä½†å¦‚æœæ˜¯è¿›ç¨‹å´©æºƒé‡å¯ï¼Œ`upload_path` æ˜¯ä¸´æ—¶ç”Ÿæˆçš„å—ï¼Ÿ
            # `video_path.parent / f"compressed_{video_path.name}"` æ˜¯åœ¨åŸç›®å½•ä¸‹ã€‚
            # å¦‚æœæˆ‘åˆ é™¤äº†ï¼Œä¸‹æ¬¡è¿›ç¨‹å¯åŠ¨è¿˜æ˜¯è¦å‹ç¼©ã€‚
            # é‰´äºç”¨æˆ·çš„éœ€æ±‚ï¼Œæˆ‘å°†æ³¨é‡Šæ‰åˆ é™¤å‹ç¼©æ–‡ä»¶çš„ä»£ç ï¼Œè®©å®ƒä¿ç•™ï¼Œ
            # æˆ–è€…å°†å…¶ç§»åŠ¨åˆ° VideoPipeline çš„æ¸…ç†é˜¶æ®µï¼Ÿ
            # ç®€å•èµ·è§ï¼Œæˆ‘å…ˆä¸åˆ é™¤å‹ç¼©æ–‡ä»¶ï¼Œè®©å®ƒå˜æˆ"ç¼“å­˜"ã€‚
            # ä¸ºäº†é¿å…åƒåœ¾å †ç§¯ï¼Œå¯ä»¥åœ¨ AnalysisResult æˆåŠŸè¿”å›å‰åˆ é™¤ï¼Ÿ
            # è¿˜æ˜¯è¯´ï¼Œç”¨æˆ·å¸Œæœ›çš„æ˜¯"å¤±è´¥é‡è¯•"æ—¶ä¸é‡æ–°å‹ç¼©ã€‚
            # å½“å‰çš„ `finally` å—æ˜¯åœ¨ `analyze_video` ç»“æŸæ—¶æ‰§è¡Œã€‚
            # å¦‚æœ `analyze_video` å¤±è´¥æŠ›å‡ºå¼‚å¸¸ï¼Œ`finally` æ‰§è¡Œï¼Œæ–‡ä»¶è¢«åˆ ã€‚
            # ä¸‹æ¬¡è°ƒç”¨ `analyze_video` (æ¯”å¦‚å¤–éƒ¨é‡è¯•)ï¼Œæ–‡ä»¶æ²¡äº†ï¼Œåˆè¦å‹ç¼©ã€‚
            # æ‰€ä»¥å¿…é¡» **ä¸åˆ é™¤** å‹ç¼©æ–‡ä»¶ï¼Œæˆ–è€…åªåœ¨ **æˆåŠŸ** ååˆ é™¤ã€‚
            
            compressed_file = video_path.parent / f"compressed_{video_path.name}"
            if compressed_file.exists():
                 # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆç»“æœåæ‰åˆ é™¤ï¼Ÿæˆ–è€…å¹²è„†ä¸åˆ ï¼Œç•™ç»™ç”¨æˆ·æ‰‹åŠ¨æ¸…ç†/å®šæœŸæ¸…ç†ï¼Ÿ
                 # ä¸ºäº†é˜²æ­¢ç£ç›˜çˆ†æ»¡ï¼Œæˆ‘ä»¬è¿˜æ˜¯å°è¯•åˆ é™¤ï¼Œä½†æ˜¯å‰ææ˜¯å¿…é¡»åŒºåˆ†"å®Œå…¨å¤±è´¥"å’Œ"æˆåŠŸ"ã€‚
                 # ç”±äº `finally` ä¸åŒºåˆ†ï¼Œæˆ‘ä»¬å¾ˆéš¾åšã€‚
                 # æœ€å¥½çš„æ–¹å¼æ˜¯ï¼šä¸åœ¨è¿™é‡Œåˆ é™¤ã€‚è®©ä¸Šå±‚ `VideoPipeline` æˆ–è€… `cleanup` æ–¹æ³•æ¥å¤„ç†ã€‚
                 # æˆ–è€…ï¼Œä»…ä»…è®°å½•æ—¥å¿—è¯´"ä¿ç•™å‹ç¼©æ–‡ä»¶ä»¥ä¾¿é‡è¯•"ã€‚
                 self.logger.info(f"ä¿ç•™å‹ç¼©æ–‡ä»¶ä»¥å¤‡é‡è¯•: {compressed_file.name}")
                 pass 

    def _generate_visual_schema(self, deep_dive_content: str) -> str:
        """æ ¹æ®æ·±åº¦è§£æå†…å®¹ç”Ÿæˆ Visual Schema"""
        
        schema_config = self.prompts.get("gemini_visual_schema", {})
        system_role = schema_config.get("system_role", "")
        main_prompt_template = schema_config.get("main_prompt", "")
        
        full_prompt = render_prompt(
            main_prompt_template, deep_dive_content=deep_dive_content
        )
        
        # ä½¿ç”¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæ¥å£
        response_text = self._call_gemini_text_api(
            system_role=system_role,
            user_prompt=full_prompt,
            temperature=0.7
        )
        
        # æå– Markdown ä»£ç å—
        if "```markdown" in response_text:
            start = response_text.find("```markdown") + 11
            end = response_text.find("```", start)
            return response_text[start:end].strip()
        elif "```" in response_text:
            start = response_text.find("```") + 3
            end = response_text.find("```", start)
            return response_text[start:end].strip()
            
        return response_text.strip()

    def _generate_content(
        self,
        video_file: Any,
        system_role: str,
        main_prompt: str,
    ) -> dict[str, Any]:
        """
        ä½¿ç”¨å·²ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶ç”Ÿæˆå†…å®¹
        """
        # full_prompt = f"{system_role}\n\n{main_prompt}" # REMOVED: using system_instruction instead

        gen_config = types.GenerateContentConfig(
            temperature=self.temperature,
            max_output_tokens=self.max_output_tokens,
            response_mime_type="application/json",
            system_instruction=system_role,  # ADDED: explicitly pass system instruction
            thinking_config=types.ThinkingConfig(
                thinking_budget=8192,
            ),
        )

        contents = [
            {
                "role": "user",
                "parts": [
                    {
                        "file_data": {
                            "file_uri": video_file.uri,
                            "mime_type": video_file.mime_type,
                        }
                    },
                    {"text": main_prompt}, # CHANGED: pass only main_prompt here
                ],
            }
        ]

        # æµå¼è°ƒç”¨ï¼Œå®æ—¶è¾“å‡º Gemini æ€è€ƒå’Œç”Ÿæˆè¿‡ç¨‹
        self.logger.info("å¼€å§‹æµå¼æ¥æ”¶ Gemini å“åº”...")
        response_text_parts: list[str] = []
        thinking_logged = False

        for chunk in self._client.models.generate_content_stream(
            model=self.model_name,
            contents=contents,
            config=gen_config,
        ):
            if not chunk.candidates:
                continue
            
            candidate = chunk.candidates[0]
            
            # æ£€æŸ¥ finish_reason
            if candidate.finish_reason:
                if candidate.finish_reason.name != "STOP":
                    self.logger.warning(
                        f"Gemini ç”Ÿæˆç»“æŸåŸå› é STOP: {candidate.finish_reason.name} "
                        f"(å¯èƒ½å‘ç”Ÿæˆªæ–­)"
                    )
                else:
                    self.logger.info("Gemini ç”Ÿæˆæ­£å¸¸ç»“æŸ (STOP)")

            # æ£€æŸ¥ content æ˜¯å¦å­˜åœ¨
            if not candidate.content or not candidate.content.parts:
                continue

            for part in candidate.content.parts:
                if part.thought:
                    if not thinking_logged:
                        self.logger.info("ğŸ’­ Gemini æ€è€ƒä¸­...")
                        thinking_logged = True
                    # å®æ—¶è¾“å‡ºæ€è€ƒç‰‡æ®µï¼ˆæˆªå–å‰ 200 å­—ç¬¦é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                    snippet = (part.text or "")[:200] if part.text else ""
                    if snippet:
                        self.logger.info(f"  ğŸ’­ {snippet}")
                else:
                    if part.text:
                        response_text_parts.append(part.text)
                        # å®æ—¶è¾“å‡ºç”Ÿæˆè¿›åº¦
                        snippet = (part.text or "")[:100].replace("\n", " ")
                        self.logger.info(f"  ğŸ“ ç”Ÿæˆä¸­: {snippet}...")

        response_text = "".join(response_text_parts)
        self.logger.info(f"æµå¼æ¥æ”¶å®Œæˆï¼Œå“åº”æ€»é•¿åº¦: {len(response_text)} å­—ç¬¦")

        self._report_usage_to_pool()
        
        response_text = response_text.strip()

        self.logger.debug(f"API å“åº”: {response_text[:200]}...")

        # è§£æ JSON å“åº”
        # 1. ä¼˜å…ˆå°è¯•æ ‡å‡†çš„ ```json ... ``` å—
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", response_text, re.DOTALL)
        if json_match:
             response_text = json_match.group(1)
        else:
             # 2. å°è¯•ä¸å¸¦ json æ ‡ç­¾çš„ ``` ... ``` å—ï¼Œä½†æ’é™¤é JSON çš„ä»£ç å—
             code_block_match = re.search(r"```\s*(\{.*?\})\s*```", response_text, re.DOTALL)
             if code_block_match:
                  response_text = code_block_match.group(1)
             else:
                  # 3. å…œåº•ï¼šå¯»æ‰¾æœ€å¤–å±‚çš„ {} 
                  start_idx = response_text.find("{")
                  end_idx = response_text.rfind("}")
                  if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                       response_text = response_text[start_idx : end_idx + 1]

        # å¤šè½® JSON ä¿®å¤
        repaired = self._try_repair_json(response_text)
        if repaired is not None:
            self.logger.info("API å“åº”è§£ææˆåŠŸ")
            response_data = repaired
        else:
            # ä¿®å¤å¤±è´¥ï¼šä¿å­˜åŸå§‹å“åº”åˆ°ç£ç›˜ä»¥ä¾¿äº‹åè°ƒè¯•
            try:
                dump_path = Path("data/output/logs") / f"failed_json_{int(time.time())}.txt"
                dump_path.parent.mkdir(parents=True, exist_ok=True)
                dump_path.write_text(response_text, encoding="utf-8")
                self.logger.error(f"JSON ä¿®å¤å¤±è´¥ï¼Œå·²ä¿å­˜åŸå§‹å“åº”åˆ°: {dump_path}")
            except Exception:
                self.logger.error("JSON ä¿®å¤å¤±è´¥ï¼Œä¸”æ— æ³•ä¿å­˜åŸå§‹å“åº”åˆ°ç£ç›˜")
            raise ValueError(f"API å“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œå¤šè½®ä¿®å¤å‡å¤±è´¥")

        expected_fields = {
            "title",
            "one_sentence_summary",
            "key_takeaways",
            "deep_dive",
            "glossary",
            "glossary",
            "visual_schema",
        }
        missing = expected_fields - response_data.keys()
        if missing:
            self.logger.warning(
                f"API å“åº” JSON ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {', '.join(sorted(missing))}ï¼Œè§¦å‘é‡è¯•"
            )
            # åªæœ‰å…³é”®å­—æ®µç¼ºå¤±æ‰æŠ›å‡ºé”™è¯¯ï¼Œå°è¯•å°½å¯èƒ½åˆ©ç”¨ç°æœ‰æ•°æ®
            # è¿™é‡Œä¿æŒåŸé€»è¾‘æŠ›å‡ºé”™è¯¯ï¼Œå› ä¸ºç¼ºå¤±å…³é”®å­—æ®µå¯èƒ½å¯¼è‡´åç»­æµç¨‹å¤±è´¥
    
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ mind_map_structure ç¼ºå¤±ä½† deep_dive å­˜åœ¨ï¼Œå¯ä»¥å°è¯•å®¹é”™æˆ–è€…é‡æ–°ç”Ÿæˆ
            # ç›®å‰ç­–ç•¥ï¼šé‡è¯•
            raise ValueError(
                f"API å“åº” JSON ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {', '.join(sorted(missing))}"
            )

        return response_data

    def _call_gemini_text_api(
        self,
        system_role: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_output_tokens: int = 8192
    ) -> str:
        """è°ƒç”¨ Gemini çº¯æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼ˆé€šè¿‡é™æµå™¨è‡ªåŠ¨å¤„ç† 429ï¼‰"""
        if not self._client:
            raise RuntimeError("Gemini Client æœªåˆå§‹åŒ– (ç¼ºå°‘ API Key)")

        def _do_text_call() -> str:
            """å•æ¬¡æ–‡æœ¬ API è°ƒç”¨"""
            # æ£€æŸ¥ client æ˜¯å¦å­˜åœ¨ (å¯èƒ½åœ¨è½®æ¢ä¸­è¢«ç½®ç©ºæˆ–é‡å»º)
            if not self._client:
                if self._allocated_api_key:
                    self._client = genai.Client(api_key=self._allocated_api_key, http_options={"timeout": 600_000})
                else:
                    raise RuntimeError("Gemini Client æœªåˆå§‹åŒ–ä¸”æ— å¯ç”¨ Key")

            # full_prompt = f"{system_role}\n\n{user_prompt}" if system_role else user_prompt # REMOVED

            gen_config = types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_output_tokens,
                system_instruction=system_role if system_role else None, # ADDED
                thinking_config=types.ThinkingConfig(
                    thinking_budget=4096,
                ),
            )

            response_parts: list[str] = []
            thinking_logged = False

            self.logger.info("å¼€å§‹æµå¼æ¥æ”¶æ–‡æœ¬ç”Ÿæˆå“åº”...")

            for chunk in self._client.models.generate_content_stream(
                model=self.model_name,
                contents=[{"role": "user", "parts": [{"text": user_prompt}]}], # CHANGED: use user_prompt directly
                config=gen_config,
            ):
                if not chunk.candidates:
                    continue
                
                candidate = chunk.candidates[0]
                
                # æ£€æŸ¥ content æ˜¯å¦å­˜åœ¨
                if not candidate.content or not candidate.content.parts:
                    continue
                    
                for part in candidate.content.parts:
                    if part.thought:
                        if not thinking_logged:
                            self.logger.info("ğŸ’­ Gemini æ€è€ƒä¸­...")
                            thinking_logged = True
                        snippet = (part.text or "")[:100] if part.text else ""
                        if snippet:
                            self.logger.info(f"  ğŸ’­ {snippet}")
                    else:
                        if part.text:
                            response_parts.append(part.text)

            self._report_usage_to_pool()
            self.api_counter.increment("Gemini")

            return "".join(response_parts).strip()

        def _on_retry(attempt: int, exc: Exception) -> None:
            self._report_error_to_pool(is_rpd_limit=True)

        return self.throttle.call_with_retry(
            _do_text_call,
            on_retry_callback=_on_retry,
        )

    @staticmethod
    def _sanitize_json_escapes(text: str) -> str:
        """ä¿®å¤ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„éæ³•è½¬ä¹‰åºåˆ—ã€‚

        å¸¸è§åœºæ™¯: Gemini è¿”å›çš„ JSON ä¸­åŒ…å« LaTeX å…¬å¼ (\frac, \sum, \ln ç­‰),
        è¿™äº›åœ¨ JSON è§„èŒƒä¸­æ˜¯éæ³•çš„è½¬ä¹‰åºåˆ—ã€‚

        ç­–ç•¥: ä»…åœ¨å­—ç¬¦ä¸²å€¼å†…éƒ¨, å°† `\X` (X é JSON åˆæ³•è½¬ä¹‰å­—ç¬¦) æ›¿æ¢ä¸º `\\X`ã€‚
        JSON åˆæ³•è½¬ä¹‰å­—ç¬¦: " \\ / b f n r t u
        """
        legal_escapes = set('"\\/ b f n r t u'.split() + ['"', '\\', '/'])
        # æ›´ç²¾ç¡®: JSON å…è®¸ \" \\\\ \/ \b \f \n \r \t \uXXXX
        legal_escape_chars = set('"\\/ bfnrtu')

        result: list[str] = []
        i = 0
        in_string = False
        length = len(text)

        while i < length:
            ch = text[i]

            if not in_string:
                result.append(ch)
                if ch == '"':
                    in_string = True
                i += 1
            else:
                # åœ¨å­—ç¬¦ä¸²å†…éƒ¨
                if ch == '\\':
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦
                    if i + 1 < length:
                        next_ch = text[i + 1]
                        if next_ch in legal_escape_chars:
                            # åˆæ³•è½¬ä¹‰, åŸæ ·ä¿ç•™
                            result.append(ch)
                            result.append(next_ch)
                            i += 2
                        else:
                            # éæ³•è½¬ä¹‰ (å¦‚ \frac, \sum): æ›¿æ¢ä¸º \\\\
                            result.append('\\\\')
                            # ä¸è·³è¿‡ next_ch, è®©å®ƒä½œä¸ºæ™®é€šå­—ç¬¦å¤„ç†
                            i += 1
                    else:
                        # åæ–œæ åœ¨æœ«å°¾, è½¬ä¹‰å®ƒ
                        result.append('\\\\')
                        i += 1
                elif ch == '"':
                    result.append(ch)
                    in_string = False
                    i += 1
                else:
                    result.append(ch)
                    i += 1

        return ''.join(result)

    @staticmethod
    def _close_truncated_json(text: str) -> str:
        """é—­åˆè¢«æˆªæ–­çš„ JSON: æœªé—­åˆçš„å­—ç¬¦ä¸²ã€å°¾éƒ¨é€—å·ã€æœªé—­åˆçš„æ‹¬å·ã€‚"""
        text = text.rstrip()

        # 1. é—­åˆæœªé—­åˆçš„å­—ç¬¦ä¸²
        in_string = False
        escape_next = False
        for ch in text:
            if escape_next:
                escape_next = False
                continue
            if ch == '\\':
                escape_next = True
                continue
            if ch == '"':
                in_string = not in_string

        if in_string:
            text += '"'

        # 2. å»é™¤å°¾éƒ¨é€—å·
        text = text.rstrip().rstrip(',')

        # 3. ç»Ÿè®¡æœªé—­åˆçš„æ‹¬å·å¹¶é€†åºé—­åˆ
        bracket_stack: list[str] = []
        in_str = False
        esc = False
        for ch in text:
            if esc:
                esc = False
                continue
            if ch == '\\' and in_str:
                esc = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch in ('{', '['):
                bracket_stack.append(ch)
            elif ch == '}' and bracket_stack and bracket_stack[-1] == '{':
                bracket_stack.pop()
            elif ch == ']' and bracket_stack and bracket_stack[-1] == '[':
                bracket_stack.pop()

        closing_map = {'[': ']', '{': '}'}
        suffix = ''.join(closing_map[b] for b in reversed(bracket_stack))
        return text + suffix

    @staticmethod
    def _truncate_to_last_complete_item(text: str) -> str | None:
        """æˆªæ–­åˆ°æœ€åä¸€ä¸ªé€—å·å¤„, ç„¶åé‡æ–°é—­åˆæ‹¬å·ã€‚ç”¨äºä¸¢å¼ƒè¢«æˆªæ–­çš„æœ€åä¸€ä¸ªå…ƒç´ ã€‚"""
        last_comma = text.rfind(',')
        if last_comma <= 0:
            return None

        truncated = text[:last_comma]

        # é‡æ–°æ‰«æå¹¶é—­åˆ
        bracket_stack: list[str] = []
        in_str = False
        esc = False
        for ch in truncated:
            if esc:
                esc = False
                continue
            if ch == '\\' and in_str:
                esc = True
                continue
            if ch == '"':
                in_str = not in_str
                continue
            if in_str:
                continue
            if ch in ('{', '['):
                bracket_stack.append(ch)
            elif ch == '}' and bracket_stack and bracket_stack[-1] == '{':
                bracket_stack.pop()
            elif ch == ']' and bracket_stack and bracket_stack[-1] == '[':
                bracket_stack.pop()

        closing_map = {'[': ']', '{': '}'}
        suffix = ''.join(closing_map[b] for b in reversed(bracket_stack))
        return truncated + suffix

    @classmethod
    def _try_repair_json(cls, text: str) -> dict[str, Any] | None:
        """å¤šè½®å°è¯•ä¿®å¤ JSON å“åº”ã€‚

        ä¿®å¤ç­–ç•¥ (é€è½®å°è¯•, æˆåŠŸå³è¿”å›):
          ç¬¬ 0 è½®: ç›´æ¥è§£æ (å¿«é€Ÿè·¯å¾„)
          ç¬¬ 1 è½®: ä¿®å¤éæ³•è½¬ä¹‰åºåˆ— (LaTeX å…¬å¼ç­‰)
          ç¬¬ 2 è½®: ä¿®å¤è½¬ä¹‰ + é—­åˆæˆªæ–­
          ç¬¬ 3 è½®: ä¿®å¤è½¬ä¹‰ + æˆªæ–­åˆ°æœ€åå®Œæ•´é¡¹
          ç¬¬ 4 è½®: ç§»é™¤æ— æ³•ä¿®å¤çš„æ§åˆ¶å­—ç¬¦åé‡è¯•
        """

        # --- ç¬¬ 0 è½®: ç›´æ¥è§£æ ---
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 1 è½®: ä»…ä¿®å¤éæ³•è½¬ä¹‰ ---
        sanitized = cls._sanitize_json_escapes(text)
        try:
            return json.loads(sanitized)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 2 è½®: ä¿®å¤è½¬ä¹‰ + é—­åˆæˆªæ–­ ---
        closed = cls._close_truncated_json(sanitized)
        try:
            return json.loads(closed)
        except json.JSONDecodeError:
            pass

        # --- ç¬¬ 3 è½®: ä¿®å¤è½¬ä¹‰ + æˆªæ–­åˆ°æœ€åå®Œæ•´é¡¹ ---
        truncated = cls._truncate_to_last_complete_item(sanitized)
        if truncated:
            try:
                return json.loads(truncated)
            except json.JSONDecodeError:
                pass

            # é—­åˆæˆªæ–­åçš„ç»“æœ
            closed_truncated = cls._close_truncated_json(truncated)
            try:
                return json.loads(closed_truncated)
            except json.JSONDecodeError:
                pass

        # --- ç¬¬ 4 è½®: æ¸…ç†æ§åˆ¶å­—ç¬¦ ---
        # ç§»é™¤ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„è£¸æ§åˆ¶å­—ç¬¦ (\x00-\x1f é™¤ \t \n \r)
        cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', sanitized)
        closed_cleaned = cls._close_truncated_json(cleaned)
        try:
            return json.loads(closed_cleaned)
        except json.JSONDecodeError:
            pass

        return None

    def generate_report(self, analysis: AnalysisResult, image_relative_path: str | None = None) -> str:
        """
        ç”ŸæˆçŸ¥è¯†ç¬”è®°æŠ¥å‘Šçš„ Markdown æ ¼å¼

        Args:
            analysis: åˆ†æç»“æœå¯¹è±¡
            image_relative_path: çŸ¥è¯†è“å›¾å›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„ï¼ˆç”¨äºåµŒå…¥æŠ¥å‘Šï¼‰

        Returns:
            Markdown æ ¼å¼çš„çŸ¥è¯†ç¬”è®°æŠ¥å‘Š
        """
        return analysis.to_markdown(image_path=image_relative_path)

    def rewrite_visual_schema(
        self,
        original_structure: str,
        feedback: str,
    ) -> str:
        """
        æ ¹æ®åé¦ˆæ”¹å†™ Visual Schema
        """
        rewrite_prompt_template = self.prompts.get("gemini_rewrite", {}).get(
            "prompt", ""
        )
        
        full_prompt = render_prompt(
            rewrite_prompt_template,
            original_structure=original_structure,
            feedback=feedback,
        )
        
        return self._call_gemini_text_api(
            system_role="",
            user_prompt=full_prompt,
        )

